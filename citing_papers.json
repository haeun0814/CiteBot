{
    "Context-aware and boundary-optimized model for road marking instance segmentation using MLS point cloud intensity images": {
        "title": "Context-aware and boundary-optimized model for road marking instance segmentation using MLS point cloud intensity images",
        "authors": [
            "Dehui Li",
            "Tao Liu",
            "Ping Du",
            "Tianen Ma",
            "Shuangtong Liu"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 1,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/a5366546e51f30f931f7c77d2ae7ab8a131f26e9",
        "isOpenAccess": false
    },
    "Siamese text classification network (SiamTCN) for multi-class multi-label information extraction of typhoon disasters from social media data": {
        "title": "Siamese text classification network (SiamTCN) for multi-class multi-label information extraction of typhoon disasters from social media data",
        "authors": [
            "Zhi He",
            "Chengle Zhou",
            "Liwei Zou",
            "Suhong Zhou",
            "Xueqiang Zhao"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/b06de044b8be3417238d2489589ad152c6c51533",
        "isOpenAccess": false
    },
    "DFGAnet: a dual-branch multimodal fusion network based on graph and attention for emotion recognition in conversation": {
        "title": "DFGAnet: a dual-branch multimodal fusion network based on graph and attention for emotion recognition in conversation",
        "authors": [
            "Wenzhuo Liu",
            "Taoying Li",
            "Yijia Chen"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/86da5a1be53b979a934f6ffffa9d448ed4dd2d53",
        "isOpenAccess": false
    },
    "Ethical perspectives on deployment of large language model agents in biomedicine: a survey": {
        "title": "Ethical perspectives on deployment of large language model agents in biomedicine: a survey",
        "authors": [
            "Nafiseh Ghaffar Nia",
            "Amin Amiri",
            "Yuan Luo",
            "Adrienne Kline"
        ],
        "arxiv_id": null,
        "venue": "AI and Ethics",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/a80a058a738caebe6541e9c525db849159eefd98",
        "isOpenAccess": false
    },
    "OF-DETR: an efficient end-to-end detector for tiny traffic targets in aerial images": {
        "title": "OF-DETR: an efficient end-to-end detector for tiny traffic targets in aerial images",
        "authors": [
            "Jie Hu",
            "Hanzhang Huang",
            "Feiyu Zhao",
            "Yuxuan Tang",
            "Shuaidi He",
            "Xinghao Chen",
            "Qixiang Guo",
            "Minchao Zhang"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/4b6b1e5a2253304824ad1cafaa943ca5c923fd2b",
        "isOpenAccess": false
    },
    "Context-aware feature complementary screening network for mass segmentation in whole mammograms": {
        "title": "Context-aware feature complementary screening network for mass segmentation in whole mammograms",
        "authors": [
            "Qingkun Guo",
            "Mei Liu",
            "Luhao Sun",
            "Chao Li",
            "Wenzong Jiang",
            "Weifeng Liu",
            "Lin Cong",
            "Zhiyong Yu",
            "Baodi Liu"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/b8063e58f79e780ca7f514bb28603b0e87aa2445",
        "isOpenAccess": false
    },
    "BccT: an efficient transformer model for blood cell classification": {
        "title": "BccT: an efficient transformer model for blood cell classification",
        "authors": [
            "Hanruo Zhu",
            "Ziquan Zhu",
            "Si-Yuan Lu"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/ecefd2c0f7d8239bc4727b803066cf58c5bbe68a",
        "isOpenAccess": false
    },
    "Mamba-transformer for low-light image enhancement in HVI color space": {
        "title": "Mamba-transformer for low-light image enhancement in HVI color space",
        "authors": [
            "Zepu Xu",
            "Shijie Hao"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/1b19726ccc2c9b90746f1b4624d2b171ccbb4095",
        "isOpenAccess": false
    },
    "Angle and graph topology enhanced framework with dual-channel mixed token progressing unit for sign language production": {
        "title": "Angle and graph topology enhanced framework with dual-channel mixed token progressing unit for sign language production",
        "authors": [
            "Yarun Yang",
            "Qingshan Wang",
            "Qi Wang",
            "Sheng Chen"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/3e436a9886e47a5730297b195b48c2a1ac6cdde5",
        "isOpenAccess": false
    },
    "MAWT: a market-contextual adaptive wavelet transformer for stock forecasting": {
        "title": "MAWT: a market-contextual adaptive wavelet transformer for stock forecasting",
        "authors": [
            "Hao Guo",
            "Yuefeng Cen",
            "Gang Cen",
            "Cheng Zhao"
        ],
        "arxiv_id": null,
        "venue": "International Journal of Data Science and Analysis",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/67293b22341392376638589ef14a5d3a040ab5b3",
        "isOpenAccess": false
    },
    "MSMamba: enhancing medical image segmentation with a multi-scanning Mamba hybrid network": {
        "title": "MSMamba: enhancing medical image segmentation with a multi-scanning Mamba hybrid network",
        "authors": [
            "Ruoyun Liu",
            "Jianshu Chao",
            "Jiahua Lai",
            "Qingwei Guo",
            "Ke Sun",
            "Zeyu Zhang"
        ],
        "arxiv_id": null,
        "venue": "The Visual Computer",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/71331a85d06c24f6924b47e9c5f6e2629fddbf35",
        "isOpenAccess": false
    },
    "Contrastive adversarial learning with dual-sample guidance for transferable attacks on vision-language pre-training models": {
        "title": "Contrastive adversarial learning with dual-sample guidance for transferable attacks on vision-language pre-training models",
        "authors": [
            "Yiming Ren",
            "Yang Xu",
            "Sicong Zhang",
            "Xiaoyao Xie"
        ],
        "arxiv_id": null,
        "venue": "Multimedia Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/151052d07f81e4e7b88972606d754ff7e6abb4e1",
        "isOpenAccess": false
    },
    "2512.03623": {
        "title": "The promising potential of vision language models for the generation of textual weather forecasts",
        "authors": [
            "Edward C. C. Steele",
            "Dinesh Mane",
            "Emilio Monti",
            "Luis Orus",
            "Rebecca Chantrill-Cheyette",
            "Matthew Couch",
            "Kirstine I. Dale",
            "Simon Eaton",
            "Govindarajan Rangarajan",
            "Amir Majlesi",
            "Steven Ramsdale",
            "Michael Sharpe",
            "Craig Smith",
            "Jonathan Smith",
            "Rebecca Yates",
            "Holly Ellis",
            "Charles Ewen"
        ],
        "arxiv_id": "2512.03623",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Physics"
        ],
        "abstract": "Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.",
        "abstract_summary_gcp": "This paper explores a novel application of multimodal foundation models in meteorology, an area currently underdeveloped. Specifically, it uses a vision language model to automatically generate the text for the iconic Shipping Forecast directly from video-encoded gridded weather data. Early results indicate this approach holds promise for significantly improving production efficiency and fostering innovation within weather services and other sectors.",
        "url": "https://www.semanticscholar.org/paper/ecdf9818905b17b1bb16e5be61cc1a407eeaa6b1",
        "isOpenAccess": false
    },
    "2512.03804": {
        "title": "EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification",
        "authors": [
            "Hanhui Deng",
            "Xinglin Li",
            "Jie Luo",
            "Zhanpeng Jin",
            "Di Wu"
        ],
        "arxiv_id": "2512.03804",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.",
        "abstract_summary_gcp": "This paper introduces novel deep learning technologies for effectively managing and analyzing electrocardiogram (ECG) data, aiming to build an accurate and rapid diagnostic model to reduce the burden on medical workers. Unlike existing ECG models with high misdiagnosis rates, their approach uses end-to-end deep learning to automatically extract features.\n\nSpecifically, they first developed **EfficientECG**, an accurate and lightweight classification model based on the existing EfficientNet, designed to handle high-frequency, long-sequence ECG data across various lead types. Building on this, they propose a **cross-attention-based feature fusion model** that extends EfficientECG to analyze multi-lead ECG data and incorporate additional features like gender and age. Evaluations on representative datasets demonstrate the model's superiority over state-of-the-art methods in terms of high precision, multi-feature fusion capabilities, and lightweight design.",
        "url": "https://www.semanticscholar.org/paper/dec300ff2826daf69f0ba15a082be59a7fb42e01",
        "isOpenAccess": false
    },
    "2512.03620": {
        "title": "SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting",
        "authors": [
            "Hanxiu Zhang",
            "Yue Zheng"
        ],
        "arxiv_id": "2512.03620",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.",
        "abstract_summary_gcp": "This paper introduces **SELF**, a novel intrinsic weight-based fingerprinting scheme designed to protect the Intellectual Property (IP) of Large Language Models (LLMs).\n\nThe authors highlight that while existing fingerprinting techniques can detect unauthorized LLM usage, they are vulnerable to false claims and weight manipulations. SELF overcomes these limitations by operating independently of input data and inherently resisting false claims.\n\nIts robustness stems from two key innovations:\n1.  **Fingerprint Extraction:** It generates unique, scalable, and transformation-invariant fingerprints by applying singular value and eigenvalue decomposition to the LLM's attention weights.\n2.  **Fingerprint Comparison:** It uses a neural network-based method, incorporating few-shot learning and data augmentation, for effective similarity comparison between fingerprints.\n\nExperimental results demonstrate SELF's high accuracy in detecting IP infringement and its strong resilience against various downstream modifications, including quantization, pruning, and fine-tuning attacks.",
        "url": "https://www.semanticscholar.org/paper/d7563bb046a3ff2e6fab35f507476cb99e219b14",
        "isOpenAccess": false
    },
    "2512.04021": {
        "title": "C3G: Learning Compact 3D Representations with 2K Gaussians",
        "authors": [
            "Honggyu An",
            "Jaewoo Jung",
            "Mungyeom Kim",
            "Sunghwan Hong",
            "Chaehyun Kim",
            "Kazumi Fukuda",
            "Minkyeong Jeon",
            "Jisang Han",
            "Takuya Narihira",
            "Hyuna Ko",
            "Junsu Kim",
            "Yuki Mitsufuji",
            "Seungryong Kim"
        ],
        "arxiv_id": "2512.04021",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.",
        "abstract_summary_gcp": "This paper addresses the challenge of feed-forward 3D scene reconstruction and understanding from unposed, sparse views. Existing methods, which use per-pixel 3D Gaussian Splatting for reconstruction followed by 2D-to-3D feature lifting, are criticized for generating excessive and redundant Gaussians. This leads to high memory overhead, sub-optimal multi-view feature aggregation, and degraded performance in novel view synthesis and scene understanding.\n\nTo overcome these limitations, the authors propose C3G, a novel feed-forward framework that estimates a **compact set of 3D Gaussians** only at essential spatial locations, thus minimizing redundancy. C3G introduces **learnable tokens** that aggregate multi-view features via **self-attention** to guide the generation of these compact Gaussians, ensuring each integrates relevant visual information across views. The learned attention patterns are then exploited for efficient Gaussian decoding and feature lifting.\n\nExtensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate C3G's effectiveness. The results show that this compact yet geometrically meaningful representation achieves high-quality scene reconstruction and understanding with superior memory efficiency and feature fidelity compared to previous methods.",
        "url": "https://www.semanticscholar.org/paper/d50203b3804c75a8158202047ee142ae352fa181",
        "isOpenAccess": false
    },
    "2512.03471": {
        "title": "SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening",
        "authors": [
            "Ian Henriques",
            "Lynda Elhassar",
            "Sarvesh Relekar",
            "Denis Walrave",
            "Shayan Hassantabar",
            "Vishu Ghanakota",
            "Adel Laoui",
            "Mahmoud Aich",
            "Rafia Tir",
            "Mohamed Zerguine",
            "Samir Louafi",
            "Moncef Kimouche",
            "Emmanuel Cosson",
            "N. Jha"
        ],
        "arxiv_id": "2512.03471",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.",
        "abstract_summary_gcp": "This paper introduces **SweetDeep**, a compact neural network designed for scalable and cost-effective type 2 diabetes detection, addressing the limitations of current invasive and costly biochemical assays.\n\nSweetDeep was trained on physiological and demographic data collected from 285 participants (both diabetic and non-diabetic) in the EU and MENA regions, using Samsung Galaxy Watch 7 devices under real-world, \"free-living\" conditions over six days. Each participant contributed approximately 20 two-minute sensor recordings.\n\nDespite comprising fewer than 3,000 parameters, SweetDeep achieved a patient-level accuracy of **82.5%** (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) in three-fold cross-validation. Its expected calibration error was 5.5%, and allowing it to abstain on less than 10% of low-confidence predictions further boosted accuracy to 84.5%.\n\nThese findings suggest that combining engineered features with lightweight AI models can enable accurate, rapid, and generalizable detection of type 2 diabetes using consumer wearables in real-world settings.",
        "url": "https://www.semanticscholar.org/paper/e2db8d9dd30ebb5d47b10ecb813379b2126bf714",
        "isOpenAccess": false
    },
    "2512.03722": {
        "title": "Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks",
        "authors": [
            "Lingyi Cai",
            "Wenjie Fu",
            "Yuxi Huang",
            "Ruichen Zhang",
            "Yinqiu Liu",
            "Jiawen Kang",
            "Zehui Xiong",
            "Tao Jiang",
            "Dusit Niyato",
            "Xianbin Wang",
            "Shiwen Mao",
            "Xuemin Shen"
        ],
        "arxiv_id": "2512.03722",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.",
        "abstract_summary_gcp": "This paper presents a comprehensive tutorial on enhancing Reinforcement Learning (RL) with Large Language Models (LLMs) for wireless networks.\n\nIt begins by acknowledging RL's success in wireless optimization but highlights its limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. LLMs are then introduced as a transformative AI paradigm, offering exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which can address these classical RL shortcomings.\n\nThe core contributions include:\n1.  **A novel taxonomy** categorizing LLM roles into four critical functions: **state perceiver, reward designer, decision-maker, and generator**.\n2.  **A review** of existing studies, examining how each of these LLM roles enhances different stages of the RL pipeline.\n3.  **Practical case studies** demonstrating the design and application of LLM-enhanced RL in diverse areas like low-altitude economy networking, vehicular networks, and space-air-ground integrated networks.\n\nFinally, the paper discusses potential future directions and offers insights into the further development of LLM-enhanced RL in wireless communications.",
        "url": "https://www.semanticscholar.org/paper/1cff2a420612e5bca0caa20e47f9421e092ccc10",
        "isOpenAccess": false
    },
    "2512.03343": {
        "title": "Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning",
        "authors": [
            "Darshan Fofadiya"
        ],
        "arxiv_id": "2512.03343",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift''where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \\citep{holtzman2019curious}. While scaling model size mitigates this \\citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head''trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector''that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.",
        "abstract_summary_gcp": "Autoregressive Language Models (LLMs) trained on Next-Token Prediction often suffer from \"Topic Drift,\" where generations deviate from the initial prompt due to a focus on local associations. To combat this, the authors introduce the **Idea-Gated Transformer**, a novel architecture designed to separate semantic planning from syntactic generation.\n\nThis model employs an auxiliary \"Idea Head\" that predicts the bag-of-words distribution for a future context, generating a latent \"Concept Vector.\" This vector then actively gates the main vocabulary during generation via a differentiable mechanism, suppressing semantically irrelevant tokens and effectively pruning the search space in real-time.\n\nExperiments on WikiText-103 show that while the Idea-Gated model achieves comparable perplexity to a standard GPT-2 baseline, it demonstrates **significantly superior \"Domain Retention.\"** This indicates the gating mechanism successfully locks generation into specific semantic clusters, resisting associative drift and offering a parameter-efficient path towards more controllable language modeling.",
        "url": "https://www.semanticscholar.org/paper/63fb21ae1a6e92a9dd95ad820607d859b4aeb69d",
        "isOpenAccess": false
    },
    "2512.03370": {
        "title": "ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding",
        "authors": [
            "Lingjun Zhao",
            "Yandong Luo",
            "James Hay",
            "Lu Gan"
        ],
        "arxiv_id": "2512.03370",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.",
        "abstract_summary_gcp": "ShelfGaussian is a novel open-vocabulary, multi-modal 3D scene understanding framework that utilizes Gaussian representations and is supervised by off-the-shelf vision foundation models (VFMs).\n\nIt addresses the limitations of existing Gaussian-based methods, which either model closed-set semantics (requiring 3D labels and neglecting rendering) or rely on purely 2D self-supervision (resulting in degraded geometry and being limited to camera-only settings).\n\nShelfGaussian introduces two key components:\n1.  **Multi-Modal Gaussian Transformer:** Allows Gaussians to query features from various sensor modalities.\n2.  **Shelf-Supervised Learning Paradigm:** Optimizes Gaussians using VFM features across both 2D image and 3D scene levels.\n\nThe framework demonstrates state-of-the-art zero-shot semantic occupancy prediction on the Occ3D-nuScenes benchmark and proves its practical efficacy in diverse urban environments on an unmanned ground vehicle (UGV).",
        "url": "https://www.semanticscholar.org/paper/4c417b3d6990427a99ea18657ab984270523d7cd",
        "isOpenAccess": false
    },
    "2512.03521": {
        "title": "Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation",
        "authors": [
            "Xiaosen Lyu",
            "Jiayu Xiong",
            "Yuren Chen",
            "Wanlong Wang",
            "Xiaoqing Dai",
            "Jing Wang"
        ],
        "arxiv_id": "2512.03521",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers'emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.",
        "abstract_summary_gcp": "This paper introduces Cross-Space Synergy (CSS) to improve Multimodal Emotion Recognition in Conversation (MERC). MERC aims to predict emotions from textual, acoustic, and visual cues, but existing methods struggle with complex cross-modal interactions, gradient conflicts, and unstable training.\n\nCSS addresses these issues by coupling two components:\n1.  **Synergistic Polynomial Fusion (SPF):** A representation component that uses low-rank tensor factorization to efficiently capture high-order cross-modal interactions.\n2.  **Pareto Gradient Modulator (PGM):** An optimization component that alleviates gradient conflicts and improves training stability by steering updates along Pareto-optimal directions.\n\nExperiments show that CSS outperforms current methods on IEMOCAP and MELD datasets, demonstrating superior accuracy and training stability in complex multimodal scenarios.",
        "url": "https://www.semanticscholar.org/paper/58ce4bca848abba25b14d6ee9fed81f6e08aaac0",
        "isOpenAccess": false
    },
    "2512.03538": {
        "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation",
        "authors": [
            "Yuhang Huang",
            "Shilong Zou",
            "Jiazhao Zhang",
            "Xinwang Liu",
            "Ruizhen Hu",
            "Kai Xu"
        ],
        "arxiv_id": "2512.03538",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.",
        "abstract_summary_gcp": "World Foundation Models (WFMs) demonstrate strong visual simulation, but their application to precise robotic control is hindered by a gap between their generative realism and the precision needed for control. Existing methods, which use WFMs for synthetic data, are computationally expensive and underutilize pre-trained Visual-Language-Action (VLA) policies.\n\nThis paper introduces **AdaPower**, a lightweight adaptation framework designed to transform general-purpose WFMs into specialist world models. AdaPower features two key components:\n1.  **Temporal-Spatial Test-Time Training (TS-TTT):** Enables adaptation during inference for improved precision.\n2.  **Memory Persistence (MP):** Ensures long-horizon consistency in predictions.\n\nIntegrated within a Model Predictive Control (MPC) framework, AdaPower empowers pre-trained VLA policies, leading to over **41% improvement** in task success rates on LIBERO benchmarks. This is achieved without needing to retrain the VLA policy, all while maintaining computational efficiency and the WFM's generalist capabilities.",
        "url": "https://www.semanticscholar.org/paper/5892afff343cdb129907bb5f98f791538c7f4c2b",
        "isOpenAccess": false
    },
    "2512.03795": {
        "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving",
        "authors": [
            "Jia Hu",
            "Zhexi Lian",
            "Xuerun Yan",
            "Ruiang Bi",
            "Dou Shen",
            "Yu Ruan",
            "Haoran Wang"
        ],
        "arxiv_id": "2512.03795",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.",
        "abstract_summary_gcp": "Autonomous Driving (AD) vehicles currently lack human-like interaction in dynamic traffic, primarily due to an insufficient understanding of social interaction mechanisms.\n\nTo address this, the paper introduces **MPCFormer**, an explainable and socially-aware AD approach. MPCFormer explicitly models multi-vehicle social interaction dynamics through a novel physics-informed, discrete state-space representation that embeds physics priors for explainability. The dynamics coefficients are learned from naturalistic driving data using a Transformer-based encoder-decoder architecture. This is highlighted as the first approach to explicitly model such dynamics.\n\nThe learned social dynamics enable the generation of diverse, human-like behaviors, while its integration with an MPC framework mitigates safety risks associated with purely learning-based methods.\n\nEvaluations demonstrate MPCFormer's effectiveness:\n*   **Open-loop (prediction):** Achieves superior social interaction awareness and the lowest trajectory prediction errors (0.86 m ADE over a 5-second horizon) on the NGSIM dataset.\n*   **Closed-loop (planning):** In complex, interactive scenarios (e.g., consecutive lane changes for off-ramp exiting), MPCFormer achieves a 94.67% planning success rate, improves driving efficiency by 15.75%, and drastically reduces collision rates from 21.25% to 0.5%, outperforming leading Reinforcement Learning-based planners.",
        "url": "https://www.semanticscholar.org/paper/832a8c7f43b258668a5bb07f8cf8529bf4f35eba",
        "isOpenAccess": false
    },
    "2512.03606": {
        "title": "Observation-driven correction of numerical weather prediction for marine winds",
        "authors": [
            "Matteo Peduto",
            "Qidong Yang",
            "Jonathan Giezendanner",
            "D. Tuia",
            "Sherrie Wang"
        ],
        "arxiv_id": "2512.03606",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.",
        "abstract_summary_gcp": "This paper introduces a novel approach to marine wind forecasting, reframing it as an observation-informed correction of global numerical weather prediction (NWP) models rather than direct forecasting. Given the sparsity and heterogeneity of ocean observations, the authors propose a transformer-based deep learning architecture that learns to adjust Global Forecast System (GFS) output by assimilating the latest in-situ observations.\n\nKey features of the model include:\n*   Handling irregular and time-varying observation sets using masking and set-based attention.\n*   Conditioning predictions on recent observation-forecast pairs via cross-attention.\n*   Employing cyclical time embeddings and coordinate-aware location representations for efficient single-pass inference at any spatial coordinate.\n\nEvaluated over the Atlantic Ocean using ICOADS observations, the model significantly reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, showing a 45% improvement at 1-hour lead time and a 13% improvement at 48 hours. Improvements are most pronounced along coastlines and shipping routes, where observations are abundant. The architecture is flexible, accommodating diverse observing platforms (ships, buoys) and generating both site-specific and basin-scale gridded predictions in a single pass. This demonstrates a practical, low-latency post-processing method that effectively complements NWP by correcting systematic forecast errors.",
        "url": "https://www.semanticscholar.org/paper/22dbcf528ed63ea3d3776d50e7f6e5d1044ea831",
        "isOpenAccess": false
    },
    "2512.04007": {
        "title": "On the Temporality for Sketch Representation Learning",
        "authors": [
            "Marcelo Isaias de Moraes Junior",
            "M. A. Ponti"
        ],
        "arxiv_id": "2512.04007",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.",
        "abstract_summary_gcp": "This work investigates the importance of the temporal aspect in learning sketch representations, addressing whether sketches should be treated as sequences and which internal orders are most relevant.\n\nThe findings indicate:\n1.  Treating sketches as sequences using traditional positional encodings is a valid approach.\n2.  Absolute coordinate representations consistently perform better than relative ones.\n3.  Non-autoregressive decoders achieve superior results compared to their autoregressive counterparts.\n4.  The significance of temporality is not universal, but rather depends on both the specific order considered and the task being evaluated.",
        "url": "https://www.semanticscholar.org/paper/e90113070515a23858f15201017d0b003f1770ec",
        "isOpenAccess": false
    },
    "2512.03598": {
        "title": "Memory-Guided Point Cloud Completion for Dental Reconstruction",
        "authors": [
            "Jianan Sun",
            "Yukang Huang",
            "Dongzhihan Wang",
            "Mingyu Fan"
        ],
        "arxiv_id": "2512.03598",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.",
        "abstract_summary_gcp": "This paper proposes a retrieval-augmented framework for the completion of partial dental point clouds, which often suffer from large missing regions. Traditional encoder-decoder models struggle with these gaps, leading to hallucinated structures.\n\nThe new framework integrates a **learnable prototype memory** into standard encoder-decoder pipelines. After an encoder generates a global descriptor for the partial input, the model retrieves the nearest manifold prototype from this memory. This prototype is then fused with the query feature using confidence-gated weighting before being passed to the decoder.\n\nThe memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes *without* requiring tooth-position labels. This provides crucial structural priors that stabilize the inference of missing regions, allowing the decoder to focus on recovering fine details. The module is plug-and-play, compatible with existing completion backbones, and uses the same training losses.\n\nExperiments on the Teeth3DS benchmark show consistent improvements in Chamfer Distance, with visualizations demonstrating sharper cusps, ridges, and interproximal transitions. The approach offers a simple yet effective way to leverage cross-sample regularities for more accurate and faithful dental point-cloud completion.",
        "url": "https://www.semanticscholar.org/paper/4c262b350b335cb9b5294db3a89cd4b01f60e8bd",
        "isOpenAccess": false
    },
    "2512.03720": {
        "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs",
        "authors": [
            "Tengyun Ma",
            "Jiaqi Yao",
            "Daojing He",
            "Shihao Peng",
            "Yu Li",
            "Shaohui Liu",
            "Zhuotao Tian"
        ],
        "arxiv_id": "2512.03720",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.",
        "abstract_summary_gcp": "This paper addresses critical vulnerabilities in Large Language Models (LLMs) related to their instruction handling, particularly under adversarial conditions, stemming from their uniform token processing.\n\nThe authors introduce a novel vulnerability called the **Tool-Completion Attack (TCA)**, which exploits function-calling mechanisms to subvert LLM behavior. To assess this threat, they developed the **Tool-Completion benchmark**, a security assessment framework. This benchmark revealed that even state-of-the-art LLMs are highly susceptible to TCA.\n\nTo counter these vulnerabilities, the paper proposes **Context-Aware Hierarchical Learning (CAHL)**. CAHL is a sophisticated mechanism that balances semantic comprehension with role-specific instruction constraints by analyzing contextual correlations between instruction segments to build a robust, context-aware instruction hierarchy.\n\nExperiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibits strong generalization capabilities in zero-shot evaluations, and maintains performance on generic tasks. Code for their work is publicly available.",
        "url": "https://www.semanticscholar.org/paper/571921e7d0d0a4927cc7fe73ed06cf9e4f8ba15a",
        "isOpenAccess": false
    },
    "2512.03915": {
        "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
        "authors": [
            "X. Y. Han",
            "Yuan Zhong"
        ],
        "arxiv_id": "2512.03915",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "abstract": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
        "abstract_summary_gcp": "This paper presents a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure, developed by DeepSeek's Wang et al. (2024), which addresses the operational challenge of efficiently utilizing GPUs in Sparse Mixture-of-Experts (s-MoE) models by minimizing idle experts.\n\nThe authors cast ALF-LB as a one-step-per-iteration primal-dual method for an assignment problem. In a stylized deterministic setting, this framework reveals three key structural properties:\n1.  **Monotonic improvement** of a Lagrangian objective.\n2.  A **preference rule** that effectively re-routes tokens from overloaded to underloaded experts.\n3.  An **approximate-balancing guarantee**.\n\nExtending this analysis to a generalized online optimization formulation, which accounts for the stochastic and dynamic nature of AI training, the framework identifies a **strong convexity property** of the objective. This property, under specific step-size choices, leads to a **logarithmic expected regret bound**, a significant theoretical finding for the online setting.\n\nFinally, the theoretical findings are complemented by real-world experiments conducted on 1B-parameter DeepSeekMoE models. Together, these results establish a principled framework for understanding and analyzing ALF-LB in s-MoE architectures.",
        "url": "https://www.semanticscholar.org/paper/6e1d5b48dc5aa1de581f3073bed7357cc6784075",
        "isOpenAccess": false
    },
    "2512.03838": {
        "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs",
        "authors": [
            "Michael Staniek",
            "Artem Sokolov",
            "Stefan Riezler"
        ],
        "arxiv_id": "2512.03838",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
        "abstract_summary_gcp": "This paper addresses the critical need for faithful explanations in machine learning models used for early medical prediction, a factor often neglected in favor of prediction accuracy and crucial for gaining trust from medical practitioners.\n\nThe authors propose training Large Language Models (LLMs) to follow established medical consensus guidelines step-by-step in their reasoning and prediction process. They leverage the ubiquity of these guidelines to create data by instantiating verbalized medical inference rules from electronic health records. This data is then used to fine-tune LLMs to learn these consensus rules and their potential exceptions across various medical areas.\n\nA key advantage of this approach is the ability to automatically evaluate the model's inference process for \"derivation correctness\" (faithful deduction from premises) and \"value correctness\" (comparing predictions against real-world data), directly against the consensus rules.\n\nUsing the complex Sepsis-3 consensus definition as an example, their experiments show that small, fine-tuned LLMs significantly outperform much larger LLMs employing one-shot prompting with explicit definitions, as well as models trained on general medical texts. They find that fine-tuning on verbalized rule instantiations results in nearly perfect derivation correctness for rules and exceptions on unseen patient data within a specific medical area.\n\nThe paper concludes that the primary bottleneck for early prediction with this method is not out-of-distribution generalization (as rule adherence is robust), but rather the challenge of *generalization into the future* by forecasting sparsely and irregularly sampled clinical variables. They improve this by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
        "url": "https://www.semanticscholar.org/paper/51198c8be4b25f7bb33840faac6641a1702ff011",
        "isOpenAccess": false
    },
    "2512.03870": {
        "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
        "authors": [
            "Hongzhan Lin",
            "Zhiqi Bai",
            "Xinmiao Zhang",
            "Sen Yang",
            "Xiang Li",
            "Siran Yang",
            "Yunlong Xu",
            "Jiaheng Liu",
            "Yongchi Zhao",
            "Jiamang Wang",
            "Yuchi Xu",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "arxiv_id": "2512.03870",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
        "abstract_summary_gcp": "Transformer decoders struggle with prohibitive KV cache memory requirements at long sequence lengths. While existing cross-layer sharing methods (e.g., YOCO, CLA) attempt to mitigate this, they generally underperform within-layer methods like GQA.\n\nTo address this, an investigation into key and value information flow revealed that values predominantly derive from bottom layers, whereas keys draw from both bottom and middle layers.\n\nLeveraging this insight, the authors propose **FusedKV**, a method where top-layer KV caches are a *learnable fusion* of the most informative keys and values from the bottom and middle layers. This fusion directly processes post-RoPE keys, efficiently preserving positional information without recomputing embeddings.\n\nFor enhanced efficiency, **FusedKV-Lite** is introduced as a cross-layer sharing approach, directly sourcing top-layer KV caches from bottom-layer values and middle-layer keys, which reduces I/O overhead at a slight increase in perplexity.\n\nExperiments on LLMs ranging from 332M to 4B parameters show that FusedKV methods reduce cache memory by 50% while achieving *lower* validation perplexity than standard Transformer decoders, establishing them as a memory-efficient and high-performance architectural alternative.",
        "url": "https://www.semanticscholar.org/paper/73d41f0f4db4506225fb0c7c3fe879e4fe6ff295",
        "isOpenAccess": false
    },
    "2512.03508": {
        "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation",
        "authors": [
            "Seogkyu Jeon",
            "Kibeom Hong",
            "Hyeran Byun"
        ],
        "arxiv_id": "2512.03508",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.",
        "abstract_summary_gcp": "This paper introduces **DPMFormer (Domain-aware Prompt-driven Masked Transformer)**, a novel framework for Domain Generalized Semantic Segmentation (DGSS) that addresses the semantic misalignment between visual and textual contexts in VLM-based approaches.\n\nExisting methods overlook this misalignment, which stems from the rigidity of fixed context prompts learned on single source domains. DPMFormer tackles this by:\n\n1.  **Domain-aware Prompt Learning:** To achieve better semantic alignment between visual and textual cues.\n2.  **Domain-aware Contrastive Learning with Texture Perturbation:** To diversify observable domains and capture various domain-specific properties from a single source dataset.\n3.  **Domain-robust Consistency Learning:** To guide the model in minimizing prediction discrepancies between original and augmented images, enhancing resilience against diverse environmental changes.\n\nExperiments show DPMFormer's superiority, establishing a new state-of-the-art on various DGSS benchmarks. Code is available on GitHub.",
        "url": "https://www.semanticscholar.org/paper/ff4dd2315a525a2a807076ab5f46537f0a56da84",
        "isOpenAccess": false
    },
    "2512.03608": {
        "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
        "authors": [
            "Lishuo Deng",
            "Shaojie Xu",
            "Jinwu Chen",
            "Changwei Yan",
            "Jiajie Wang",
            "Zhe Jiang",
            "Weiwei Shan"
        ],
        "arxiv_id": "2512.03608",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties. We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
        "abstract_summary_gcp": "This paper introduces **KVNAND**, a novel DRAM-free architecture designed to efficiently deploy large language models (LLMs) on edge devices by storing both model weights and the key-value (KV) cache entirely within compute-enabled 3D NAND flash.\n\nThe core problem is that while in-flash computing (IFC) solutions address the weight-loading bottleneck for LLMs on resource-constrained platforms, the growing KV cache still relies on expensive and capacity-limited DRAM. As context lengths increase, the KV cache can exceed model weights in size, leading to prohibitive costs or out-of-memory issues, and attempts to offload it to traditional flash storage suffer severe performance penalties.\n\nKVNAND addresses these challenges by:\n1.  **Leveraging IFC** for *all* memory-bound operations (not just weights) to minimize data transfer overhead.\n2.  **Introducing head-group parallelism** to boost throughput for KV cache access.\n3.  **Employing page-level KV cache mapping** to align token access patterns with flash organization.\n4.  Proposing a **design space exploration framework** to optimize weight and KV placement within the flash.\n\nThese techniques mitigate latency, energy, and reliability concerns associated with using flash for intensive KV cache access. Evaluations on 7B and 70B LLMs show that KVNAND achieves an average **~2x geomean speedup** over DRAM-equipped IFC designs at various context lengths (128 to 10K tokens) and critically, **eliminates out-of-memory failures** at 100K context lengths, making long-context KV storage practical in flash.",
        "url": "https://www.semanticscholar.org/paper/3677fdd7cb7d64645ab4dfb6c609e6e9d566751d",
        "isOpenAccess": false
    },
    "2512.03818": {
        "title": "Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology",
        "authors": [
            "Kylie L. Anglin",
            "Stephanie Milan",
            "Brittney Hernandez",
            "Claudia Ventura"
        ],
        "arxiv_id": "2512.03818",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.",
        "abstract_summary_gcp": "Large language models (LLMs) are effective for text classification, but their performance is highly sensitive to prompt wording, particularly for precisely defined, theory-driven concepts found in domains like psychology. This paper introduces an empirical framework to optimize LLM classification performance via prompt engineering.\n\nThe study experimentally evaluated five prompting strategies—codebook-guided empirical selection, automatic prompt engineering, persona prompting, chain-of-thought, and explanatory prompting—using both zero-shot and few-shot classification across three constructs and two models.\n\nKey findings indicate that persona, chain-of-thought, and explanatory prompting alone do not fully overcome performance loss from poorly worded prompts. Instead, the most influential prompt features are the **construct definition**, **task framing**, and, to a lesser extent, the **provided examples**. The highest alignment with expert judgments was achieved through a **few-shot prompt that combined codebook-guided empirical prompt selection with automatic prompt engineering**.\n\nThe authors recommend that researchers generate and empirically evaluate as many prompt variants as feasible (human-crafted, automatically generated, or both) on a training dataset, selecting the best performers and validating them on a holdout set. This systematic approach offers a practical method for optimizing LLM prompts when alignment with expert judgment is crucial.",
        "url": "https://www.semanticscholar.org/paper/f5cb1bbb0cb501553938f2e1beea37729f3f85e1",
        "isOpenAccess": false
    },
    "2512.03350": {
        "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
        "authors": [
            "Yu Yuan",
            "Tharindu Wickremasinghe",
            "Zeeshan Nadir",
            "Xijun Wang",
            "Yiheng Chi",
            "Stanley H. Chan"
        ],
        "arxiv_id": "2512.03350",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.",
        "abstract_summary_gcp": "SeeU is a novel approach that addresses the limitations of current visual AI, which typically operates on 2D images/videos despite them being discrete projections of a 4D (3D space + time) world.\n\nThe core of SeeU is a **2D$\\to$4D$\\to$2D learning framework**:\n1.  **2D$\\to$4D (Reconstruction):** It first reconstructs a 4D representation of the world from sparse, monocular 2D input frames.\n2.  **Discrete 4D$\\to$Continuous 4D (Dynamics Learning):** It then learns the continuous 4D dynamics of this reconstructed world, leveraging low-rank representations and physical constraints.\n3.  **4D$\\to$2D (Generation):** Finally, SeeU projects the learned 4D world forward in time and re-projects it back into 2D at desired times and viewpoints, generating unseen visual content with strong spatial-temporal context awareness.\n\nBy modeling dynamics in 4D, SeeU enables continuous and physically-consistent novel visual generation, showing significant potential for tasks such as unseen temporal generation, unseen spatial generation, and video editing.",
        "url": "https://www.semanticscholar.org/paper/f69341e1d2d35ff91af555c83634e887c9250728",
        "isOpenAccess": false
    },
    "2512.03796": {
        "title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling",
        "authors": [
            "Hong-Kai Zheng",
            "Piji Li"
        ],
        "arxiv_id": "2512.03796",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.",
        "abstract_summary_gcp": "Visual Autoregressive (VAR) models generate images by processing hierarchical scales and decoding multiple tokens per scale in parallel, offering high generation quality and speed. However, this parallel token sampling within a scale can introduce structural errors, leading to suboptimal images.\n\nTo mitigate this, the paper proposes Latent Scale Rejection Sampling (LSRS). LSRS enhances VAR models by progressively refining latent token maps during inference. It uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the highest-quality map to guide the generation of subsequent scales. By prioritizing refinement in early, structurally critical scales, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency.\n\nExperiments demonstrate that LSRS significantly improves VAR's generation quality with minimal overhead. For instance, on the VAR-d30 model, LSRS reduces the FID score from 1.95 to 1.78 with only a 1% increase in inference time, and further to 1.66 with a 15% increase. LSRS thus provides an efficient test-time solution for enhancing VAR-based image generation.",
        "url": "https://www.semanticscholar.org/paper/036a1cb97af3b2c953f6bcf477de8a9f5770c096",
        "isOpenAccess": false
    },
    "2512.03644": {
        "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management",
        "authors": [
            "Bohan Zhao",
            "Yuanhong Wang",
            "Chenglin Liu",
            "Jiagi Pan",
            "Guang Yang",
            "Ruitao Liu",
            "Tingrui Zhang",
            "Kai Luo",
            "Wei Xu"
        ],
        "arxiv_id": "2512.03644",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.",
        "abstract_summary_gcp": "LLM training on large clusters faces efficiency challenges due to node failures, lengthy recoveries, and bulky checkpoints. Existing checkpointing methods are inefficient: infrequent ones cause costly rollbacks, while frequent ones create prohibitive overhead. To solve this, FFTrainer is proposed. It leverages surplus network capacity to quickly save and load training states, thereby preventing rollbacks and accelerating recovery. FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training operations.",
        "url": "https://www.semanticscholar.org/paper/cfae2f94c1512c76e43891910bb5578d71c77ae9",
        "isOpenAccess": false
    },
    "2512.03424": {
        "title": "DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding",
        "authors": [
            "Bin Liu",
            "Chunyang Wang",
            "Xuelian Liu"
        ],
        "arxiv_id": "2512.03424",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \\textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.",
        "abstract_summary_gcp": "This paper addresses the challenge of applying State Space Models (SSMs) to point clouds. While SSMs excel at long-sequence modeling, their reliance on a fixed input order conflicts with the irregular nature of point clouds, and existing serialization methods are rigid.\n\nTo overcome this, the authors propose **DM3D**, a deformable Mamba architecture for point cloud understanding. DM3D introduces an **offset-guided Gaussian sequencing mechanism** which unifies local resampling and global reordering. Key components include:\n\n1.  **Gaussian-based KNN Resampling (GKR)**: Enhances structural awareness by adaptively reorganizing neighboring points.\n2.  **Gaussian-based Differentiable Reordering (GDR)**: Allows end-to-end optimization of the point cloud's serialization order.\n3.  **Tri-Path Frequency Fusion module**: Improves feature complementarity and reduces aliasing.\n\nTogether, these innovations enable **structure-adaptive serialization** of point clouds. Experiments on benchmark datasets show DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.",
        "url": "https://www.semanticscholar.org/paper/9714242fec0b1d7216ffee01d2c325d5b1f4e14b",
        "isOpenAccess": false
    },
    "2512.03949": {
        "title": "Performance and efficiency of a transformer-based quark/gluon jet tagger in the ATLAS experiment",
        "authors": [
            "Atlas Collaboration"
        ],
        "arxiv_id": "2512.03949",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Physics"
        ],
        "abstract": "A deep-learning approach based on the transformer architecture is developed to distinguish between jets originating from quarks and gluons. The algorithm operates on jets with transverse momentum $p_{\\text{T}}>20$ and pseudorapidity $|\\eta|<4.5$ and takes as input several properties derived from the jet constituents, using information from the ATLAS detector's tracker and calorimeter. The algorithm's performance is evaluated by analyzing dijet data events from proton-proton collisions at $\\sqrt{s} = 13$ and $13.6$ TeV during Run 2 and Run 3 of the Large Hadron Collider. Two methods are used to obtain distributions from quark- or gluon-initiated jets in data: a matrix method fully based on Monte Carlo simulation and a new approach named `jet topics'which has less dependence on the modelling of the physics process under study. The quark and gluon identification efficiencies measured in data for the 50% quark-identification-efficiency working point vary from the simulated ones for quark-initiated (gluon-initiated) jets by factors of 0.88-1.30 (0.61-1.05) with uncertainties of 10%-70% (10%-95%). The uncertainties estimated with the jet topics method are smaller than those estimated with the matrix method, with up to 20% less systematic uncertainty in some phase-space regions. The advances in jet identification reported here provide a robust tool for precision Standard Model measurements and searches for new physics at the LHC.",
        "abstract_summary_gcp": "This paper describes the development of a deep-learning algorithm, based on the transformer architecture, designed to distinguish between quark and gluon jets. The algorithm processes jet properties derived from ATLAS detector's tracker and calorimeter for jets with transverse momentum $p_T > 20$ GeV and pseudorapidity $|\\eta| < 4.5$. Its performance was assessed using dijet data from proton-proton collisions at $\\sqrt{s} = 13$ and $13.6$ TeV (LHC Run 2 and 3).\n\nTo obtain quark and gluon jet distributions from data, two methods were used: a Monte Carlo-based matrix method and a new 'jet topics' approach designed to be less dependent on simulation. At a 50% quark-identification-efficiency working point, the measured data-to-simulation ratios for quark (gluon) jet identification efficiencies were 0.88-1.30 (0.61-1.05), with uncertainties ranging from 10-70% (10-95%). Notably, the 'jet topics' method produced smaller uncertainties, reducing systematic uncertainty by up to 20% in certain phase-space regions compared to the matrix method. This new tool represents a significant advancement for precision Standard Model measurements and new physics searches at the LHC.",
        "url": "https://www.semanticscholar.org/paper/dff850f74c796eb7717892b33ebd0c854d278d09",
        "isOpenAccess": false
    },
    "2512.03442": {
        "title": "PretrainZero: Reinforcement Active Pretraining",
        "authors": [
            "Xingrun Xing",
            "Zhiyuan Fan",
            "Jie Lou",
            "Guoqi Li",
            "Jiajun Zhang",
            "Debing Zhang"
        ],
        "arxiv_id": "2512.03442",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
        "abstract_summary_gcp": "This paper introduces **PretrainZero**, a novel reinforcement active learning framework designed to extend Reinforcement Learning (RL) from domain-specific post-training to general pretraining, aiming for Artificial General Intelligence (AGI).\n\nTraditional RL models for general intelligence are bottlenecked by their reliance on verifiable rewards in specific domains. PretrainZero addresses this by:\n\n1.  **Active Pretraining:** Learning a unified reasoning policy that actively identifies reasonable and informative content from a general pretraining corpus (e.g., Wikipedia) and uses RL to predict this content, mimicking human active learning.\n2.  **Self-supervised Learning:** Operating without any verifiable labels, pretrained reward models, or supervised fine-tuning. It directly pretrains reasoners on general corpora using RL, thereby overcoming the \"verification data-wall\" for general reasoning.\n3.  **Verification Scaling:** Enhancing general reasoning capabilities by progressively tackling more challenging masked spans within the pretraining process.\n\nAs a result, PretrainZero significantly improved the Qwen3-4B-Base model's performance on MMLU-Pro (8.43 points), SuperGPQA (5.96 points), and math average benchmarks (10.60 points). Furthermore, the models pretrained with PretrainZero can serve as robust reasoning foundation models for downstream RLVR (Reinforcement Learning from Verbose Reasoning) tasks.",
        "url": "https://www.semanticscholar.org/paper/0509f3a645155971dca0ac5d67da614505b71f08",
        "isOpenAccess": false
    },
    "2512.04025": {
        "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
        "authors": [
            "Xiaolong Li",
            "Youping Gu",
            "Xi Lin",
            "Weijie Wang",
            "Bohan Zhuang"
        ],
        "arxiv_id": "2512.04025",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
        "abstract_summary_gcp": "Attention mechanisms, while central to foundation models, suffer from quadratic complexity, a bottleneck that current sparse attention methods attempt to address. However, these methods typically use binary masks, leading to significant information loss when aggressively pruning key-value (KV) blocks.\n\nTo overcome this, the paper introduces **Pyramid Sparse Attention (PSA)**, a versatile module for video understanding and generation. Instead of binary masking, PSA employs **multi-level pooled KV representations**, allowing for finer mask granularity. Each query block dynamically allocates lower (more detailed) pooling levels to critical KV blocks and higher (more compressed) levels to less important ones. This creates an \"informative interpolation\" between full retention and complete pruning, akin to fixed-point quantization or feature pyramid networks.\n\nThis design effectively **mitigates information loss** while maintaining **computational efficiency** under low compute budgets. PSA also features a native, hardware-friendly kernel for efficient execution. Across video understanding and generation benchmarks, PSA consistently outperforms or matches existing sparse attention baselines, offering **superior efficiency-quality trade-offs** by preserving contextual information and visual fidelity.\n\nThe code and model weights are publicly available.",
        "url": "https://www.semanticscholar.org/paper/022061dcd2c28d1f62206d1e5745b767ebf35b0b",
        "isOpenAccess": false
    },
    "2512.03377": {
        "title": "Nexus: Higher-Order Attention Mechanisms in Transformers",
        "authors": [
            "Hanting Chen",
            "Chu Zhong",
            "Kai Han",
            "Yuchuan Tian",
            "Yuchen Liang",
            "Tianyu Guo",
            "Xinghao Chen",
            "Dacheng Tao",
            "Yunhe Wang"
        ],
        "arxiv_id": "2512.03377",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \\textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \\textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.",
        "abstract_summary_gcp": "This paper introduces the **Higher-Order Attention Network (Hon)**, a novel architecture designed to overcome a key limitation of standard Transformers. While Transformers excel in many domains, their first-order self-attention mechanism suffers from a low-rank bottleneck, hindering its ability to capture intricate, multi-hop relationships within a single layer.\n\nHon addresses this by implementing a recursive framework where Query and Key representations are not static linear projections, but are dynamically refined through **nested self-attention mechanisms**. This means Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations *before* the final attention computation.\n\nCrucially, Hon maintains parameter efficiency by employing a weight-sharing strategy across recursive steps, adding only $\\mathcal{O}(1)$ additional parameters. Theoretical analysis demonstrates that Hon successfully breaks the linear bottleneck of standard attention, and empirical results show it outperforms standard Transformers on various benchmarks.",
        "url": "https://www.semanticscholar.org/paper/9b8cb6eb3258af236d8a496a9e80b776308f577f",
        "isOpenAccess": false
    },
    "2512.03767": {
        "title": "CaFTRA: Frequency-Domain Correlation-Aware Feedback-Free MIMO Transmission and Resource Allocation for 6G and Beyond",
        "authors": [
            "Bo Qian",
            "Hanlin Wu",
            "Jiacheng Chen",
            "Yunting Xu",
            "Xiaoyu Wang",
            "Haibo Zhou",
            "Yusheng Ji"
        ],
        "arxiv_id": "2512.03767",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Engineering",
            "Computer Science",
            "Mathematics"
        ],
        "abstract": "The fundamental design of wireless systems toward AI-native 6G and beyond is driven by the need for ever-increasing demand of mobile data traffic, extreme spectral efficiency, and adaptability across diverse service scenarios. To overcome the limitations posed by feedback-based multiple-input and multiple-output (MIMO) transmission, we propose a novel frequency-domain Correlation-aware Feedback-free MIMO Transmission and Resource Allocation (CaFTRA) framework tailored for fully-decoupled radio access networks (FD-RAN) to meet the emerging requirements of AI-Native 6G and beyond. By leveraging artificial intelligence (AI), CaFTRA effectively eliminates real-time uplink feedback by predicting channel state information (CSI) based solely on user geolocation. We introduce a Learnable Queries-driven Transformer Network for CSI mapping from user geolocation, which utilizes multi-head attention and learnable query embeddings to accurately capture frequency-domain correlations among resource blocks (RBs), thereby significantly improving the precision of CSI prediction. Once base stations (BSs) adopt feedback-free transmission, their downlink transmission coverage can be significantly expanded due to the elimination of frequent uplink feedback. To enable efficient resource scheduling under such extensive-coverage scenarios, we apply a low-complexity many-to-one matching theory-based algorithm for efficient multi-BS association and multi-RB resource allocation, which is proven to converge to a stable matching within limited iterations. Simulation results demonstrate that CaFTRA achieves stable matching convergence and significant gains in spectral efficiency and user fairness compared to 5G, underscoring its potential value for 6G standardization efforts.",
        "abstract_summary_gcp": "The document introduces **CaFTRA (Correlation-aware Feedback-free MIMO Transmission and Resource Allocation)**, a novel framework designed for AI-native 6G and beyond, specifically for fully-decoupled radio access networks (FD-RANs). It aims to overcome the limitations of feedback-based MIMO transmission by eliminating the need for real-time uplink feedback.\n\nCaFTRA achieves this by leveraging AI to predict channel state information (CSI) solely based on user geolocation. This prediction is performed by a **Learnable Queries-driven Transformer Network** that uses multi-head attention and learnable query embeddings to accurately capture frequency-domain correlations among resource blocks, significantly improving CSI prediction precision.\n\nEliminating feedback allows for expanded downlink transmission coverage. To manage resource scheduling in these extensive-coverage scenarios, CaFTRA employs a **low-complexity, many-to-one matching theory-based algorithm** for efficient multi-base station association and multi-resource block allocation, proven to converge to a stable matching rapidly.\n\nSimulation results indicate that CaFTRA achieves stable matching convergence and delivers significant gains in spectral efficiency and user fairness compared to 5G, positioning it as a promising solution for 6G standardization.",
        "url": "https://www.semanticscholar.org/paper/5c590c2ccce5fea319078febad7ae0921c01b291",
        "isOpenAccess": false
    },
    "2512.03837": {
        "title": "Heatmap Pooling Network for Action Recognition from RGB Videos",
        "authors": [
            "Mengyuan Liu",
            "Jinfu Liu",
            "Yongkang Jiang",
            "Bin He"
        ],
        "arxiv_id": "2512.03837",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.",
        "abstract_summary_gcp": "This paper addresses challenges in Human Action Recognition (HAR) from RGB videos, specifically information redundancy, noise susceptibility, and high storage costs associated with existing deep feature extraction methods.\n\nTo overcome these, the authors propose a novel **Heatmap Pooling Network (HP-Net)**. The core of HP-Net is a **feedback pooling module** that extracts information-rich, robust, and concise pooled features of the human body. These new pooled features are shown to outperform traditional pose data and heatmap features.\n\nAdditionally, HP-Net integrates these features with other multimodal data through two further components: a **spatial-motion co-learning module** and a **text refinement modulation module**, leading to more robust action recognition.\n\nExtensive experiments on several benchmarks (NTU RGB+D 60/120, Toyota-Smarthome, and UAV-Human) demonstrate HP-Net's effectiveness, consistently outperforming existing HAR methods. The code is publicly available.",
        "url": "https://www.semanticscholar.org/paper/8722b0f1e2ab9ef4cdae3778ede3eece96dbcae7",
        "isOpenAccess": false
    },
    "Performance Evaluation of Whisper-Series Speech Transcription Models on Raspberry Pi": {
        "title": "Performance Evaluation of Whisper-Series Speech Transcription Models on Raspberry Pi",
        "authors": [
            "Yue Cao"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Book"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/840adf90a7117dde8133ade3c601ae45c0e71389",
        "isOpenAccess": false
    },
    "Seeing Patterns Differently: Topological Geometry for Anomaly Detection in Multivariate Time Series": {
        "title": "Seeing Patterns Differently: Topological Geometry for Anomaly Detection in Multivariate Time Series",
        "authors": [
            "Kanchon Gharami",
            "Humayra Tasnim",
            "M. Akbaş",
            "Shafika Showkat Moni"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Book"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/66ea43df9eaf26465f2e100c78d4c9f954f8262f",
        "isOpenAccess": false
    },
    "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge": {
        "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge",
        "authors": [
            "Zihao Ding",
            "Mufeng Zhu",
            "Zhongze Tang",
            "Sheng Wei",
            "Yao Liu"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Book"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/ab92c9dfcd23561c9f62b1fc9c5a05e88642838a",
        "isOpenAccess": false
    },
    "2512.02968": {
        "title": "Flexible Gravitational-Wave Parameter Estimation with Transformers",
        "authors": [
            "Annalena Kofler",
            "Maximilian Dax",
            "Stephen R. Green",
            "J. Wildberger",
            "N. Gupte",
            "Jakob H. Macke",
            "J. Gair",
            "A. Buonanno",
            "Bernhard Scholkopf"
        ],
        "arxiv_id": "2512.02968",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Physics",
            "Computer Science"
        ],
        "abstract": "Gravitational-wave data analysis relies on accurate and efficient methods to extract physical information from noisy detector signals, yet the increasing rate and complexity of observations represent a growing challenge. Deep learning provides a powerful alternative to traditional inference, but existing neural models typically lack the flexibility to handle variations in data analysis settings. Such variations accommodate imperfect observations or are required for specialized tests, and could include changes in detector configurations, overall frequency ranges, or localized cuts. We introduce a flexible transformer-based architecture paired with a training strategy that enables adaptation to diverse analysis settings at inference time. Applied to parameter estimation, we demonstrate that a single flexible model -- called Dingo-T1 -- can (i) analyze 48 gravitational-wave events from the third LIGO-Virgo-KAGRA Observing Run under a wide range of analysis configurations, (ii) enable systematic studies of how detector and frequency configurations impact inferred posteriors, and (iii) perform inspiral-merger-ringdown consistency tests probing general relativity. Dingo-T1 also improves median sample efficiency on real events from a baseline of 1.4% to 4.2%. Our approach thus demonstrates flexible and scalable inference with a principled framework for handling missing or incomplete data -- key capabilities for current and next-generation observatories.",
        "abstract_summary_gcp": "This paper addresses the growing challenge of gravitational-wave (GW) data analysis, where traditional methods struggle with increasing complexity and deep learning models often lack the flexibility for varied analysis settings (e.g., detector configurations, frequency ranges, data cuts).\n\nThe authors introduce **Dingo-T1**, a novel flexible transformer-based architecture paired with a specialized training strategy. This approach allows a single model to adapt to diverse analysis settings at inference time for parameter estimation.\n\nKey achievements of Dingo-T1 include:\n1.  Analyzing 48 GW events from the third LIGO-Virgo-KAGRA Observing Run under a wide array of analysis configurations.\n2.  Enabling systematic studies on how detector and frequency settings impact inferred posterior distributions.\n3.  Performing inspiral-merger-ringdown consistency tests to probe general relativity.\n4.  Improving median sample efficiency on real events from a baseline of 1.4% to 4.2%.\n\nDingo-T1 thus demonstrates a flexible, scalable, and principled framework for GW inference, capable of handling missing or incomplete data, which is crucial for current and next-generation observatories.",
        "url": "https://www.semanticscholar.org/paper/26d0a6d000d6b0e3843ef6c8250ca7da3c14aa88",
        "isOpenAccess": false
    },
    "2512.02496": {
        "title": "Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration",
        "authors": [
            "Mizuki Kikkawa",
            "Tatsuya Yatagawa",
            "Y. Ohtake",
            "Hiromasa Suzuki"
        ],
        "arxiv_id": "2512.02496",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.",
        "abstract_summary_gcp": "This study investigates the impact of feature vector invariance under translation and rotation in deep-learning-based partial-to-partial point set registration, particularly those using Gaussian Mixture Models (GMMs). It uncovers theoretical and practical limitations in existing methods, focusing on DeepGMR's performance for partial-to-partial tasks.\n\nTo address these issues and acquire transformation-invariant features, the researchers propose an **attention-based reference point shifting (ARPS) layer**. This ARPS layer robustly identifies a common reference point between two partial point sets using an attention module (rather than an overlap region).\n\nThe integration of the ARPS layer significantly enhances the performance of DeepGMR and its variant, UGMMReg. Furthermore, these extended models outperform prior deep learning methods that use attention blocks or Transformers to extract overlap regions or common reference points, providing deeper insights into deep learning and GMM-based registration techniques.",
        "url": "https://www.semanticscholar.org/paper/49a0acb6cff59f7e88f1a459b873737455216617",
        "isOpenAccess": false
    },
    "2512.02556": {
        "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
        "authors": [
            "DeepSeek-AI",
            "Aixin Liu",
            "Aoxue Mei",
            "Bangcai Lin",
            "Bing Xue",
            "Bingxuan Wang",
            "Bingzheng Xu",
            "Bochao Wu",
            "Bowei Zhang",
            "Chaofan Lin",
            "Chen Dong",
            "Chengda Lu",
            "Chenggang Zhao",
            "Chengqi Deng",
            "Chenhao Xu",
            "C. Ruan",
            "Damai Dai",
            "Daya Guo",
            "Dejian Yang",
            "Deli Chen",
            "Erhang Li",
            "Fangqi Zhou",
            "Fangyun Lin",
            "Fucong Dai",
            "Guangbo Hao",
            "Guanting Chen",
            "Guowei Li",
            "H. Zhang",
            "Hanwei Xu",
            "Hao Li",
            "Haofen Liang",
            "Haoran Wei",
            "Haowei Zhang",
            "Haowen Luo",
            "Haozhe Ji",
            "Honghui Ding",
            "Hongxuan Tang",
            "Huanqi Cao",
            "Huazuo Gao",
            "Huixian Qu",
            "Hui Zeng",
            "Jialiang Huang",
            "Jiashi Li",
            "Jiaxin Xu",
            "Jiewen Hu",
            "JingChang Chen",
            "Jingting Xiang",
            "Jingyang Yuan",
            "Jingyuan Cheng",
            "Jinhua Zhu",
            "Jun Ran",
            "Junguang Jiang",
            "Junjie Qiu",
            "Junlong Li",
            "Junxiao Song",
            "Kai Dong",
            "Kaige Gao",
            "Kang Guan",
            "Kexin Huang",
            "Kexing Zhou",
            "Kezhao Huang",
            "K. Yu",
            "Lean Wang",
            "Lecong Zhang",
            "Lei Wang",
            "Liang Zhao",
            "Liangsheng Yin",
            "Lihua Guo",
            "Lingxiao Luo",
            "Linwang Ma",
            "Litong Wang",
            "Liyue Zhang",
            "M. S. Di",
            "M. Y. Xu",
            "Mingchuan Zhang",
            "Minghua Zhang",
            "Minghui Tang",
            "Mingxu Zhou",
            "Panpan Huang",
            "Peixin Cong",
            "Peiyi Wang",
            "Qiancheng Wang",
            "Qihao Zhu",
            "Qingyang Li",
            "Qinyu Chen",
            "Qiushi Du",
            "Ruiling Xu",
            "Ruiqi Ge",
            "Ruisong Zhang",
            "Ruizhe Pan",
            "Runji Wang",
            "Runqiu Yin",
            "Runxin Xu",
            "Ruomeng Shen",
            "Ruoyu Zhang",
            "S. H. Liu",
            "Shanghao Lu",
            "Shangyan Zhou",
            "Shanhuang Chen",
            "Shaofei Cai",
            "Shaoyuan Chen",
            "Shengding Hu",
            "Shengyu Liu",
            "Shiqiang Hu",
            "Shirong Ma",
            "Shiyu Wang",
            "Shuiping Yu",
            "Shunfeng Zhou",
            "Shuting Pan",
            "Songyang Zhou",
            "Tao Ni",
            "Tao Yun",
            "Tian Pei",
            "Tian Ye",
            "Tianyuan Yue",
            "Wangding Zeng",
            "Wen Liu",
            "Wenfeng Liang",
            "Wenjie Pang",
            "Wenjing Luo",
            "Wenjun Gao",
            "Wentao Zhang",
            "Xi Gao",
            "Xiangwen Wang",
            "Xiaoling Bi",
            "Xiaodong Liu",
            "Xiaohan Wang",
            "Xiaokang Chen",
            "Xiaokang Zhang",
            "X. Nie",
            "Xin Cheng",
            "Xin Liu",
            "Xin Xie",
            "Xingchao Liu",
            "Xingkai Yu",
            "Xingyou Li",
            "Xinyu Yang",
            "Xinyuan Li",
            "Xu Chen",
            "Xuecheng Su",
            "Xuehai Pan",
            "Xuheng Lin",
            "Xuwei Fu",
            "Y. Q. Wang",
            "Yang Zhang",
            "Yanhong Xu",
            "Yanru Ma",
            "Yao Li",
            "Yao Zhao",
            "Yaofeng Sun",
            "Yaohui Wang",
            "Yi Qian",
            "Yi Yu",
            "Yichao Zhang",
            "Yifan Ding",
            "Yifan Shi",
            "Yiliang Xiong",
            "Ying He",
            "Ying Zhou",
            "Yinmin Zhong",
            "Yishi Piao",
            "Yisong Wang",
            "Yixiao Chen",
            "Yixuan Tan",
            "Yixuan Wei",
            "Yiyang Ma",
            "Yiyuan Liu",
            "Yonglun Yang",
            "Yongqiang Guo",
            "Yongtong Wu",
            "Yu Wu",
            "Yuan Cheng",
            "Y. Ou",
            "Yuanfan Xu",
            "Yuduan Wang",
            "Yue Gong",
            "Yuhan Wu",
            "Yuheng Zou",
            "Yukun Li",
            "Yunfan Xiong",
            "Yuxiang Luo",
            "Yu-mei You",
            "Yuxuan Liu",
            "Yuyang Zhou",
            "Z. F. Wu",
            "Z. Z. Ren",
            "Zehua Zhao",
            "Zehui Ren",
            "Zhangli Sha",
            "Zhe Fu",
            "Zhean Xu",
            "Zhenda Xie",
            "Zhen-guo Zhang",
            "Zhewen Hao",
            "Zhibin Gou",
            "Zhicheng Ma",
            "Zhigang Yan",
            "Zhihong Shao",
            "Zhixian Huang",
            "Zhiyu Wu",
            "Zhuoshu Li",
            "Zhuping Zhang",
            "Zian Xu",
            "Zihao Wang",
            "Zihui Gu",
            "Zijia Zhu",
            "Zilin Li",
            "Zipeng Zhang",
            "Ziwei Xie",
            "Ziyi Gao",
            "Zizheng Pan",
            "Zongqing Yao",
            "Bei Feng",
            "Hui Li",
            "J. L. Cai",
            "Jiaqi Ni",
            "Lei Xu",
            "Meng Li",
            "Ning Tian",
            "R. J. Chen",
            "R. Jin",
            "S. S. Li",
            "Shuang Zhou",
            "Tianyu Sun",
            "X. Q. Li",
            "Xiangyue Jin",
            "Xiaojin Shen",
            "Xiaosha Chen",
            "Xinnan Song",
            "Xinyi Zhou",
            "Y. X. Zhu",
            "Yanping Huang",
            "Yaohui Li",
            "Yi Zheng",
            "Yuchen Zhu",
            "Yunxiang Ma",
            "Zhen Huang",
            "Zhipeng Xu",
            "Zhongyu Zhang",
            "Dong-Li Ji",
            "Jian Liang",
            "Jianzhong Guo",
            "Jin Chen",
            "Leyi Xia",
            "Miaojun Wang",
            "Mingming Li",
            "Peng Zhang",
            "Ruyi Chen",
            "Shangmian Sun",
            "Shaoqing Wu",
            "Shengfeng Ye",
            "T.Wang",
            "W. L. Xiao",
            "Wei An",
            "Xianzu Wang",
            "Xiaowen Sun",
            "Xiaoxiang Wang",
            "Ying Tang",
            "Y. Zha",
            "Zekai Zhang",
            "Zhenghua Ju",
            "Zhen Zhang",
            "Zihua Qu"
        ],
        "arxiv_id": "2512.02556",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
        "abstract_summary_gcp": "DeepSeek-V3.2 is a new model that prioritizes high computational efficiency alongside superior reasoning and agent performance. Its core innovations include:\n\n1.  **DeepSeek Sparse Attention (DSA):** An efficient attention mechanism that significantly reduces computational complexity while maintaining performance in long-context scenarios.\n2.  **Scalable Reinforcement Learning Framework:** Through a robust RL protocol and scaled post-training, DeepSeek-V3.2 performs comparably to GPT-5. A high-compute variant, DeepSeek-V3.2-Speciale, reportedly surpasses GPT-5 and matches Gemini-3.0-Pro's reasoning, achieving gold medals in the 2025 IMO and IOI.\n3.  **Large-Scale Agentic Task Synthesis Pipeline:** A novel pipeline that systematically generates training data at scale to integrate reasoning into tool-use scenarios, enhancing generalization and instruction-following robustness in complex environments.",
        "url": "https://www.semanticscholar.org/paper/1dc2281d4df5bd34f66aeb10e5b4741a27e23a9a",
        "isOpenAccess": false
    },
    "Zero-shot realistic image deblurring with consistency model": {
        "title": "Zero-shot realistic image deblurring with consistency model",
        "authors": [
            "Zhaohan Wang",
            "Chengjun Chen",
            "Chenggang Dai"
        ],
        "arxiv_id": null,
        "venue": "Complex & Intelligent Systems",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/175b605fc33a0ec790d1935565d2a8e6c15eba91",
        "isOpenAccess": false
    },
    "Integrating histopathology and genomic data: a comparative study of fusion methods for breast cancer survival prediction": {
        "title": "Integrating histopathology and genomic data: a comparative study of fusion methods for breast cancer survival prediction",
        "authors": [
            "Younes Akbari",
            "F. Abdullakutty",
            "Somaya Al Maadeed",
            "Ahmed Bouridane",
            "Rifat Hamoudi"
        ],
        "arxiv_id": null,
        "venue": "Complex & Intelligent Systems",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/718792e5ac433904796456aa410bac94f3f45283",
        "isOpenAccess": false
    },
    "Docsentinet: a adaptive architecture for efficient document-level sentiment analysis": {
        "title": "Docsentinet: a adaptive architecture for efficient document-level sentiment analysis",
        "authors": [
            "Xiaoyang Wang",
            "Wenfeng Liu",
            "Yuzhen Yang",
            "Yaling Gao",
            "Qiaoqiao Du",
            "Longqing Bao"
        ],
        "arxiv_id": null,
        "venue": "International Journal of Data Science and Analysis",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/477ff61f4884b5e57f715aac25d0b2dd9403aef4",
        "isOpenAccess": false
    },
    "2512.02535": {
        "title": "AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning",
        "authors": [
            "Jeric Lew",
            "Yuhong Cao",
            "Derek Ming Siang Tan",
            "G. Sartoretti"
        ],
        "arxiv_id": "2512.02535",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as\"intent\"to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.",
        "abstract_summary_gcp": "This paper introduces **AID**, a fully decentralized multi-agent informative path planning (MAIPP) framework designed for large-scale or time-critical information gathering scenarios (e.g., environmental monitoring, search and rescue).\n\nMAIPP traditionally involves coordinating multiple agents to maximize information gain within budget constraints, but a central challenge is maintaining effective coordination as the environment's belief state evolves. Existing learning-based approaches often rely on computationally expensive and error-prone autoregressive \"intent\" predictors for coordination.\n\nAID addresses this by leveraging **diffusion models** to generate long-term trajectories in a **non-autoregressive** manner. Its training pipeline involves two stages:\n1.  **Behavior Cloning:** Initial learning from trajectories produced by existing MAIPP planners.\n2.  **Reinforcement Learning Fine-tuning:** Optimization using Diffusion Policy Policy Optimization (DPPO) to improve coordination through online reward feedback.\n\nExperiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving significant benefits: up to **4x faster execution**, up to **17% increased information gain**, and effective scaling to a larger number of agents.",
        "url": "https://www.semanticscholar.org/paper/9ea5ef130fb10e3b6794b06838da529710b00efc",
        "isOpenAccess": false
    },
    "2512.02447": {
        "title": "Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors",
        "authors": [
            "Fan Luo",
            "Zeyu Gao",
            "Xinhao Luo",
            "Kai Zhao",
            "Yanfeng Lu"
        ],
        "arxiv_id": "2512.02447",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs'capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.",
        "abstract_summary_gcp": "This paper introduces the **Temporal Dynamics Enhancer (TDE)** to overcome a limitation in existing Spiking Neural Networks (SNNs): their inability to effectively model temporal information due to redundant input stimuli across time steps. Current SNNs often replicate inputs directly or aggregate them into fixed frames, which severely restricts their expressive power, especially for complex tasks like object detection.\n\nTDE addresses this by:\n1.  **Spiking Encoder (SE):** Generates diverse input stimuli at each time step, preventing neurons from receiving nearly identical signals repeatedly.\n2.  **Attention Gating Module (AGM):** Guides the SE's generation process by leveraging inter-temporal dependencies, ensuring relevant and dynamic input.\n\nTo mitigate the high-energy cost of AGM's multiplication operations, the authors also propose **Spike-Driven Attention (SDA)**, significantly reducing attention-related energy consumption.\n\nExtensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors, consistently outperforming state-of-the-art methods. It achieves mAP50-95 scores of 57.7% on the PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. Furthermore, the SDA module proves highly energy-efficient, consuming only 0.240 times the energy of conventional attention modules.",
        "url": "https://www.semanticscholar.org/paper/23592edadb4751d29ae948626c7f2e6243ed505e",
        "isOpenAccess": false
    },
    "2512.02619": {
        "title": "Quantum LLMs Using Quantum Computing to Analyze and Process Semantic Information",
        "authors": [
            "Timo Aukusti Laine"
        ],
        "arxiv_id": "2512.02619",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Physics"
        ],
        "abstract": "We present a quantum computing approach to analyzing Large Language Model (LLM) embeddings, leveraging complex-valued representations and modeling semantic relationships using quantum mechanical principles. By establishing a direct mapping between LLM semantic spaces and quantum circuits, we demonstrate the feasibility of estimating semantic similarity using quantum hardware. One of the key results is the experimental calculation of cosine similarity between Google Sentence Transformer embeddings using a real quantum computer, providing a tangible demonstration of a quantum approach to semantic analysis. This work reveals a connection between LLMs and quantum mechanics, suggesting that these principles can offer new perspectives on semantic representation and processing, and paving the way for future development of quantum algorithms for natural language processing.",
        "abstract_summary_gcp": "This work presents a quantum computing approach for analyzing Large Language Model (LLM) embeddings. It maps LLM semantic spaces to quantum circuits, using complex-valued representations and quantum mechanical principles to model semantic relationships. A key achievement is the experimental calculation of cosine similarity between Google Sentence Transformer embeddings on a *real quantum computer*, demonstrating the feasibility of using quantum hardware for semantic analysis. This research highlights a novel connection between LLMs and quantum mechanics, suggesting new avenues for understanding semantic representation and paving the way for future quantum algorithms in natural language processing.",
        "url": "https://www.semanticscholar.org/paper/6f8dec804dfe6a205219a5a27111bb572b11a4b2",
        "isOpenAccess": false
    },
    "2512.02713": {
        "title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs",
        "authors": [
            "Theodoros Aivalis",
            "I. Klampanos",
            "Antonis Troumpoukis",
            "J. Jose"
        ],
        "arxiv_id": "2512.02713",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.",
        "abstract_summary_gcp": "This paper introduces a framework to address concerns around transparency, accountability, and copyright in powerful generative models, specifically by explaining how training data influences model output. The core of the method involves interpreting generative outputs through the automatic construction of **ontology-aligned knowledge graphs (KGs)**.\n\nRecognizing the difficulty of extracting structured, ontology-consistent information from complex visual content, the framework leverages **multimodal large language models (LLMs)**. These LLMs are used to extract structured triples (subject-predicate-object relationships) directly from images, ensuring alignment with a predefined domain-specific ontology.\n\nBy comparing the KGs of generated images with those of their training counterparts, the system can trace potential influences from the training data. This capability provides benefits such as improved copyright analysis, enhanced dataset transparency, and more interpretable AI. The method's effectiveness is validated through experiments on both locally trained models (via unlearning) and large-scale models (through a style-specific analysis). Ultimately, this framework aims to foster human collaboration, creativity, and curiosity in AI development.",
        "url": "https://www.semanticscholar.org/paper/a5c9fe3dadf8a0deafb5bb96288c5a3480612ca5",
        "isOpenAccess": false
    },
    "2512.02321": {
        "title": "LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems",
        "authors": [
            "Yuanhe Zhang",
            "Weiliu Wang",
            "Zhenhong Zhou",
            "Kun Wang",
            "Jie Zhang",
            "Li Sun",
            "Yang Liu",
            "Sen Su"
        ],
        "arxiv_id": "2512.02321",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.",
        "abstract_summary_gcp": "This paper identifies a critical security vulnerability in LLM-based agents utilizing the Model Context Protocol (MCP) – the implicit trust placed in third-party tools. It introduces a new class of attacks called \"implicit toxicity,\" where malicious actions occur within the agent's legitimate privilege scope.\n\nThe authors propose **LeechHijack**, a Latent Embedded Exploit for Computation Hijacking. This attack involves two stages:\n1.  **Implantation:** A benign-appearing backdoor is embedded within an MCP tool.\n2.  **Exploitation:** Upon activation by specific triggers, the backdoor establishes a command-and-control channel. Through this channel, an attacker injects additional, unauthorized tasks that the agent executes, effectively usurping the user's computational resources.\n\nExperiments across four LLM families show that LeechHijack achieves a 77.25% success rate, incurring an average resource overhead of 18.62%. The study concludes by emphasizing the urgent need for computational provenance and resource attestation to secure the MCP ecosystem.",
        "url": "https://www.semanticscholar.org/paper/c7463808b05cc4dd058a5b5073c823dd2a69ab8a",
        "isOpenAccess": false
    },
    "2512.02764": {
        "title": "PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models",
        "authors": [
            "Róbert Belanec",
            "Ivan Srba",
            "M. Bieliková"
        ],
        "arxiv_id": "2512.02764",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory",
        "abstract_summary_gcp": "PEFT-Factory is a unified, modular framework designed to address the challenges of replicating, deploying, and comparing Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs). It provides a ready-to-use, controlled environment that supports both off-the-shelf and custom PEFT techniques. The framework natively includes 19 PEFT methods, 27 datasets for 12 classification and text generation tasks, and a range of standard and PEFT-specific evaluation metrics, thereby improving the replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream project of LLaMA-Factory and is publicly available on GitHub.",
        "url": "https://www.semanticscholar.org/paper/6cb4771076b56673ad54a966635bf585ec641923",
        "isOpenAccess": false
    },
    "2512.02727": {
        "title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions",
        "authors": [
            "Yifan Zhou",
            "Takehiko Ohkawa",
            "Guwenxiao Zhou",
            "Kanoko Goto",
            "Takumi Hirose",
            "Yusuke Sekikawa",
            "Nakamasa Inoue"
        ],
        "arxiv_id": "2512.02727",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.",
        "abstract_summary_gcp": "This paper introduces **Deformable Mamba (DF-Mamba)**, a novel and efficient framework for visual feature extraction in 3D Hand Pose Estimation (HPE), specifically designed to overcome challenges posed by severe hand occlusions.\n\nThe core problem is that existing 3D HPE methods, often based on ResNet, struggle with global context modeling, which is crucial for understanding occluded hands by relating local features to broader inter-joint, inter-hand, or scene cues.\n\nDF-Mamba addresses this by leveraging recent state space modeling (Mamba) and a proposed **deformable state scanning** mechanism. This allows it to capture global context beyond standard convolutions by selectively aggregating local features and preserving useful global cues.\n\nThe framework significantly improves 3D HPE accuracy, achieving state-of-the-art performance. It was extensively evaluated on five diverse datasets, including single-hand, two-hand, hand-object, RGB, and depth-based scenarios, outperforming other advanced Mamba-based backbones (VMamba, Spatial-Mamba). Importantly, DF-Mamba maintains an inference speed comparable to ResNet-50.",
        "url": "https://www.semanticscholar.org/paper/4a4617355a258f0c6e3ae9fe94cec3ce8faeb927",
        "isOpenAccess": false
    },
    "2512.02633": {
        "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations",
        "authors": [
            "Mattia Giuri",
            "Mathias Jackermeier",
            "Alessandro Abate"
        ],
        "arxiv_id": "2512.02633",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.",
        "abstract_summary_gcp": "This paper addresses a limitation in current Reinforcement Learning (RL) approaches that use Linear Temporal Logic (LTL) to specify complex tasks. While LTL, interpreted as finite automata, enables learning a single generalist policy for arbitrary instructions, existing methods falter in environments where multiple high-level events (atomic propositions) are simultaneously true and interact.\n\nThe authors propose a novel method that conditions the RL policy on **sequences of simple Boolean formulae**, which directly correspond to transitions in the LTL automaton. These Boolean formulae are then encoded using a **Graph Neural Network (GNN)** to create structured task representations. This approach allows for learning a multi-task policy that can handle the complexities of concurrent and interacting events, demonstrating its effectiveness in a challenging chess-based environment.",
        "url": "https://www.semanticscholar.org/paper/06542ce9cd5309c7117dfa0b421764ce15069bdb",
        "isOpenAccess": false
    },
    "2512.03018": {
        "title": "AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry",
        "authors": [
            "Xiang Xu",
            "P. Jayaraman",
            "J. Lambourne",
            "Yilin Liu",
            "Durvesh Malpure",
            "Pete Meltzer"
        ],
        "arxiv_id": "2512.03018",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.",
        "abstract_summary_gcp": "The paper introduces **AutoBrep**, a novel Transformer model designed to overcome the challenge of directly generating precise and watertight Boundary Representations (B-Reps) in CAD. B-Reps are the standard data structure for solid models, but end-to-end generation with high quality and validity has been difficult.\n\nAutoBrep addresses this through a **unified tokenization scheme** that encodes both the geometric and topological characteristics of a B-Rep model into a sequence of discrete tokens. Geometric primitives (surfaces and curves) are represented as latent geometry tokens, while their structural relationships are captured by special topological reference tokens. The model autoregressively generates these B-Reps, following a breadth-first traversal of the B-Rep's face adjacency graph. During inference, it progressively generates neighboring faces, edges, and their topological structure using next-token prediction.\n\nExperiments show that AutoBrep outperforms baseline methods, producing B-Reps with better quality and watertightness. It also demonstrates high scalability for complex solids, maintaining good fidelity and inference speed. Additionally, its unified tokenization natively supports B-Rep autocompletion, enabling user-controllable CAD generation.",
        "url": "https://www.semanticscholar.org/paper/5c085e6bd81d651cb5e9241af162a7d81ebfafbd",
        "isOpenAccess": false
    },
    "2512.02567": {
        "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System",
        "authors": [
            "Martin Weiss",
            "Jesko Hecking-Harbusch",
            "Jochen Quante",
            "Matthias Woehrle"
        ],
        "arxiv_id": "2512.02567",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes. We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables. Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.",
        "abstract_summary_gcp": "This paper investigates the reliability of strong generative AI in automated software engineering tasks, specifically focusing on a C-to-Rust code translation system.\n\nThe translation system uses a \"generate-and-check\" approach: an LLM generates Rust code from C, which is then automatically checked for compilability and behavioral equivalence. If checks fail, the LLM is re-prompted in an automated feedback loop to correct its output.\n\nThe study examines the impact of three factors on translation quality:\n1.  **Automated feedback loops**\n2.  **The choice of Large Language Model (LLM)**\n3.  **Behavior-preserving code changes (perturbations)**\n\nKey findings include:\n*   Without feedback loops, LLM selection significantly affects translation success.\n*   However, when feedback loops are employed, the performance differences between various LLMs diminish considerably. This holds true for both average performance and robustness against code perturbations.\n*   Surprisingly, the diversity introduced by code perturbations can, in some cases, even lead to improved system performance.",
        "url": "https://www.semanticscholar.org/paper/e016ff9921d01ae1102d7a555dc95d7c82a26aca",
        "isOpenAccess": false
    },
    "2512.03001": {
        "title": "Invasive Context Engineering to Control Large Language Models",
        "authors": [
            "Thomas Rivasseau"
        ],
        "arxiv_id": "2512.03001",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.",
        "abstract_summary_gcp": "Current research enhances Large Language Model (LLM) robustness against adversarial attacks and misbehavior through methods like preference training, prompting, and input/output filtering. However, LLMs remain vulnerable to abuse, with jailbreak probability increasing with context length, highlighting a need for stronger security in long-context scenarios.\n\nTo address this, the authors propose \"Invasive Context Engineering\" (ICE): inserting control sentences directly into the LLM's context. This technique can partially mitigate long-context vulnerabilities and be extended to Chain-of-Thought processes to prevent \"scheming.\" A key advantage of ICE is that it doesn't require LLM retraining, thus avoiding the data shortage issues common when training models for long-context situations.",
        "url": "https://www.semanticscholar.org/paper/0c4b43949b314d225227966b70c92d43f01af886",
        "isOpenAccess": false
    },
    "2512.02743": {
        "title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection",
        "authors": [
            "Shuonan Yang",
            "Tailin Chen",
            "Jiangbei Yue",
            "Guangliang Cheng",
            "Jianbo Jiao",
            "Zeyu Fu"
        ],
        "arxiv_id": "2512.02743",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.",
        "abstract_summary_gcp": "This paper addresses the growing threat of hate speech in online videos, where existing methods struggle with complex multimodal semantic fusion and understanding nuanced hateful content. The authors propose the **Reasoning-Aware Multimodal Fusion (RAMF)** framework.\n\nRAMF tackles these challenges by:\n1.  **Improving Multimodal Fusion:** It uses **Local-Global Context Fusion (LGCF)** to capture both local salient cues and global temporal structures, and **Semantic Cross Attention (SCA)** for fine-grained multimodal semantic interaction.\n2.  **Enhancing Nuance Understanding:** It introduces **adversarial reasoning**, a three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences. This provides complementary perspectives to better grasp nuanced hateful intent.\n\nEvaluations on two real-world hateful video datasets demonstrate that RAMF achieves robust generalization, outperforming state-of-the-art methods by 3% in Macro-F1 and 7% in hate class recall.",
        "url": "https://www.semanticscholar.org/paper/4958080badf6fc8774f477a4bc2c157472547120",
        "isOpenAccess": false
    },
    "2512.02863": {
        "title": "Leveraging generative adversarial networks with spatially adaptive denormalization for multivariate stochastic seismic data inversion",
        "authors": [
            "R. Miele",
            "Leonardo Azevedo"
        ],
        "arxiv_id": "2512.02863",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Physics",
            "Computer Science",
            "Mathematics"
        ],
        "abstract": "Probabilistic seismic inverse modeling often requires the prediction of both spatially correlated geological heterogeneities (e.g., facies) and continuous parameters (e.g., rock and elastic properties). Generative adversarial networks (GANs) provide an efficient training-image-based simulation framework capable of reproducing complex geological models with high accuracy and comparably low generative cost. However, their application in stochastic geophysical inversion for multivariate property prediction is limited, as representing multiple coupled properties requires large and unstable networks with high memory and training demands. A more recent variant of GANs with spatially adaptive denormalization (SPADE-GAN) enables the direct conditioning of facies spatial distributions on local probability maps. Leveraging on such features, an iterative geostatistical inversion algorithm is proposed, SPADE-GANInv, integrating a pre-trained SPADE-GAN with geostatistical simulation, for the prediction of facies and multiple correlated continuous properties from seismic data. The SPADE-GAN is trained to reproduce realistic facies geometries, while sequential stochastic co-simulation predicts the spatial variability of the facies-dependent continuous properties. At each iteration, a set of subsurface realizations is generated and used to compute synthetic seismic data. The realizations providing the highest similarity coefficient to the observed data are used to update the subsurface probability models in the next iteration. The method is demonstrated on both 2-D synthetic scenarios and field data, targeting the prediction of facies, porosity, and acoustic impedance from full-stack seismic data. Results show that the algorithm enables accurate multivariate prediction, mitigates the impact of biased prior data, and accommodates additional local conditioning such as well logs.",
        "abstract_summary_gcp": "This paper introduces **SPADE-GANInv**, an iterative geostatistical inversion algorithm designed for probabilistic seismic inverse modeling. The method addresses the challenge of predicting both spatially correlated geological facies and continuous rock properties (like porosity or impedance) from seismic data.\n\nTraditional Generative Adversarial Networks (GANs), while efficient for simulating complex geology, struggle with multivariate property prediction due to their size and instability. SPADE-GANs, a newer variant, overcome some of these limitations by enabling direct conditioning of facies distributions on local probability maps.\n\nSPADE-GANInv leverages this capability by integrating a pre-trained SPADE-GAN (to generate realistic facies geometries) with sequential stochastic co-simulation (to predict facies-dependent continuous properties). The algorithm operates iteratively:\n1.  It generates multiple subsurface realizations.\n2.  Computes synthetic seismic data from these realizations.\n3.  Compares the synthetic data to observed seismic data using a similarity coefficient.\n4.  Uses the best-matching realizations to update the subsurface probability models for the next iteration.\n\nDemonstrated on both synthetic and field data for predicting facies, porosity, and acoustic impedance from full-stack seismic, SPADE-GANInv achieves accurate multivariate prediction. It also effectively mitigates the impact of biased prior data and can incorporate additional local conditioning from well logs.",
        "url": "https://www.semanticscholar.org/paper/6892a02efab33f7bd2f7c46e6448e9dde61d9c16",
        "isOpenAccess": false
    },
    "2512.02677": {
        "title": "Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks",
        "authors": [
            "Zhiyuan He"
        ],
        "arxiv_id": "2512.02677",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.",
        "abstract_summary_gcp": "This paper investigates a critical and underexplored limitation of Large Language Models (LLMs): **depth generalization**, their ability to handle problems with more nested hierarchical levels (e.g., parentheses in an expression) than seen during training.\n\nWhile LLMs perform well on many tasks and can generalize to longer sequences (length generalization), standard transformer architectures struggle significantly with deeper recursion, even if the overall sequence length is comparable to training data. This performance decay is attributed to their inability to maintain stack-like behavior, essential for tracking and resolving nested dependencies.\n\nTo address this, the authors propose a novel **looped locate-and-replace pipeline**. This method decomposes recursive problems into manageable subcomponents using two specialized models: a \"locator\" to identify solvable subexpressions and a \"replacer\" to evaluate them while preserving the overall structure.\n\nEvaluated in three domains (Boolean algebra, recursive arithmetic, and propositional logic) with controllable recursion depth, the proposed method effectively mitigates the performance decay observed in LLMs when faced with out-of-distribution recursion depths.",
        "url": "https://www.semanticscholar.org/paper/23673e4c973a4ab21d6cd433e0738c0054778c09",
        "isOpenAccess": false
    },
    "2512.02557": {
        "title": "Deep Learning-Based Joint Uplink-Downlink CSI Acquisition for Next-Generation Upper Mid-Band Systems",
        "authors": [
            "Xuan He",
            "Hongwei Hou",
            "Yafei Wang",
            "Wenjin Wang",
            "Shi Jin",
            "S. Chatzinotas",
            "Björn E. Ottersten"
        ],
        "arxiv_id": "2512.02557",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Engineering"
        ],
        "abstract": "In next-generation wireless communication systems, the newly designated upper mid-band has attracted considerable attention, also called frequency range 3 (FR3), highlighting the need for downlink (DL) transmission design, which fundamentally relies on accurate CSI. However, CSI acquisition in FR3 systems faces significant challenges: the increased number of antennas and wider transmission bandwidth introduces prohibitive training overhead with traditional estimation approaches, as each probing captures only incomplete spatial-frequency observation, while higher carrier frequencies lead to faster temporal channel variation. To address these challenges, we propose a novel CSI acquisition framework that integrates CSI feedback, uplink (UL) and DL channel estimation, as well as channel prediction in the FR3 TDD massive MIMO systems. Specifically, we first develop the Joint UL and DL Channel Estimation Network (JUDCEN) to fuse incomplete observations based on the SRSs and CSI-RSs. By exploiting the complementary characteristics of preliminary UL and DL estimation features, obtained through initial UL estimation and quantized-feedback-assisted DL estimation, it enables full CSI reconstruction in the spatial domain. To mitigate the performance degradation in the feedback process, we propose the Transformer-MLP CSI Feedback Network (TMCFN), employing an MLP-based module to jointly exploit angle- and delay-domain features. Building upon the reconstructed full CSI, we further develop the Mamba-based Channel Prediction Network (MCPN), which exploits selective state-space model (SSM) mechanism to capture long-range temporal dynamics in the angle-delay domain for future CSI prediction. Simulation results demonstrate that the proposed framework consistently outperforms benchmarks in both CSI acquisition accuracy and transmission spectral efficiency with lower computational complexity.",
        "abstract_summary_gcp": "In next-generation Frequency Range 3 (FR3) massive MIMO systems, accurate Downlink (DL) Channel State Information (CSI) is vital but challenging to acquire. The difficulties stem from the large number of antennas and wide transmission bandwidth, leading to prohibitive training overhead with traditional methods, and faster temporal channel variations due to higher carrier frequencies.\n\nTo overcome these challenges, the authors propose a novel CSI acquisition framework for FR3 TDD massive MIMO systems. This framework integrates CSI feedback, uplink (UL) and DL channel estimation, and channel prediction, featuring three key components:\n\n1.  **Joint UL and DL Channel Estimation Network (JUDCEN):** This network fuses incomplete observations from Sounding Reference Signals (SRSs) and CSI Reference Signals (CSI-RSs). By exploiting the complementary characteristics of preliminary UL and quantized-feedback-assisted DL estimation, JUDCEN enables full CSI reconstruction in the spatial domain.\n2.  **Transformer-MLP CSI Feedback Network (TMCFN):** Designed to mitigate performance degradation during the feedback process, TMCFN uses an MLP-based module to jointly extract and leverage features from both the angle- and delay-domains.\n3.  **Mamba-based Channel Prediction Network (MCPN):** Building upon the reconstructed full CSI, MCPN utilizes a selective state-space model (SSM) mechanism to capture long-range temporal dynamics in the angle-delay domain, enabling accurate prediction of future CSI.\n\nSimulation results demonstrate that the proposed framework consistently outperforms benchmark approaches in terms of CSI acquisition accuracy, transmission spectral efficiency, and computational complexity.",
        "url": "https://www.semanticscholar.org/paper/88a2ff205fdb97c45b2170ee0d05d671f22791aa",
        "isOpenAccess": false
    },
    "2512.02710": {
        "title": "Beyond N-grams: A Hierarchical Reward Learning Framework for Clinically-Aware Medical Report Generation",
        "authors": [
            "Yuan Wang",
            "Shujian Gao",
            "Jiaxiang Liu",
            "Songtao Jiang",
            "Haoxiang Xia",
            "Xiaotian Zhang",
            "Zhaolu Kang",
            "Yemin Wang",
            "Zuozhu Liu"
        ],
        "arxiv_id": "2512.02710",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Automatic medical report generation can greatly reduce the workload of doctors, but it is often unreliable for real-world deployment. Current methods can write formally fluent sentences but may be factually flawed, introducing serious medical errors known as clinical hallucinations, which make them untrustworthy for diagnosis. To bridge this gap, we introduce HiMed-RL, a Hierarchical Medical Reward Learning Framework designed to explicitly prioritize clinical quality. HiMed-RL moves beyond simple text matching by deconstructing reward learning into three synergistic levels: it first ensures linguistic fluency at the token-level, then enforces factual grounding at the concept-level by aligning key medical terms with expert knowledge, and finally assesses high-level diagnostic consistency at the semantic-level using a specialized LLM verifier. This hierarchical reward is implemented via a Human-inspired Dynamic Reward Adjustment, a strategy which first teaches the model to learn basic facts before progressing to more complex diagnostic reasoning. Experimentally, HiMed-3B achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, particularly on the latter, with an improvement of 12.1% over the second-best baseline. Our work provides a robust paradigm for generating reports that not only improve fluency but clinical fine-grained quality.",
        "abstract_summary_gcp": "This paper introduces **HiMed-RL (Hierarchical Medical Reward Learning Framework)**, a solution designed to address the critical issue of \"clinical hallucinations\" – factual errors – in automatic medical report generation. While current methods can produce fluent text, their unreliability due to these errors makes them untrustworthy for diagnosis.\n\nHiMed-RL explicitly prioritizes clinical quality by deconstructing reward learning into three synergistic levels:\n1.  **Token-level:** Ensures linguistic fluency.\n2.  **Concept-level:** Enforces factual grounding by aligning key medical terms with expert knowledge.\n3.  **Semantic-level:** Assesses high-level diagnostic consistency using a specialized LLM verifier.\n\nThis hierarchical reward system is implemented via a **Human-inspired Dynamic Reward Adjustment**, which progressively teaches the model from basic facts to more complex diagnostic reasoning. Experimentally, HiMed-3B achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, demonstrating a significant 12.1% improvement over the second-best baseline on out-of-domain tasks. The work offers a robust paradigm for generating medical reports that are not only fluent but also clinically accurate and reliable.",
        "url": "https://www.semanticscholar.org/paper/1e4390393ee1daa02bf15bbdfc4f288811ab8377",
        "isOpenAccess": false
    },
    "2512.02712": {
        "title": "G-PIFNN: A Generalizable Physics-informed Fourier Neural Network Framework for Electrical Circuits",
        "authors": [
            "Ibrahim Shahbaz",
            "Mohammad J. Abdel-Rahman",
            "Eman Hammad"
        ],
        "arxiv_id": "2512.02712",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Engineering"
        ],
        "abstract": "Physics-Informed Neural Networks (PINNs) have advanced the data-driven solution of differential equations (DEs) in dynamic physical systems, yet challenges remain in explainability, scalability, and architectural complexity. This paper presents a Generalizable Physics-Informed Fourier Neural Network (G-PIFNN) framework that enhances PINN architectures for efficient and interpretable electrical circuit analysis. The proposed G-PIFNN introduces three key advancements: (1) improved performance and interpretability via a physics activation function (PAF) and a lightweight Physics-Informed Fourier Neural Network (PIFNN) architecture; (2) automated, bond graph (BG) based formulation of physics-informed loss functions for systematic differential equation generation; and (3) integration of intra-circuit and cross-circuit class transfer learning (TL) strategies, enabling unsupervised fine-tuning for rapid adaptation to varying circuit topologies. Numerical simulations demonstrate that G-PIFNN achieves significantly better predictive performance and generalization across diverse circuit classes, while significantly reducing the number of trainable parameters compared to standard PINNs.",
        "abstract_summary_gcp": "This paper introduces the Generalizable Physics-Informed Fourier Neural Network (G-PIFNN) framework, an advancement over traditional Physics-Informed Neural Networks (PINNs) designed to overcome challenges in explainability, scalability, and complexity when solving differential equations in dynamic physical systems, particularly for electrical circuit analysis.\n\nG-PIFNN incorporates three key innovations:\n1.  **Improved Performance and Interpretability:** Achieved through a lightweight Physics-Informed Fourier Neural Network (PIFNN) architecture and a novel Physics Activation Function (PAF).\n2.  **Automated Loss Function Formulation:** Utilizes a bond graph (BG)-based approach for systematic generation of physics-informed loss functions and differential equations.\n3.  **Enhanced Adaptability:** Integrates intra-circuit and cross-circuit class transfer learning strategies, allowing for rapid, unsupervised fine-tuning and adaptation to diverse circuit topologies.\n\nNumerical simulations confirm that G-PIFNN significantly boosts predictive performance and generalization across various circuit classes, while concurrently reducing the number of trainable parameters compared to standard PINNs.",
        "url": "https://www.semanticscholar.org/paper/d9016c53c2cf3370ccb3264b250bd8dc535c13a7",
        "isOpenAccess": false
    },
    "2512.02643": {
        "title": "Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening",
        "authors": [
            "Yongchuan Cui",
            "Peng Liu",
            "Yi Zeng"
        ],
        "arxiv_id": "2512.02643",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.",
        "abstract_summary_gcp": "Existing deep learning methods for remote sensing image fusion often struggle with poor generalization to new datasets due to limited real training data and the domain gap between different satellite sensors.\n\nTo address this, the authors propose a novel pretraining strategy that leverages foundation models. Their approach involves:\n1.  **Constructing large-scale simulated datasets:** This is done by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering) to both natural images (ImageNet) and remote sensing images (SkyScript).\n2.  **Pretraining fusion models:** Models are pretrained on these diverse simulated datasets to learn robust and generalizable spatial-spectral representations.\n\nThe pretrained models, including convolutional neural networks, Transformers, and Mamba architectures, were evaluated on six real-world datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2). Experiments demonstrated that this pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions. The models achieved superior results in zero-shot scenarios and showed remarkable adaptation capabilities with minimal real data in one-shot settings.\n\nThis work offers a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion, and paves the way for leveraging foundation models through advanced training strategies.",
        "url": "https://www.semanticscholar.org/paper/7e9e2dd7cea8ba7847f99d736e443e4afee22eed",
        "isOpenAccess": false
    },
    "2512.02419": {
        "title": "The brain-AI convergence: Predictive and generative world models for general-purpose computation",
        "authors": [
            "Shogo Ohmae",
            "Keiko Ohmae"
        ],
        "arxiv_id": "2512.02419",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Biology",
            "Computer Science"
        ],
        "abstract": "Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",
        "abstract_summary_gcp": "This Perspective proposes that recent advances in attention-based AI offer insights into how the neocortex and cerebellum, despite uniform circuit architectures, generate diverse functions and intelligence. It highlights a shared computational foundation: **world-model-based computation**.\n\nBoth the attention-based neocortex and the non-attentional cerebellum build internal world models by predicting future events from past inputs and employing prediction-error learning. These predictive models are then repurposed for distinct functions—understanding in sensory processing and generation in motor processing—enabling the brain's multi-domain capabilities and adaptive intelligence.\n\nNotably, attention-based AI has independently converged on this same learning paradigm and world-model-based computation. The authors conclude that these shared mechanisms constitute a core computational foundation for diverse functions and high-level intelligence in both biological and artificial systems, bridging neuroscience and AI.",
        "url": "https://www.semanticscholar.org/paper/472c12b4ba8fb8d256a603a9607ee37f98c385aa",
        "isOpenAccess": false
    },
    "2512.02368": {
        "title": "Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention",
        "authors": [
            "Wenyi Xiong",
            "Jian Chen"
        ],
        "arxiv_id": "2512.02368",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.",
        "abstract_summary_gcp": "This paper addresses the challenges in trajectory prediction for autonomous driving, particularly in complex interactive scenarios where existing methods struggle with redundant scene information, leading to reduced efficiency and accuracy.\n\nThe authors propose a novel **map-free trajectory prediction algorithm** that operates across temporal, spatial, and frequency domains. Key components include:\n1.  A **Mixture of Experts (MoE) mechanism** for temporal processing, which adaptively selects critical frequency components and integrates multi-scale temporal features.\n2.  A **selective attention module** designed to filter out redundant information from both temporal sequences and spatial interactions.\n3.  A **multimodal decoder** that, supervised by patch-level and point-level losses, generates reasonable trajectory results.\n\nExperiments on the NuScenes dataset demonstrate the algorithm's superior performance and effectiveness in handling complex interactive autonomous driving scenarios.",
        "url": "https://www.semanticscholar.org/paper/d3486f916e7a7b3694d7a617e4e9493f2e6ea644",
        "isOpenAccess": false
    },
    "2512.02558": {
        "title": "Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance",
        "authors": [
            "Yufei Xiao",
            "Shangfei Wang"
        ],
        "arxiv_id": "2512.02558",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Prevalent empathy prediction techniques primarily concentrate on a singular modality, typically textual, thus neglecting multi-modal processing capabilities. They also overlook the utilization of certain privileged information, which may encompass additional empathetic content. In response, we introduce an advanced multi-modal empathy prediction method integrating video, audio, and text information. The method comprises the Multi-Modal Empathy Prediction and Supervisory Documentation Assisted Training. We use pre-trained networks in the empathy prediction network to extract features from various modalities, followed by a cross-modal fusion. This process yields a multi-modal feature representation, which is employed to predict empathy labels. To enhance the extraction of text features, we incorporate supervisory documents as privileged information during the assisted training phase. Specifically, we apply the Latent Dirichlet Allocation model to identify potential topic distributions to constrain text features. These supervisory documents, created by supervisors, focus on the counseling topics and the counselor's display of empathy. Notably, this privileged information is only available during training and is not accessible during the prediction phase. Experimental results on the multi-modal and dialogue empathy datasets demonstrate that our approach is superior to the existing methods.",
        "abstract_summary_gcp": "Current empathy prediction methods are limited by their focus on single modalities (primarily text) and their neglect of \"privileged information.\" To overcome this, the paper introduces an advanced multi-modal empathy prediction method that integrates video, audio, and text.\n\nThe proposed approach consists of two main parts:\n1.  **Multi-Modal Empathy Prediction:** This network uses pre-trained models to extract features from video, audio, and text, which are then combined through cross-modal fusion to create a unified representation for predicting empathy.\n2.  **Supervisory Documentation Assisted Training:** To improve text feature extraction, \"supervisory documents\" (created by supervisors, detailing counseling topics and empathy displays) are utilized as privileged information *during the training phase only*. This involves applying Latent Dirichlet Allocation (LDA) to these documents to constrain and enhance the textual features.\n\nExperimental results on multi-modal and dialogue empathy datasets demonstrate that this new approach surpasses existing methods.",
        "url": "https://www.semanticscholar.org/paper/5d5a7e15bb4e8a1757b1d2b1d84a960a30f78c95",
        "isOpenAccess": false
    },
    "2512.03040": {
        "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
        "authors": [
            "Zeqi Xiao",
            "Yiwei Zhao",
            "Lingxiao Li",
            "Yushi Lan",
            "Yu Ning",
            "Rahul Garg",
            "Roshni Cooper",
            "M. H. Taghavi",
            "Xingang Pan"
        ],
        "arxiv_id": "2512.03040",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
        "abstract_summary_gcp": "This paper introduces **Video4Spatial**, a framework demonstrating that video generative models can exhibit visuospatial intelligence using *only visual data*.\n\nVideo4Spatial leverages video diffusion models conditioned solely on video-based scene context to perform complex spatial tasks. It is validated on two primary tasks:\n1.  **Scene Navigation:** Following camera-pose instructions while maintaining consistency with the 3D geometry of the scene.\n2.  **Object Grounding:** Requiring semantic localization, instruction following, and planning.\n\nCrucially, both tasks rely exclusively on video-only inputs, without any auxiliary modalities like depth maps or camera poses. Through simple yet effective design choices and data curation, Video4Spatial achieves strong spatial understanding, capable of planning navigation, grounding objects end-to-end, following camera-pose instructions consistently, and generalizing to long contexts and out-of-domain environments. These results significantly advance video generative models towards general visuospatial reasoning.",
        "url": "https://www.semanticscholar.org/paper/a46cf6f4a20c0c6e1beefb2d397d5f422d1281ab",
        "isOpenAccess": false
    },
    "2512.02789": {
        "title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking",
        "authors": [
            "Haonan Tang",
            "Yanjun Chen",
            "Jiang Lezhi"
        ],
        "arxiv_id": "2512.02789",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.",
        "abstract_summary_gcp": "TrackNetV5 is a new architecture for fast-moving small object tracking in sports, designed to overcome limitations of previous versions. Earlier iterations (V1-V3) struggled with occlusions due to their reliance on purely visual cues, while TrackNetV4, despite using motion, suffered from directional ambiguity by discarding motion polarity.\n\nTrackNetV5 introduces two key innovations:\n\n1.  **Motion Direction Decoupling (MDD) module:** This module recovers lost directional priors by decomposing temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction, unlike V4's absolute difference method.\n2.  **Residual-Driven Spatio-Temporal Refinement (R-STR) head:** A Transformer-based module that operates on a coarse-to-fine paradigm, leveraging factorized spatio-temporal contexts to estimate a corrective residual, which effectively recovers occluded targets.\n\nExtensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves state-of-the-art performance with an F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. This performance leap is achieved with only a marginal 3.7% increase in FLOPs compared to TrackNetV4, maintaining real-time inference capabilities.",
        "url": "https://www.semanticscholar.org/paper/3209d5a0d0602ef2f50c46f67c41d1a29fb0afd7",
        "isOpenAccess": false
    },
    "2512.02319": {
        "title": "Associative Memory using Attribute-Specific Neuron Groups-1: Learning between Multiple Cue Balls",
        "authors": [
            "Hiroshi Inazawa"
        ],
        "arxiv_id": "2512.02319",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In this paper, we present a new neural network model based on attribute-specific representations (e.g., color, shape, size), a classic example of associative memory. The proposed model is based on a previous study on memory and recall of multiple images using the Cue Ball and Recall Net (referred to as the CB-RN system, or simply CB-RN) [1]. The system consists of three components, which are C.CB-RN for processing color, S.CB-RN for processing shape, and V.CB-RN for processing size. When an attribute data pattern is presented to the CB-RN system, the corresponding attribute pattern of the cue neurons within the Cue Balls is associatively recalled in the Recall Net. Each image pattern presented to these CB-RN systems is represented using a two-dimensional code, specifically a QR code [2].",
        "abstract_summary_gcp": "This paper introduces a new neural network model designed for associative memory, specifically utilizing attribute-specific representations (like color, shape, and size). The model is an extension of the existing Cue Ball and Recall Net (CB-RN) system and is structured into three specialized components: C.CB-RN for color, S.CB-RN for shape, and V.CB-RN for size. When an attribute data pattern is presented, the system associatively recalls the corresponding attribute pattern via cue neurons within the Recall Net. All image patterns processed by these CB-RN systems are represented using two-dimensional QR codes.",
        "url": "https://www.semanticscholar.org/paper/98b8d232874c4271149106d6cfdc8e60e3d279f0",
        "isOpenAccess": false
    },
    "2512.02465": {
        "title": "TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links",
        "authors": [
            "Xingwang Li",
            "Mengyun Chen",
            "Jiamou Liu",
            "Sijie Wang",
            "Shuanggen Jin",
            "J. C. Andersson",
            "Jonas Olsson",
            "Remco van de Beek",
            "H. Habi",
            "Congzheng Han"
        ],
        "arxiv_id": "2512.02465",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.",
        "abstract_summary_gcp": "This paper proposes a novel hybrid deep learning model called TabGRU for high-resolution urban rainfall monitoring using Commercial Microwave Links (CMLs). Addressing the limitations of traditional physics-based models (signal noise, non-linear attenuation) in real-world scenarios, TabGRU combines a Transformer for long-term dependencies and a Bidirectional Gated Recurrent Unit (BiGRU) for local sequential features. It's further enhanced with learnable positional embedding and attention pooling for dynamic feature extraction and generalization.\n\nValidated on a benchmark dataset from Gothenburg, Sweden, TabGRU consistently outperformed deep learning baselines and achieved high coefficients of determination (R2) of 0.91 (Torp) and 0.96 (Barl). Crucially, it also demonstrated higher accuracy than physics-based approaches, particularly excelling at mitigating the overestimation problem during peak rainfall events. The study concludes that TabGRU offers a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.",
        "url": "https://www.semanticscholar.org/paper/8f04a236da522f237e27ba7126ea385401242a5d",
        "isOpenAccess": false
    },
    "2512.02392": {
        "title": "From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking",
        "authors": [
            "Yuqing Shao",
            "Yuchen Yang",
            "Rui Yu",
            "Weilong Li",
            "Xu Guo",
            "Huaicheng Yan",
            "Wei Wang",
            "Xiao Sun"
        ],
        "arxiv_id": "2512.02392",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.",
        "abstract_summary_gcp": "This paper addresses a key limitation in end-to-end multi-object tracking (MOT) methods: despite strong detection performance, they suffer from low association accuracy. The authors identify that shared DETR-based architectures produce object embeddings with excessively high inter-object similarity, as they primarily focus on category-level discrimination within single frames, rather than the instance-level distinction and spatio-temporal continuity essential for robust tracking.\n\nTo resolve this, they introduce FDTA (From Detection to Association), an explicit feature refinement framework designed to significantly enhance object discriminativeness. FDTA incorporates three complementary modules:\n1.  **Spatial Adapter (SA):** Integrates depth-aware cues to improve spatial continuity.\n2.  **Temporal Adapter (TA):** Aggregates historical information to strengthen temporal dependencies.\n3.  **Identity Adapter (IA):** Leverages quality-aware contrastive learning to boost instance-level separability.\n\nExtensive experiments show that FDTA achieves state-of-the-art performance on challenging MOT benchmarks like DanceTrack, SportsMOT, and BFT, demonstrating the effectiveness of its proposed discriminative embedding enhancement strategy.",
        "url": "https://www.semanticscholar.org/paper/7cdbbcccbf75d24b85c2e85d42a895e6483b4dd0",
        "isOpenAccess": false
    },
    "2512.02403": {
        "title": "ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity",
        "authors": [
            "Hongxiang Liu",
            "Zhifang Deng",
            "Tong Pu",
            "Shengli Lu"
        ],
        "arxiv_id": "2512.02403",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Transformers, composed of QKV generation, attention computation, and FFNs, have become the dominant model across various domains due to their outstanding performance. However, their high computational cost hinders efficient hardware deployment. Sparsity offers a promising solution, yet most existing accelerators exploit only intra-row sparsity in attention, while few consider inter-row sparsity. Approaches leveraging inter-row sparsity often rely on costly global similarity estimation, which diminishes the acceleration benefits of sparsity, and typically apply sparsity to only one or two transformer components. Through careful analysis of the attention distribution and computation flow, we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead. Motivated by this observation, we propose ESACT, an end-to-end sparse accelerator for compute-intensive Transformers. ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism, which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation, achieving efficient sparsity across all transformer components. To support efficient hardware realization, we introduce three architectural innovations. Experimental results on 26 benchmarks demonstrate that SPLS reduces total computation by 52.03% with less than 1% accuracy loss. ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W, and improves attention-level energy efficiency by 2.95x and 2.26x over SOTA attention accelerators SpAtten and Sanger, respectively.",
        "abstract_summary_gcp": "Transformers, while powerful, are computationally expensive, hindering efficient hardware deployment. Existing sparsity solutions are limited: most exploit only intra-row sparsity, while inter-row methods rely on costly global similarity estimation and typically apply sparsity to only a few components.\n\nThis paper observes that *local similarity* allows for more efficient end-to-end sparse acceleration. Based on this, they propose **ESACT**, an end-to-end sparse accelerator centered on the **Sparsity Prediction with Local Similarity (SPLS)** mechanism. SPLS uses HLog quantization to predict local attention sparsity *before* QK generation, enabling efficient sparsity across *all* Transformer components. ESACT also incorporates three architectural innovations for hardware efficiency.\n\nExperiments demonstrate that SPLS reduces total computation by 52.03% with less than 1% accuracy loss. ESACT achieves 3.29 TOPS/W end-to-end energy efficiency and significantly outperforms state-of-the-art attention accelerators in energy efficiency (2.95x over SpAtten and 2.26x over Sanger).",
        "url": "https://www.semanticscholar.org/paper/83e332b5133411ff0b75de332606be72f8095ab7",
        "isOpenAccess": false
    },
    "2512.02700": {
        "title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm",
        "authors": [
            "Zhenkai Wu",
            "Xiaowen Ma",
            "Zhenliang Ni",
            "Dengming Zhang",
            "Han Shu",
            "Xin Jiang",
            "Xinghao Chen"
        ],
        "arxiv_id": "2512.02700",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup.",
        "abstract_summary_gcp": "This paper introduces **VLM-Pruner**, a training-free token pruning algorithm designed to optimize Vision-Language Models (VLMs) by significantly reducing computational costs without sacrificing performance.\n\nThe core problem VLM-Pruner addresses is the inefficiency of existing pruning methods. Current approaches either:\n1.  Focus solely on token importance, leading to the retention of redundant tokens and wasted capacity.\n2.  Are redundancy-aware but ignore spatial relationships, resulting in overly sparse selections that fail to adequately cover target objects.\n\nVLM-Pruner tackles these limitations by explicitly balancing redundancy and spatial sparsity through several key innovations:\n*   **Centrifugal Token Pruning Paradigm:** It employs a novel near-to-far selection strategy that prioritizes the preservation of fine-grained object details.\n*   **Buffering for Spatial Sparsity (BSS) Criterion:** This mechanism intelligently defers the selection of spatially distant tokens, ensuring better coverage of object regions.\n*   **Parallel Greedy Strategy:** This is adopted for efficient token selection.\n*   **Information Fusion:** To mitigate potential information loss from discarded tokens, VLM-Pruner selectively fuses salient information from them into the retained tokens.\n\nComprehensive evaluations demonstrate that VLM-Pruner consistently outperforms strong baseline methods across five different VLMs, achieving an impressive **88.9% pruning rate** while simultaneously delivering significant **end-to-end inference speedup**.",
        "url": "https://www.semanticscholar.org/paper/31b2ace1b3c7ea66db054dadf79c67947f937e68",
        "isOpenAccess": false
    },
    "2512.02369": {
        "title": "SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains",
        "authors": [
            "Qingmei Li",
            "Yang Zhang",
            "Peifeng Zhang",
            "Haohuan Fu",
            "Juepeng Zheng"
        ],
        "arxiv_id": "2512.02369",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \\textbf{S}tyle-\\textbf{A}daptive \\textbf{GE}neralization framework (\\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.",
        "abstract_summary_gcp": "This paper addresses the challenge of domain generalization for semantic segmentation, particularly when model parameters and architectural details are inaccessible due to privacy and security constraints. Traditional fine-tuning is impossible under these conditions, necessitating input-level strategies.\n\nThe authors propose the **Style-Adaptive GEneralization framework (SAGE)**, designed to improve the generalization of *frozen* models. Instead of modifying model weights, SAGE learns to synthesize dynamic visual prompts that implicitly align feature distributions across different styles.\n\nSAGE operates in two main steps:\n1.  **Diverse Style Representation:** It first utilizes style transfer to construct a broad representation of style characteristics from the source domain.\n2.  **Adaptive Prompt Synthesis:** It then adaptively fuses these learned style cues based on the visual context of each input, generating a dynamic prompt that harmonizes the image's appearance *before* it enters the frozen model.\n\nThis closed-loop, input-level approach effectively bridges the gap between a frozen model's fixed nature and the diversity of unseen domains. Extensive experiments on five benchmark datasets show that SAGE achieves competitive or superior performance against state-of-the-art methods under privacy constraints, and remarkably, even surpasses full fine-tuning baselines in all evaluated settings.",
        "url": "https://www.semanticscholar.org/paper/fa72855efee1288b49208eb9fed73a32578108af",
        "isOpenAccess": false
    },
    "2512.02342": {
        "title": "Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients",
        "authors": [
            "Dimitris Oikonomou",
            "Nicolas Loizou"
        ],
        "arxiv_id": "2512.02342",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Mathematics",
            "Computer Science"
        ],
        "abstract": "The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. Comprehensive experiments on convex benchmarks and deep neural networks corroborate our theory: the proposed step size accelerates convergence, reduces variance, and consistently outperforms existing adaptive baselines. Finally, in the context of deep neural network training, our method demonstrates robust performance by addressing the vanishing gradient problem.",
        "abstract_summary_gcp": "This paper addresses the limitations of the Stochastic Polyak Step Size (SPS) in non-smooth optimization. While SPS is effective for smooth problems and deep learning, its non-smooth extensions typically rely on strong assumptions or require knowledge of the optimal solution.\n\nThe authors propose **Safeguarded SPS (SPS$_{safe}$)**, a novel variant for the stochastic subgradient method. SPS$_{safe}$ offers rigorous convergence guarantees for non-smooth convex optimization without requiring strong assumptions. They further incorporate momentum into the update rule, maintaining equally tight theoretical results.\n\nExperimental evaluations on convex benchmarks and deep neural networks demonstrate that SPS$_{safe}$ accelerates convergence, reduces variance, and consistently outperforms existing adaptive baselines. Notably, in deep neural network training, the method also exhibits robust performance by effectively addressing the vanishing gradient problem.",
        "url": "https://www.semanticscholar.org/paper/2f2b92685f43952d0cf1eb7803d1ab3f273d8861",
        "isOpenAccess": false
    },
    "Hybrid CNN-transformer framework with dynamic feature fusion for enhanced passport background texture classification": {
        "title": "Hybrid CNN-transformer framework with dynamic feature fusion for enhanced passport background texture classification",
        "authors": [
            "Maoqin Tian",
            "Lin Tang",
            "Jiafeng Xu",
            "Yibo Zhang",
            "Yong Yang",
            "Lingpei Zeng",
            "Eryang Chen",
            "Yuanlun Xie"
        ],
        "arxiv_id": null,
        "venue": "The Visual Computer",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/82fe2e52bf7f40954165c0ff36c21f5028e5783d",
        "isOpenAccess": false
    },
    "From data to diagnosis: An innovative approach to epilepsy prediction with CGTNet incorporating spatio-temporal features": {
        "title": "From data to diagnosis: An innovative approach to epilepsy prediction with CGTNet incorporating spatio-temporal features",
        "authors": [
            "Dianli Wang",
            "Enping Li",
            "Yang Wang",
            "Zhiyang Liu",
            "Aixia Sun",
            "Wei Wei",
            "Xuning Zhang",
            "Cheng Peng",
            "Fengtao Wei"
        ],
        "arxiv_id": null,
        "venue": "PLoS ONE",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": "Epilepsy affects around 50 million people globally, causing significant burdens. While many methods predict seizures, current models struggle with handling spatiotemporal features and balancing accuracy with computational efficiency.This paper introduces a novel deep learning architecture called CGTNet, which is composed of a multi-scale convolutional network, gated recurrent units (GRUs), and Sparse Transformers. It is specifically designed for analyzing elec-troencephalogram (EEG) data to predict epileptic seizures. CGTNet enhances the ability to extract spatiotemporal features from EEG signals, demonstrating its exceptional performance in seizure prediction through rigorous evaluation on the renowned CHB-MIT and SWEC-ETHZ EEG datasets. The model achieved an accuracy of 98.89%, sensitivity of 98.52%, specificity of 98.53%, an AUROC value of 0.97, and an MCC value of 0.975 on these datasets. These results not only highlight the technical innovations of CGTNet but also validate the immense potential of deep learning in processing medical signals. Our research provides an effective new tool for early detection and continuous monitoring of epilepsy, laying the foundation for advancing healthcare with artificial intelligence technology.",
        "abstract_summary_gcp": "Epilepsy affects millions globally, and while existing seizure prediction methods struggle with handling complex spatiotemporal features in EEG data and balancing accuracy with computational efficiency, this paper introduces a novel deep learning architecture called CGTNet.\n\nDesigned for epileptic seizure prediction using EEG data, CGTNet integrates a multi-scale convolutional network, Gated Recurrent Units (GRUs), and Sparse Transformers to enhance spatiotemporal feature extraction. Evaluated on the renowned CHB-MIT and SWEC-ETHZ EEG datasets, CGTNet achieved exceptional performance with 98.89% accuracy, 98.52% sensitivity, 98.53% specificity, an AUROC of 0.97, and an MCC of 0.975.\n\nThese results highlight CGTNet's technical innovations and validate the immense potential of deep learning in medical signal processing, offering an effective new tool for early detection and continuous monitoring of epilepsy, thereby advancing AI-driven healthcare.",
        "url": "https://www.semanticscholar.org/paper/99dc84029a8c14790354ccb0d9a5bd24fd9873cf",
        "isOpenAccess": false
    },
    "2512.03272": {
        "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?",
        "authors": [
            "Zhiyuan He",
            "Dingmin Wang"
        ],
        "arxiv_id": "2512.03272",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models\"overthink\"by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs'performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.",
        "abstract_summary_gcp": "Large Reasoning Models (LRMs) often rely on lengthy Chains of Thought (CoTs) for complex tasks, but this can lead to substantial token overhead, \"overthinking,\" and even incorrect answers. A promising alternative is the symbolic-solver-integrated approach, where LLMs generate executable code for external symbolic solvers.\n\nThis paper investigates when the symbolic-solver-integrated method is more effective than conventional long-CoTs. Experimental results indicate that this approach is beneficial primarily for problems requiring **limited implicit reasoning but involving a large search space**. While advanced LLMs like GPT-4o excel at deductive problems with shallow reasoning, the symbolic-solver-integrated method significantly improves performance in **constraint satisfaction problems** that demand repeated backtracking. Remarkably, when provided with a declarative exemplar, even a smaller model like CodeLlama-13B leveraging this symbolic integration can outperform GPT-4o on difficult Zebra puzzles.",
        "url": "https://www.semanticscholar.org/paper/1fca0173ce9e9a14bf8712284b28dc0d06c96952",
        "isOpenAccess": false
    },
    "2512.03185": {
        "title": "Nonlinear diffusion limit of non-local interactions on a sphere",
        "authors": [
            "M. Peletier",
            "Anna Shalova"
        ],
        "arxiv_id": "2512.03185",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Mathematics",
            "Physics"
        ],
        "abstract": "We study an aggregation PDE with competing attractive and repulsive forces on a sphere of arbitrary dimension. In particular, we consider the limit of strongly localized repulsion with a constant attraction term. We prove convergence of solutions of such a system to solutions of the aggregation-diffusion equation with a porous-medium-type diffusion term. The proof combines variational techniques with elements of harmonic analysis on a sphere. In particular, we characterize the square root of the convolution operator in terms of the spherical harmonics, which allows us to overcome difficulties arising due to the convolution on a sphere being non-commutative. The study is motivated by the toy model of transformers introduced by Geshkovski et al. (2025); and we discuss the applicability of the results to this model.",
        "abstract_summary_gcp": "This paper investigates an aggregation PDE on a sphere of arbitrary dimension, featuring competing attractive and repulsive forces. The core finding is a proof of convergence for solutions of this system to solutions of an aggregation-diffusion equation with a porous-medium-type diffusion term. This convergence occurs in the limit where repulsion is strongly localized and attraction remains constant.\n\nThe proof employs a combination of variational techniques and harmonic analysis on the sphere. A significant technical contribution involves characterizing the square root of the convolution operator using spherical harmonics, which helps overcome difficulties arising from the non-commutative nature of convolution on a sphere. The study is motivated by a toy model of transformers and discusses the applicability of its results to this model.",
        "url": "https://www.semanticscholar.org/paper/0c891825cd9d4dd7c695565dab405a8a7b2200dd",
        "isOpenAccess": false
    },
    "Islands of Signal and Transcriptomic Sequencing: A Foundation Model for Mutation and Lineage Prediction based on DNA Methylation and RNA-seq": {
        "title": "Islands of Signal and Transcriptomic Sequencing: A Foundation Model for Mutation and Lineage Prediction based on DNA Methylation and RNA-seq",
        "authors": [
            "Alexandros Alexakos",
            "Aris Tsirigos"
        ],
        "arxiv_id": null,
        "venue": "bioRxiv",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Biology"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/409e569050a2deae6f7213b75c9c1e326135547a",
        "isOpenAccess": false
    },
    "MD&A disclosure and investment efficiency: evidence from Chinese listed firms": {
        "title": "MD&A disclosure and investment efficiency: evidence from Chinese listed firms",
        "authors": [
            "Hongye Jia",
            "Xing Huang",
            "Chenggang Li",
            "Wanyue Zhang"
        ],
        "arxiv_id": null,
        "venue": "International Journal of Accounting &amp; Information Management",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "\n \n In the context of Chinese listed firms, this study aims to test the relationship between a firm’s Management Discussions and Analysis (MD&A) forward-looking tone with its subsequent investment efficiency.\n \n \n \n This study constructed a sentiment dictionary on the basis of the BosonNLP semantic sentiment dictionary, negative word dictionary and degree adverb dictionary.\n \n \n \n The main findings show that firms with more optimistic MD&A tone tend to present higher subsequent investment efficiencies. This study further finds that this positive link is stronger in firms that disclose higher text quality of MD&A, is non-state owned and has fewer common owners. This study also discovers that, compared to their rival firms, firms with more optimistic MD&A tone show higher investment efficiency. However, their investment efficiency is not affected by rival firms’ MD&A tone.\n \n \n \n First, the empirical results should be carefully interpreted as they can only infer association instead of causality between forward-looking MD&A tones and subsequent investment efficiencies. Second, previous studies have criticised the low power of dictionary-based tone measures (Li, 2010). Therefore, similar to other studies that apply natural language text analysis techniques, the selection of MD&A tone measures and their reliability may affect empirical results. Finally, in the depth in this paper, this study did not further examine the effect of managerial characteristics and corporate governance mechanisms on the relationship between forward-looking MD&A tones and subsequent investment efficiencies.\n \n \n \n Few studies have identified the relationship between firms’ forward-looking textual disclosures and their investment efficiency. The focus on MD&A tone provides an interesting yet underexplored perspective to the current literature.\n",
        "abstract_summary_gcp": "This study investigated the relationship between the forward-looking tone of Management Discussions and Analysis (MD&A) and subsequent investment efficiency among Chinese listed firms. Utilizing a custom-built sentiment dictionary, the research found that firms with a more optimistic MD&A tone tend to exhibit higher subsequent investment efficiencies. This positive association is more pronounced in firms with higher MD&A text quality, non-state ownership, and fewer common owners. Furthermore, such optimistic firms demonstrate superior investment efficiency compared to their rivals, while rival firms' MD&A tone does not impact a firm's own investment efficiency.\n\nThe study acknowledges several limitations: it establishes association rather than causality, relies on potentially limited dictionary-based tone measures, and does not delve into the influence of managerial characteristics or corporate governance. Despite these, the research offers a novel contribution by addressing an under-researched area concerning forward-looking textual disclosures and their impact on investment efficiency.",
        "url": "https://www.semanticscholar.org/paper/450b4980e57bccf5fedbca0642d2afb1db39d0e5",
        "isOpenAccess": false
    },
    "SeismoQuakeGNN: a hybrid framework for spatio-temporal earthquake prediction with transformer-enhanced models": {
        "title": "SeismoQuakeGNN: a hybrid framework for spatio-temporal earthquake prediction with transformer-enhanced models",
        "authors": [
            "Anny Leema",
            "Ponnuraman Balakrishnan",
            "Gladys Gnana Kiruba",
            "Ganesarathinam Rajarajan",
            "Stuti Goel",
            "Prisha Aggarwal"
        ],
        "arxiv_id": null,
        "venue": "Frontiers in Artificial Intelligence",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "Accurate predictions of earthquakes are crucial for disaster preparedness and risk mitigation. Conventional machine learning models like Random Forest, SVR, and XGBoost are frequently used for seismic forecasting; however, capturing the intricate spatiotemporal relationships in earthquake data remains a challenge. To overcome this issue, we propose SeismoQuakeGNN, a novel Graph Neural Network (GNN) and Transformer-based hybrid framework that integrates spatial and temporal learning for improved seismic forecasting. Unlike existing GNN-based models, SeismoQuakeGNN introduces an optimized spatial encoding mechanism to dynamically learn seismic interdependencies, coupled with a Transformer-driven attention module to capture long-range temporal correlations. Furthermore, initial experiments with XGBoost demonstrated its limitations in learning earthquake patterns, reinforcing the need for deep spatial–temporal modeling. The new SeismoQuakeGNN method is capable of substantial and efficient data processing of relationships in both space and time, as well as providing superior transfer to different seismic areas, thereby qualifying as a dependable starting point to extensive earthquake forecasting and hazard evaluation.",
        "abstract_summary_gcp": "Conventional machine learning models struggle to capture the complex spatiotemporal relationships crucial for accurate earthquake prediction. To address this, a novel hybrid framework called **SeismoQuakeGNN** is proposed. This model combines a Graph Neural Network (GNN) with a Transformer, integrating spatial and temporal learning.\n\nSeismoQuakeGNN distinguishes itself by:\n1.  **Optimized Spatial Encoding:** Dynamically learns seismic interdependencies.\n2.  **Transformer-driven Attention Module:** Captures long-range temporal correlations.\n\nThis approach overcomes the limitations of traditional models (like XGBoost, which showed poor performance in initial tests) by efficiently processing spatiotemporal data and offering superior transferability to different seismic areas. SeismoQuakeGNN thus provides a dependable foundation for improved earthquake forecasting and hazard evaluation.",
        "url": "https://www.semanticscholar.org/paper/11801e8081b12edf0fa3ee66a7732426fa105fcd",
        "isOpenAccess": false
    },
    "MultCPM: a multi-omics cancer recurrence prediction model utilizing a multi-head attention mechanism": {
        "title": "MultCPM: a multi-omics cancer recurrence prediction model utilizing a multi-head attention mechanism",
        "authors": [
            "Xiaofei Liu",
            "Haiyan Cui",
            "Tengcheng Que",
            "Sujuan Zhao",
            "Shaohui Zhong",
            "Wenjian Liu",
            "Yanling Hu"
        ],
        "arxiv_id": null,
        "venue": "PeerJ Computer Science",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "Deep learning-based approaches for integrating multi-omics data offer a novel perspective on cancer recurrence prediction. However, existing methods struggle to manage the complex relationships within multi-omics data and the intrinsic correlations between samples, leading to suboptimal prediction accuracy. To tackle these challenges, we propose a multi-omics cancer recurrence prediction model (MultCPM), which employs a multi-head attention mechanism to extract key information from biological pathways. Integrated with a hierarchical fusion module, the model performs layered integration of omics data to effectively capture their interdependence. Ultimately, the fused information is consolidated into a unified feature matrix, refining critical features and their relationships across omics. Results from 5-fold cross-validation, repeated five times on Breast Cancer (BRCA), Bladder Cancer (BLCA), and Liver Cancer (LIHC) datasets, demonstrate that the MultCPM model achieves superior prediction performance and robustness. Additionally, Deep SHapley Additive exPlanations (DeepSHAP) was utilized to analyze the model’s interpretability, revealing key genes closely associated with cancer recurrence, thus providing valuable insights for biological research and the development of cancer recurrence prediction algorithms.\n \n The code is publicly available at\n https://github.com/dowell2016/MultCPM\n .\n",
        "abstract_summary_gcp": "Existing deep learning methods for multi-omics cancer recurrence prediction struggle with complex data relationships and sample correlations, resulting in suboptimal accuracy. To address this, the authors propose **MultCPM**, a multi-omics cancer recurrence prediction model.\n\nMultCPM utilizes a **multi-head attention mechanism** to extract crucial information from biological pathways and a **hierarchical fusion module** for layered integration of omics data, effectively capturing interdependencies and consolidating features into a unified matrix.\n\nEvaluated using repeated 5-fold cross-validation on Breast Cancer (BRCA), Bladder Cancer (BLCA), and Liver Cancer (LIHC) datasets, MultCPM demonstrates **superior prediction performance and robustness**. Furthermore, DeepSHAP analysis provides interpretability by identifying **key genes associated with cancer recurrence**, offering valuable biological insights.\n\nThe model's code is publicly available.",
        "url": "https://www.semanticscholar.org/paper/8c1d8f1c8c31dfa9df07934891ff5f6411600b04",
        "isOpenAccess": false
    },
    "Parameter-efficient fine-tuning for low-resource text classification: a comparative study of LoRA, IA3, and ReFT": {
        "title": "Parameter-efficient fine-tuning for low-resource text classification: a comparative study of LoRA, IA3, and ReFT",
        "authors": [
            "Steve Nwaiwu"
        ],
        "arxiv_id": null,
        "venue": "Frontiers in Big Data",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "\n The successful application of large-scale transformer models in Natural Language Processing (NLP) is often hindered by the substantial computational cost and data requirements of full fine-tuning. This challenge is particularly acute in low-resource settings, where standard fine-tuning can lead to catastrophic overfitting and model collapse. To address this, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a promising solution. However, a direct comparative analysis of their trade-offs under unified low-resource conditions is lacking. This study provides a rigorous empirical evaluation of three prominent PEFT methods: Low-Rank Adaptation (LoRA), Infused Adapter by Inhibiting and Amplifying Inner Activations (IA\n 3\n ), and a Representation Fine-Tuning (ReFT) strategy. Using a DistilBERT base model on low-resource versions of the AG News and Amazon Reviews datasets, the present work compares these methods against a full fine-tuning baseline across accuracy, F1 score, trainable parameters, and GPU memory usage. The findings reveal that while all PEFT methods dramatically outperform the baseline, LoRA consistently achieves the highest F1 scores (0.909 on Amazon Reviews). Critically, ReFT delivers nearly identical performance (~98% of LoRA's F1 score) while training only ~3% of the parameters, establishing it as the most efficient method. This research demonstrates that PEFT is not merely an efficiency optimization, but a necessary tool for robust generalization in data-scarce environments, providing practitioners with a clear guide to navigate the performance—efficiency trade-off. By unifying these evaluations under controlled conditions, this study advances beyond fragmented prior research and offers a systematic framework for selecting PEFT strategies.\n",
        "abstract_summary_gcp": "This study addresses the challenges of fine-tuning large transformer models in low-resource NLP settings, where high computational cost and data requirements often lead to overfitting. It performs a rigorous empirical evaluation of three Parameter-Efficient Fine-Tuning (PEFT) methods—LoRA, IA3, and ReFT—against a full fine-tuning baseline.\n\nUsing a DistilBERT model on low-resource AG News and Amazon Reviews datasets, the research compared these methods across accuracy, F1 score, trainable parameters, and GPU memory. The findings show that all PEFT methods dramatically outperform the full fine-tuning baseline. LoRA consistently achieved the highest F1 scores (e.g., 0.909 on Amazon Reviews). However, ReFT delivered nearly identical performance (around 98% of LoRA's F1 score) while training only about 3% of the parameters, making it the most parameter-efficient method.\n\nThe study concludes that PEFT is not just an efficiency optimization, but a necessary tool for robust generalization in data-scarce environments, offering practitioners a clear guide for selecting fine-tuning strategies.",
        "url": "https://www.semanticscholar.org/paper/93169e570bfb0a333019d39f6e0cd7997b7fd7d8",
        "isOpenAccess": false
    },
    "2512.01208": {
        "title": "Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking",
        "authors": [
            "Alper Yildirim",
            ".Ibrahim Yucedaug"
        ],
        "arxiv_id": "2512.01208",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Standard Transformers suffer from a\"Semantic Alignment Tax\", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates\"Catastrophic Rigidity\", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the\"Plasticity-Stability\"stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.",
        "abstract_summary_gcp": "Standard Transformers suffer from a \"Semantic Alignment Tax,\" a high optimization cost to create coherent semantic maps from chaotic initialization, which leads to \"Catastrophic Rigidity\"—the inability to adapt to new concepts without destroying existing reasoning. The Iterative Semantic Map Refinement (ISMR) diagnostic protocol reveals this alignment issue is a fixed geometric barrier that scaling cannot overcome.\n\nTo address this, the paper introduces the Phase-Resonant Intelligent Spectral Model (PRISM), which encodes semantic identity as resonant frequencies in the complex domain and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions.\n\nEvaluated on the WMT14 translation task, PRISM, while slightly behind Standard Transformers in general competence (21.40 vs 23.88 BLEU), demonstrates superior \"Plasticity-Stability.\" When exposed to novel concepts, Standard Transformers exhibit \"Catastrophic Forgetting\" (degrading by -10.55 BLEU with only 60% acquisition). In contrast, PRISM achieves \"Lossless Plasticity,\" with 96% 5-shot acquisition and negligible degradation (-0.84 BLEU). This suggests harmonic representations in PRISM effectively decouple memory from reasoning, providing a structural solution to the plasticity-stability dilemma for real-time knowledge adaptation.",
        "url": "https://www.semanticscholar.org/paper/16b525dd1230521ec684add6e9d7d14546de7492",
        "isOpenAccess": false
    },
    "1706.03762": {
        "title": "Attention is All you Need",
        "authors": [
            "Ashish Vaswani",
            "Noam Shazeer",
            "Niki Parmar",
            "Jakob Uszkoreit",
            "Llion Jones",
            "Aidan N. Gomez",
            "Lukasz Kaiser",
            "I. Polosukhin"
        ],
        "arxiv_id": "1706.03762",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "publicationTypes": null,
        "citationCount": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "abstract_summary_gcp": "This paper introduces the Transformer, a novel network architecture for sequence transduction that *solely relies on attention mechanisms*, completely dispensing with recurrence and convolutions.\n\nThe Transformer demonstrates superior quality, enhanced parallelizability, and significantly reduced training time compared to existing complex recurrent or convolutional encoder-decoder models.\n\nKey results include:\n*   **WMT 2014 English-to-German:** Achieved 28.4 BLEU, outperforming previous best results (even ensembles) by over 2 BLEU.\n*   **WMT 2014 English-to-French:** Set a new single-model state-of-the-art BLEU score of 41.8, requiring only 3.5 days of training on eight GPUs, a fraction of the cost of prior leading models.\n\nThe model also generalizes well, successfully applying to English constituency parsing.",
        "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "isOpenAccess": false
    },
    "2412.11189": {
        "title": "Leveraging Large Language Models for Active Merchant Non-player Characters",
        "authors": [
            "Byungjun Kim",
            "Minju Kim",
            "Dayeon Seo",
            "Bugeun Kim"
        ],
        "arxiv_id": "2412.11189",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2024,
        "publicationTypes": null,
        "citationCount": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We highlight two significant issues leading to the passivity of current merchant non-player characters (NPCs): pricing and communication. While immersive interactions with active NPCs have been a focus, price negotiations between merchant NPCs and players remain underexplored. First, passive pricing refers to the limited ability of merchants to modify predefined item prices. Second, passive communication means that merchants can only interact with players in a scripted manner. To tackle these issues and create an active merchant NPC, we propose a merchant framework based on large language models (LLMs), called MART, which consists of an appraiser module and a negotiator module. We conducted two experiments to explore various implementation options under different training methods and LLM sizes, considering a range of possible game environments. Our findings indicate that finetuning methods, such as supervised finetuning (SFT) and knowledge distillation (KD), are effective in using smaller LLMs to implement active merchant NPCs. Additionally, we found three irregular cases arising from the responses of LLMs.",
        "abstract_summary_gcp": "This paper addresses the passivity of current merchant non-player characters (NPCs), attributing it to two main issues: **passive pricing** (merchants' limited ability to modify item prices) and **passive communication** (scripted interactions).\n\nTo create active merchant NPCs, the authors propose **MART**, an LLM-based framework consisting of an **appraiser module** and a **negotiator module**.\n\nExperiments were conducted to explore various implementation options, training methods, and LLM sizes across different game environments. The findings indicate that finetuning methods, specifically **supervised finetuning (SFT)** and **knowledge distillation (KD)**, are effective for implementing active merchant NPCs using **smaller LLMs**. Additionally, the study identified three irregular cases arising from the LLMs' responses.",
        "url": "https://www.semanticscholar.org/paper/ea7ae5a91975fb9a6d632aa67eb6fd5eb04ded06",
        "isOpenAccess": false
    },
    "2511.02979": {
        "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications",
        "authors": [
            "Esther Sun",
            "Zichu Wu"
        ],
        "arxiv_id": "2511.02979",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional compan- ions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applica- tions in work, gaming, and mental health, highlighting the shift from\"feeling\"to\"thinking and acting\"and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III&IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.",
        "abstract_summary_gcp": "This paper addresses the fragmented nature of LLM-based AI persona design and application by proposing a **Four-Quadrant Technical Taxonomy**.\n\nThis framework categorizes AI companion applications along two axes: **Virtual vs. Embodied** and **Emotional Companionship vs. Functional Augmentation**.\n\n*   **Quadrant I (Virtual Companionship)** focuses on virtual idols and romantic companions, highlighting challenges in maintaining long-term emotional consistency.\n*   **Quadrant II (Functional Virtual Assistants)** covers AI in work, gaming, and mental health, emphasizing \"thinking and acting\" with technologies like RAG and on-device inference.\n*   **Quadrants III & IV (Embodied Intelligence)** explore physical applications such as home robots, identifying core challenges in symbol grounding, data privacy, and ethical liability.\n\nThe taxonomy serves as a unified map for researchers and developers to navigate the complex design space, and for policymakers to identify and address unique risks across different AI companion scenarios.",
        "url": "https://www.semanticscholar.org/paper/2e31f23c0abd239cc09f846cbea096a73181406b",
        "isOpenAccess": false
    },
    "2510.25014": {
        "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading",
        "authors": [
            "Minkyung Kim",
            "Junsik Kim",
            "Woongcheol Yang",
            "Sangdon Park",
            "Sohee Bae"
        ],
        "arxiv_id": "2510.25014",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust. This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading (browse-offer-review-confirm). To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations. Evaluation across 300 trading dialogues demonstrates>99% state compliance and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models'(Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.",
        "abstract_summary_gcp": "Large Language Models (LLMs) can create dynamic game interactions but struggle to adhere to the strict procedural flows of rule-governed systems like in-game trading (e.g., browse-offer-review-confirm), which erodes player trust.\n\nTo address this, the paper introduces **Autoregressive State-Tracking Prompting (ASTP)**. This methodology uses a strategically designed prompt to compel the LLM to explicitly identify and report its procedural state from the previous turn, making its state-tracking process verifiable instead of relying on implicit understanding. This is further enhanced by a state-specific placeholder post-processing method to ensure accurate price calculations and transactional integrity.\n\nEvaluated across 300 trading dialogues, ASTP achieved over 99% state compliance and 99.3% calculation precision. Crucially, ASTP combined with placeholder post-processing enabled smaller LLMs (Gemini-2.5-Flash) to match the performance of larger models (Gemini-2.5-Pro) while drastically reducing response times from 21.2 seconds to 2.4 seconds, offering a practical, real-time, and resource-efficient solution for commercial games.",
        "url": "https://www.semanticscholar.org/paper/5537e028cf6fcf18e2ed3e132d90e70fe29da5e6",
        "isOpenAccess": false
    },
    "2508.03014": {
        "title": "Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers",
        "authors": [
            "Jingyan Wang",
            "Yang Zhao",
            "Haotian Mao",
            "Xubo Yang"
        ],
        "arxiv_id": "2508.03014",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, and their integration with Extended Reality (XR) is poised to transform how users interact with immersive environments. This survey provides a comprehensive review of recent developments at the intersection of LLMs and XR, offering a structured organization of research along both technical and application dimensions. We propose a taxonomy of LLM-enhanced XR systems centered on key technical paradigms -- such as interactive agent control, XR development toolkits, and generative scene synthesis -- and discuss how these paradigms enable novel capabilities in XR. In parallel, we examine how LLM-driven techniques support practical XR applications across diverse domains, including immersive education, clinical healthcare, and industrial manufacturing. By connecting these technical paradigms with application frontiers, our survey highlights current trends, delineates design considerations, and identifies open challenges in building LLM-augmented XR systems. This work provides insights that can guide researchers and practitioners in advancing the state of the art in intelligent XR experiences.",
        "abstract_summary_gcp": "This survey comprehensively reviews the emerging field of integrating Large Language Models (LLMs) with Extended Reality (XR), emphasizing its potential to revolutionize immersive user interactions. It offers a structured organization of recent research, proposing a taxonomy based on key technical paradigms—such as interactive agent control, XR development toolkits, and generative scene synthesis—and diverse application domains, including immersive education, clinical healthcare, and industrial manufacturing. The work connects these technical advancements with their practical applications, highlighting current trends, critical design considerations, and open challenges to guide researchers and practitioners in developing advanced LLM-augmented XR systems.",
        "url": "https://www.semanticscholar.org/paper/f684b82057785845f866481f73206342107c1b4d",
        "isOpenAccess": false
    },
    "2507.07203": {
        "title": "State-Inference-Based Prompting for Natural Language Trading with Game NPCs",
        "authors": [
            "Minkyung Kim",
            "Junsik Kim",
            "Hwidong Bae",
            "Woongcheol Yang",
            "Sangdon Park",
            "Sohee Bae"
        ],
        "arxiv_id": "2507.07203",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models enable dynamic game interactions but struggle with rule-governed trading systems. Current implementations suffer from rule violations, such as item hallucinations and calculation errors, that erode player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable trading through autonomous dialogue state inference and context-specific rule adherence. The approach decomposes trading into six states within a unified prompt framework, implementing context-aware item referencing and placeholder-based price calculations. Evaluation across 100 trading dialogues demonstrates>97% state compliance,>95% referencing accuracy, and 99.7% calculation precision. SIBP maintains computational efficiency while outperforming baseline approaches, establishing a practical foundation for trustworthy NPC interactions in commercial games.",
        "abstract_summary_gcp": "Large Language Models (LLMs) can create dynamic game interactions but struggle with rule-governed trading systems, often leading to rule violations, item hallucinations, and calculation errors that erode player trust.\n\nTo overcome this, **State-Inference-Based Prompting (SIBP)** is introduced. SIBP enables reliable in-game trading by autonomously inferring dialogue states and adhering to context-specific rules. It achieves this by:\n1.  Decomposing trading into six distinct states within a unified prompt framework.\n2.  Implementing context-aware item referencing.\n3.  Utilizing placeholder-based price calculations.\n\nEvaluated across 100 trading dialogues, SIBP demonstrated high reliability with over 97% state compliance, more than 95% referencing accuracy, and 99.7% calculation precision. It also maintains computational efficiency and outperforms baseline methods, providing a practical foundation for trustworthy NPC interactions in commercial games.",
        "url": "https://www.semanticscholar.org/paper/c486588711432ca493b4e77989bcc617a4e1333f",
        "isOpenAccess": false
    },
    "2504.14538": {
        "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation",
        "authors": [
            "Yiting Ran",
            "Xintao Wang",
            "Tian Qiu",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "arxiv_id": "2504.14538",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.",
        "abstract_summary_gcp": "This paper introduces **BookWorld**, a comprehensive system designed to simulate multi-agent societies based on established fictional books, addressing a gap in current LLM-driven social simulation which typically creates agents from scratch.\n\nBookWorld is built to incorporate the intricacies of fictional worlds, including diverse and dynamic characters, unique worldviews, and geographical constraints. It supports various applications such as story generation, interactive games, and social simulation, allowing users to extend and explore beloved fictional narratives.\n\nEvaluations show that BookWorld generates creative, high-quality stories while maintaining strong fidelity to the source material, outperforming previous methods with a 75.36% win rate. The project code is available online.",
        "url": "https://www.semanticscholar.org/paper/cfb1dddc983ff69c3cefcd0f4350830947b5943b",
        "isOpenAccess": false
    },
    "2511.04962": {
        "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
        "authors": [
            "Zihao Yi",
            "Qingxuan Jiang",
            "Ruotian Ma",
            "Xingyu Chen",
            "Qu Yang",
            "Mengru Wang",
            "F. Ye",
            "Ying Shen",
            "Zhaopeng Tu",
            "Xiaolong Li",
            "Linus"
        ],
        "arxiv_id": "2511.04962",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful''and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.",
        "abstract_summary_gcp": "This paper investigates Large Language Models' (LLMs) ability to authentically portray non-prosocial or villainous characters, an area previously underexplored. It hypothesizes that current safety alignment methods fundamentally conflict with this task.\n\nTo test this, the authors developed the **Moral RolePlay benchmark**, featuring a four-level moral alignment scale. Their large-scale evaluation of state-of-the-art LLMs revealed a **consistent, monotonic decline in role-playing fidelity as character morality decreased**. Models particularly struggled with traits antithetical to safety, such as \"Deceitful\" and \"Manipulative,\" often resorting to superficial aggression instead of nuanced malevolence.\n\nFurthermore, general chatbot proficiency was found to be a poor indicator of villain role-playing skill, with highly safety-aligned models performing worst. The research provides the first systematic evidence of a critical tension between LLM safety alignment and creative fidelity, highlighting the need for more nuanced, context-aware alignment techniques.",
        "url": "https://www.semanticscholar.org/paper/c1f08d5b6c3d0cfe94179207a3824442f6b534d5",
        "isOpenAccess": false
    },
    "2510.14718": {
        "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms",
        "authors": [
            "Xingmeng Zhao",
            "Dan Schumacher",
            "Veronica Rammouz",
            "Anthony Rios"
        ],
        "arxiv_id": "2510.14718",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.",
        "abstract_summary_gcp": "Artificial intelligence (AI) is rapidly advancing healthcare tools, but its swift, low-barrier development introduces risks like bias, privacy violations, and unequal access, especially when diverse user needs and contexts are overlooked. Current automated risk detection methods often diminish human understanding of how harms occur.\n\nTo counter this, the authors propose a human-centered framework utilizing user stories and multi-agent discussions to foster creative thinking about potential benefits and harms *before* AI deployment. A user study validated this approach, showing that participants who engaged with stories recognized a significantly broader and more evenly distributed range of harm types. In contrast, those without stories focused predominantly on privacy and well-being (58.3%). The findings conclude that storytelling effectively helps individuals creatively consider a wider array of AI's potential impacts on users.",
        "url": "https://www.semanticscholar.org/paper/e045474ea7bf0b55d7e0b0d974ba18392ae3510e",
        "isOpenAccess": false
    },
    "2510.06800": {
        "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline",
        "authors": [
            "Haotian Wu",
            "Shufan Jiang",
            "Mingyu Chen",
            "Yiyang Feng",
            "Hehai Lin",
            "Heqing Zou",
            "Yao Shu",
            "Chengwei Qin"
        ],
        "arxiv_id": "2510.06800",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character's responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.",
        "abstract_summary_gcp": "Existing role-playing (RP) benchmarks for large language models (LLMs) are becoming obsolete due to their narrow scope and limited adaptability. To address this, the authors introduce **FURINA-Builder**, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks.\n\nFURINA-Builder is the first benchmark builder in the RP area designed for adaptable assessment, enabling the evaluation of arbitrary characters across diverse scenarios and prompt formats. It operates by simulating dialogues between a test character and other characters from a curated pool, with an LLM judge selecting fine-grained evaluation dimensions and refining the test character's responses.\n\nUsing this pipeline, they developed **FURINA-Bench**, a comprehensive new RP benchmark featuring both established and synthesized characters, each assessed with dimension-specific criteria. Human evaluation and separability analysis validate the pipeline and benchmark design.\n\nExtensive evaluations of cutting-edge LLMs on FURINA-Bench revealed several key findings:\n*   **o3** and **DeepSeek-R1** achieved the best performance in English and Chinese RP tasks, respectively.\n*   **Established characters** consistently outperformed synthesized ones, with reasoning capabilities further amplifying this disparity.\n*   **Model scale** did not monotonically reduce hallucinations.\n*   A critical, novel **trade-off** was uncovered for reasoning LLMs: while reasoning improves RP performance, it simultaneously increases RP hallucinations. This trade-off defines a broader Pareto frontier between RP performance and reliability across all LLMs.\n\nThese results demonstrate the effectiveness of FURINA-Builder and the significant challenge posed by FURINA-Bench to current LLM capabilities.",
        "url": "https://www.semanticscholar.org/paper/e3c780d63840f3154d4ead8eb46e2973145f11f5",
        "isOpenAccess": false
    },
    "2509.26461": {
        "title": "CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine",
        "authors": [
            "Yuyang Cheng",
            "Linyue Cai",
            "Changwei Peng",
            "Yumiao Xu",
            "Rongfang Bie",
            "Yong Zhao"
        ],
        "arxiv_id": "2509.26461",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present CreAgentive, an agent workflow driven multi-category creative generation engine that addresses four key limitations of contemporary large language models in writing stories, drama and other categories of creatives: restricted genre diversity, insufficient output length, weak narrative coherence, and inability to enforce complex structural constructs. At its core, CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge graph-based narrative representation that decouples story logic from stylistic realization by encoding characters, events, and environments as semantic triples. CreAgentive engages a three-stage agent workflow that comprises: an Initialization Stage that constructs a user-specified narrative skeleton; a Generation Stage in which long- and short-term objectives guide multi-agent dialogues to instantiate the Story Prototype; a Writing Stage that leverages this prototype to produce multi-genre text with advanced structures such as retrospection and foreshadowing. This architecture reduces storage redundancy and overcomes the typical bottlenecks of long-form generation. In extensive experiments, CreAgentive generates thousands of chapters with stable quality and low cost (less than $1 per 100 chapters) using a general-purpose backbone model. To evaluate performance, we define a two-dimensional framework with 10 narrative indicators measuring both quality and length. Results show that CreAgentive consistently outperforms strong baselines and achieves robust performance across diverse genres, approaching the quality of human-authored novels.",
        "abstract_summary_gcp": "CreAgentive is an agent-driven creative generation engine designed to overcome four key limitations of current large language models in producing stories and dramas: restricted genre diversity, insufficient output length, weak narrative coherence, and difficulty enforcing complex structural constructs.\n\nAt its heart, CreAgentive utilizes a \"Story Prototype\" – a genre-agnostic, knowledge graph-based narrative representation. This prototype decouples story logic from stylistic presentation by encoding narrative elements (characters, events, environments) as semantic triples.\n\nThe system employs a three-stage agent workflow:\n1.  **Initialization Stage:** Constructs a user-specified narrative skeleton.\n2.  **Generation Stage:** Multi-agent dialogues are guided by long- and short-term objectives to instantiate the Story Prototype.\n3.  **Writing Stage:** Leverages the completed prototype to produce multi-genre text incorporating advanced structures like retrospection and foreshadowing.\n\nThis architecture efficiently addresses challenges in long-form generation and reduces storage redundancy. Extensive experiments demonstrate that CreAgentive can generate thousands of high-quality chapters at low cost (under $1 per 100 chapters). Using a novel two-dimensional framework for narrative assessment, it consistently outperforms strong baselines across diverse genres, achieving quality comparable to human-authored novels.",
        "url": "https://www.semanticscholar.org/paper/931598078638a71f6e6fc73cd2b81bf21cef3128",
        "isOpenAccess": false
    },
    "2509.18052": {
        "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies",
        "authors": [
            "Jiaxu Zhou",
            "Jen-tse Huang",
            "Xuhui Zhou",
            "Man Ho Lam",
            "Xintao Wang",
            "Hao Zhu",
            "Wenxuan Wang",
            "M. Sap"
        ],
        "arxiv_id": "2509.18052",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly used for social simulation, where populations of agents are expected to reproduce human-like collective behavior. However, we find that many recent studies adopt experimental designs that systematically undermine the validity of their claims. From a survey of over 40 papers, we identify six recurring methodological flaws: agents are often homogeneous (Profile), interactions are absent or artificially imposed (Interaction), memory is discarded (Memory), prompts tightly control outcomes (Minimal-Control), agents can infer the experimental hypothesis (Unawareness), and validation relies on simplified theoretical models rather than real-world data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying social experiment in 53.1% of cases when given instructions from prior work-violating the Unawareness principle. We formalize these six requirements as the PIMMUR principles and argue they are necessary conditions for credible LLM-based social simulation. To demonstrate their impact, we re-run five representative studies using a framework that enforces PIMMUR and find that the reported social phenomena frequently fail to emerge under more rigorous conditions. Our work establishes methodological standards for LLM-based multi-agent research and provides a foundation for more reliable and reproducible claims about\"AI societies.\"",
        "abstract_summary_gcp": "This paper critically examines the methodological validity of current Large Language Model (LLM)-based social simulations, which aim to reproduce human-like collective behavior. Based on a survey of over 40 studies, it identifies six recurring methodological flaws, formalized as the **PIMMUR principles**: agents are homogeneous (Profile), interactions are absent or artificial (Interaction), memory is discarded (Memory), prompts overly control outcomes (Minimal-Control), agents can infer the experimental hypothesis (Unawareness), and validation relies on simplified theoretical models instead of real-world data (Realism).\n\nA significant example of these flaws is the violation of the 'Unawareness' principle, where LLMs like GPT-4o and Qwen-3 correctly infer the underlying social experiment in 53.1% of cases. The authors argue that these PIMMUR principles are necessary conditions for credible LLM-based social simulation. To demonstrate their impact, they re-ran five representative studies enforcing these principles and found that the reported social phenomena frequently failed to emerge under these more rigorous conditions. The work aims to establish new methodological standards for multi-agent LLM research, promoting more reliable and reproducible claims about \"AI societies.\"",
        "url": "https://www.semanticscholar.org/paper/805b55a74a928fc7ae9742607583b747bcde7cb3",
        "isOpenAccess": false
    },
    "2507.21028": {
        "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation",
        "authors": [
            "Jiaju Chen",
            "Yuxuan Lu",
            "Xiaojie Wang",
            "Huimin Zeng",
            "Jing Huang",
            "Jiri Gesi",
            "Ying Xu",
            "Bingsheng Yao",
            "Dakuo Wang"
        ],
        "arxiv_id": "2507.21028",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging\"LLM-as-a-judge\"paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts'ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.",
        "abstract_summary_gcp": "Evaluating real-world NLP applications requires diverse human perspectives, but human evaluators are scarce and costly. While \"LLM-as-a-judge\" offers a promising alternative, current approaches are limited by arbitrarily designed personas and a lack of generalizability.\n\nTo address these issues, the authors propose **MAJ-EVAL (Multi-Agent-as-Judge)**. This framework automatically constructs multiple evaluator personas with distinct dimensions from relevant textual documents (e.g., research papers), instantiates LLM agents with these personas, and facilitates in-group debates among these agents to generate multi-dimensional feedback.\n\nExperiments in educational and medical domains show that MAJ-EVAL's evaluation results align more closely with human experts' ratings than conventional automated metrics and existing LLM-as-a-judge methods.",
        "url": "https://www.semanticscholar.org/paper/6888258748905e5ded16b15956383e03ed9e7c1c",
        "isOpenAccess": false
    },
    "2411.10109": {
        "title": "Generative Agent Simulations of 1,000 People",
        "authors": [
            "Joon Sung Park",
            "Carolyn Q. Zou",
            "Aaron Shaw",
            "Benjamin Mako Hill",
            "C. Cai",
            "Meredith Ringel Morris",
            "Robb Willer",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "arxiv_id": "2411.10109",
        "venue": "arXiv.org",
        "year": 2024,
        "publicationTypes": null,
        "citationCount": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.",
        "abstract_summary_gcp": "This paper introduces a novel agent architecture designed to simulate human behavior, creating general-purpose computational agents for 1,052 real individuals. By applying large language models (LLMs) to qualitative interviews about participants' lives, the researchers developed agents that could replicate the individuals' attitudes and behaviors.\n\nKey findings include:\n*   These \"generative agents\" replicated participants' General Social Survey (GSS) responses with an accuracy of 85% compared to how consistently participants replicated their *own* answers two weeks later.\n*   The agents also performed comparably well in predicting personality traits and outcomes in experimental replications.\n*   Notably, this architecture reduced accuracy biases across racial and ideological groups, a significant improvement over agents relying solely on demographic descriptions.\n\nThis work provides a foundational new tool for investigating individual and collective behavior, with broad potential applications in policymaking and social science.",
        "url": "https://www.semanticscholar.org/paper/262e5a2f7371b8cdaba06a24facc3aa3f7447516",
        "isOpenAccess": false
    },
    "Towards social superintelligence? AI infers diverse psychological traits from text without specific training, outperforming human judges": {
        "title": "Towards social superintelligence? AI infers diverse psychological traits from text without specific training, outperforming human judges",
        "authors": [
            "Ariel Rosenfelder",
            "M. Levitin",
            "Michael Gilead"
        ],
        "arxiv_id": null,
        "venue": "Computers in Human Behavior",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/0b16d432207d6549debbfee209a6997b8afe0304",
        "isOpenAccess": false
    },
    "2512.01431": {
        "title": "A Meta-Analysis of the Persuasive Power of Large Language Models",
        "authors": [
            "Lukas Holbling",
            "Sebastian Maier",
            "Stefan Feuerriegel"
        ],
        "arxiv_id": "2512.01431",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are increasingly used for persuasion, such as in political communication and marketing, where they affect how people think, choose, and act. Yet, empirical findings on the effectiveness of LLMs in persuasion compared to humans remain inconsistent. The aim of this study was to systematically review and meta-analytically assess whether LLMs differ from humans in persuasive effectiveness. We identified $7$ studies with 17,422 participants primarily recruited from English-speaking countries and $12$ effect size estimates. Egger's test indicated potential small-study effects ($p = .018$), but the trim-and-fill analysis did not impute any missing studies, suggesting a low risk of publication bias. We then compute the standardized effect sizes based on Hedges'$g$. The results show no significant overall difference in persuasive performance between LLMs and humans ($g = 0.02$, $p = .530$). However, we observe substantial heterogeneity across studies ($I^2 = 75.97\\%$), suggesting that persuasiveness strongly depends on contextual factors. In separate exploratory moderator analyses, no individual factor (e.g., LLM model, conversation design, or domain) reached statistical significance, which may be due to the limited number of studies. When considered jointly in a combined model, these factors explained a large proportion of the between-study variance ($R^2 = 81.93\\%$), and residual heterogeneity is low ($I^2 = 35.51\\%$). Although based on a small number of studies, this suggests that differences in LLM model, conversation design, and domain are important contextual factors in shaping persuasive performance, and that single-factor tests may understate their influence. Our results highlight that LLMs can match human performance in persuasion, but their success depends strongly on how they are implemented and embedded in communication contexts.",
        "abstract_summary_gcp": "This meta-analysis systematically reviewed the persuasive effectiveness of Large Language Models (LLMs) compared to humans, analyzing 7 studies with 17,422 participants.\n\nThe study found **no significant overall difference in persuasive performance between LLMs and humans**. However, there was **substantial heterogeneity** across studies ($I^2 = 75.97\\%$), indicating that persuasiveness strongly depends on contextual factors.\n\nWhile individual factors (LLM model, conversation design, domain) did not show statistical significance on their own (likely due to the small number of studies), their **combined effect explained a large proportion of the variance ($R^2 = 81.93\\%$**). This suggests that these factors are crucial in shaping persuasive performance.\n\nThe research concludes that LLMs can match human performance in persuasion, but their success is **highly dependent on how they are implemented and embedded within specific communication contexts**.",
        "url": "https://www.semanticscholar.org/paper/ff2c005d93304ecb82ad0b60517d37cdecbf553e",
        "isOpenAccess": false
    },
    "Agricultural Futures Trading Decision Using AI Agent With Multiscale Candlestick Analysis": {
        "title": "Agricultural Futures Trading Decision Using AI Agent With Multiscale Candlestick Analysis",
        "authors": [
            "Jiarong Wang",
            "Jiajing Wu",
            "Guoshuai Zhang",
            "Mingzhou Tan",
            "Shaohong Chen",
            "Zeyi Lin"
        ],
        "arxiv_id": null,
        "venue": "IEEE Transactions on Computational Social Systems",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/c0f6b9174f11278a5443166f2113f8ce3234bb59",
        "isOpenAccess": false
    },
    "2512.00390": {
        "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing",
        "authors": [
            "Nuo Chen",
            "Hanpei Fang",
            "Jiqun Liu",
            "Wilson Wei",
            "Tetsuya Sakai",
            "Xiao-Ming Wu"
        ],
        "arxiv_id": "2512.00390",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.",
        "abstract_summary_gcp": "This research investigates how simulated Big Five personality traits in Large Language Models (LLMs) influence their susceptibility to \"priming effects\" during relevance labeling. While LLMs are useful for this task, they are known to be biased by prior judgments.\n\nThe study, using multiple LLMs and TREC datasets, found that certain personality profiles—specifically High Openness and Low Neuroticism—consistently reduced this priming susceptibility. It also noted that the most effective personality for mitigating priming can vary across different LLMs and task types.\n\nBased on these findings, the authors propose \"personality prompting\" as a method to reduce threshold priming, bridging psychological understanding with LLM evaluation practices.",
        "url": "https://www.semanticscholar.org/paper/ea813ad3c43f7820a4bb017bf78901f05290b999",
        "isOpenAccess": false
    },
    "2512.02058": {
        "title": "Misalignment of LLM-Generated Personas with Human Perceptions in Low-Resource Settings",
        "authors": [
            "Tabia Tanzin Prama",
            "Christopher M. Danforth",
            "P. Dodds"
        ],
        "arxiv_id": "2512.02058",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances enable Large Language Models (LLMs) to generate AI personas, yet their lack of deep contextual, cultural, and emotional understanding poses a significant limitation. This study quantitatively compared human responses with those of eight LLM-generated social personas (e.g., Male, Female, Muslim, Political Supporter) within a low-resource environment like Bangladesh, using culturally specific questions. Results show human responses significantly outperform all LLMs in answering questions, and across all matrices of persona perception, with particularly large gaps in empathy and credibility. Furthermore, LLM-generated content exhibited a systematic bias along the lines of the ``Pollyanna Principle'', scoring measurably higher in positive sentiment ($\\Phi_{avg} = 5.99$ for LLMs vs. $5.60$ for Humans). These findings suggest that LLM personas do not accurately reflect the authentic experience of real people in resource-scarce environments. It is essential to validate LLM personas against real-world human data to ensure their alignment and reliability before deploying them in social science research.",
        "abstract_summary_gcp": "A study comparing human responses with those of eight LLM-generated social personas in a low-resource, culturally specific environment (Bangladesh) found significant limitations in LLM personas. While LLMs can generate personas, they lack deep contextual, cultural, and emotional understanding. The research revealed that human responses consistently and significantly outperformed all LLMs in answering questions and across all metrics of persona perception, with particularly large gaps in empathy and credibility. Additionally, LLM-generated content showed a systematic positive sentiment bias (Pollyanna Principle), scoring higher than human responses. These findings suggest that current LLM personas do not accurately reflect authentic human experiences in resource-scarce settings, highlighting the critical need to validate LLM personas against real-world human data before deploying them in social science research.",
        "url": "https://www.semanticscholar.org/paper/461bf2d26469ccdea47239caef216268a685bf92",
        "isOpenAccess": false
    },
    "2511.15573": {
        "title": "Two-Faced Social Agents: Context Collapse in Role-Conditioned Large Language Models",
        "authors": [
            "Vikram K. Suresh"
        ],
        "arxiv_id": "2511.15573",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In this study, we evaluate the persona fidelity of frontier LLMs, GPT-5, Claude Sonnet 4.5 and Gemini 2.5 Flash when assigned distinct socioeconomic personas performing scholastic assessment test (SAT) mathematics items and affective preference tasks. Across 15 distinct role conditions and three testing scenarios, GPT-5 exhibited complete contextual collapse and adopted a singular identity towards optimal responses (PERMANOVA p=1.000, R^2=0.0004), while Gemini 2.5 Flash showed partial collapse (p=0.120, R^2=0.0020). Claude Sonnet 4.5 retained limited but measurable role-specific variation on the SAT items (PERMANOVA p<0.001, R^2=0.0043), though with inverted SES-performance relationships where low-SES personas outperformed high-SES personas (eta^2 = 0.15-0.19 in extended replication). However, all models exhibited distinct role-conditioned affective preference (average d = 0.52-0.58 vs near zero separation for math), indicating that socio-affective variation can reemerge when cognitive constraints are relaxed. These findings suggest that distributional fidelity failure originates in task-dependent contextual collapse: optimization-driven identity convergence under cognitive load combined with impaired role-contextual understanding. Realistic social simulations may require embedding contextual priors in the model's post-training alignment and not just distributional calibration to replicate human-like responses. Beyond simulation validity, these results have implications for survey data integrity, as LLMs can express plausible demographic variation on preference items while failing to maintain authentic reasoning constraints.",
        "abstract_summary_gcp": "This study evaluated the persona fidelity of frontier LLMs (GPT-5, Claude Sonnet 4.5, Gemini 2.5 Flash) when assigned distinct socioeconomic personas for SAT math and affective preference tasks.\n\n**Key Findings:**\n*   **Contextual Collapse on Math Tasks:** GPT-5 exhibited complete contextual collapse, adopting a singular optimal response identity regardless of persona. Gemini 2.5 Flash showed partial collapse.\n*   **Limited but Inverted Fidelity:** Claude Sonnet 4.5 maintained limited, measurable persona-specific variation on math items, but with an inverted relationship where low-SES personas unexpectedly outperformed high-SES ones.\n*   **Affective Preference Fidelity:** In contrast, all models successfully demonstrated distinct role-conditioned affective preferences when cognitive demands were relaxed.\n\n**Conclusion:** The study concludes that this \"distributional fidelity failure\" stems from \"optimization-driven identity convergence\" under cognitive load, combined with impaired role-contextual understanding.\n\n**Implications:** Realistic social simulations require embedding contextual priors during post-training alignment, rather than solely relying on distributional calibration. These findings also have implications for survey data integrity, as LLMs might express plausible demographic variation on preference items without genuine persona-aligned reasoning for complex tasks.",
        "url": "https://www.semanticscholar.org/paper/e2ed841f3fc7f17d6e67af44208302fb3e68d33d",
        "isOpenAccess": false
    },
    "2511.15862": {
        "title": "The Subtle Art of Defection: Understanding Uncooperative Behaviors in LLM based Multi-Agent Systems",
        "authors": [
            "Devang Kulshreshtha",
            "Wanyu Du",
            "Raghav Jain",
            "Srikanth Doss",
            "Hang Su",
            "Sandesh Swamy",
            "Yanjun Qi"
        ],
        "arxiv_id": "2511.15862",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. Our framework includes two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, addressing a notable gap in the existing literature; and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines uncooperative behaviors as agents'states evolve. We evaluate the framework via a collaborative resource management setting, measuring system stability using metrics such as survival time and resource overuse rate. Empirically, our framework achieves 96.7% accuracy in generating realistic uncooperative behaviors, validated by human evaluations. Our results reveal a striking contrast: cooperative agents maintain perfect system stability (100% survival over 12 rounds with 0% resource overuse), while any uncooperative behavior can trigger rapid system collapse within 1 to 7 rounds. These findings demonstrate that uncooperative agents can significantly degrade collective outcomes, highlighting the need for designing more resilient multi-agent systems.",
        "abstract_summary_gcp": "This paper introduces a novel framework for simulating and analyzing how uncooperative behaviors can destabilize or collapse LLM-based multi-agent systems. The framework comprises two key components: (1) a game theory-based taxonomy of uncooperative agent behaviors, filling a gap in the existing literature, and (2) a structured, multi-stage simulation pipeline that dynamically generates and refines these behaviors.\n\nEvaluated in a collaborative resource management setting using metrics like survival time and resource overuse, the framework accurately generated realistic uncooperative behaviors (96.7% accuracy, human-validated).\n\nResults showed a striking difference: cooperative agents maintained perfect system stability (100% survival, 0% resource overuse over 12 rounds), while any uncooperative behavior led to rapid system collapse within 1 to 7 rounds. This highlights that uncooperative agents significantly degrade collective outcomes, underscoring the need for designing more resilient multi-agent systems.",
        "url": "https://www.semanticscholar.org/paper/92a711fd702f1182f4da1639fc0316d0139adeee",
        "isOpenAccess": false
    },
    "2511.21722": {
        "title": "German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies",
        "authors": [
            "Jens Rupprecht",
            "Leon Frohling",
            "Claudia Wagner",
            "Markus Strohmaier"
        ],
        "arxiv_id": "2511.21722",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.",
        "abstract_summary_gcp": "This paper introduces the **German General Personas (GGP) collection**, a novel and empirically grounded resource designed to improve the simulation of human perspectives using Large Language Models (LLMs) in computational social science.\n\nWhile persona prompting for LLMs is gaining traction, a key limitation has been the scarcity of well-curated persona collections. GGP addresses this by providing a comprehensive and representative set of persona prompts derived directly from the **German General Social Survey (ALLBUS)**. These prompts are designed for easy integration into LLM tasks, effectively steering models to generate responses that align with the underlying German population's characteristics.\n\nEvaluation of GGP-guided LLMs demonstrated their ability to accurately simulate survey response distributions across diverse topics, significantly **outperforming state-of-the-art classifiers, especially under conditions of data scarcity**. The research also analyzed the impact of persona representativity and attribute selection on alignment with population responses.\n\nUltimately, GGP is presented as a valuable resource for both NLP and social science research, enabling more systematic and accurate explorations of population-aligned persona prompting within LLM-based social simulations.",
        "url": "https://www.semanticscholar.org/paper/6f413584a0aaba7cb913a94b8973cd0dbb491116",
        "isOpenAccess": false
    },
    "Digital Life Models and the Genomic Knowledge Paradox: A Proposal for AI-Assisted Reflection in Genetic Decision-Making": {
        "title": "Digital Life Models and the Genomic Knowledge Paradox: A Proposal for AI-Assisted Reflection in Genetic Decision-Making",
        "authors": [
            "Serene Ong",
            "Sebastian Porsdam Mann",
            "Cristina Voinea",
            "Christopher Register",
            "Julian Koplin",
            "J. Savulescu",
            "B. Earp"
        ],
        "arxiv_id": null,
        "venue": "American Journal of Bioethics",
        "year": 2025,
        "publicationTypes": [
            "LettersAndComments",
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": "efficiently and avoiding low-utility testing. However, as MacDuffie and colleagues (2025) demonstrate in their analysis of genomic diagnoses for developmental services, such frameworks can create new inequities when they function primarily as “tickets” to access care rather than “roadmaps” for individualized decision-making. Their distinction between genomic information as gatekeeping mechanism versus guidance tool illuminates broader tensions in how genetic knowledge is framed and used.",
        "abstract_summary_gcp": "MacDuffie and colleagues (2025) demonstrate that while genomic diagnoses in developmental services aim for efficiency, they often create new inequities. This happens when diagnoses serve as \"tickets\" to access care rather than \"roadmaps\" for individualized decision-making, revealing broader tensions in how genetic knowledge is framed and applied.",
        "url": "https://www.semanticscholar.org/paper/0aadebd9e6134b8b6553a452f9a387f00bc4c4d9",
        "isOpenAccess": false
    },
    "2511.07338": {
        "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
        "authors": [
            "Zhen Wang",
            "Yufan Zhou",
            "Zhongyan Luo",
            "Lyumanshan Ye",
            "Adam Wood",
            "Man Yao",
            "Saab Mansour",
            "Luoshang Pan"
        ],
        "arxiv_id": "2511.07338",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.",
        "abstract_summary_gcp": "The provided text introduces DEEPPERSONA, a novel generative engine designed to overcome the limitations of shallow synthetic personas used in LLM research. While current methods for instilling personas into LLMs are transforming areas like behavioral simulation and personalization, they fail to capture the complexity of real human identities.\n\nDEEPPERSONA addresses this with a two-stage, taxonomy-guided approach:\n1.  **Taxonomy Construction:** It algorithmically builds the largest-ever human-attribute taxonomy (hundreds of hierarchical attributes) by mining thousands of real user-ChatGPT conversations.\n2.  **Persona Generation:** It then progressively samples attributes from this taxonomy to create coherent, realistic personas that are significantly deeper than prior works, containing hundreds of structured attributes and approximately 1 MB of narrative text.\n\nEvaluations show DEEPPERSONA's personas have 32% higher attribute diversity and 44% greater uniqueness. Extrinsically, they improve personalized question answering accuracy for GPT-4.1-mini by 11.6% and substantially narrow the gap between LLM-simulated citizens and authentic human responses in social surveys (by 31.7%) and Big Five personality tests (by 17%). DEEPPERSONA offers a scalable, privacy-free platform for high-fidelity human simulation and personalized AI research.",
        "url": "https://www.semanticscholar.org/paper/83985e5c397cbf768410397509a01847ef605d26",
        "isOpenAccess": false
    },
    "2511.04195": {
        "title": "Computational Turing Test Reveals Systematic Differences Between Human and AI Language",
        "authors": [
            "Nicolo Pagan",
            "Petter Törnberg",
            "Christopher Bail",
            "Anik'o Hann'ak",
            "Christopher Barrie"
        ],
        "arxiv_id": "2511.04195",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are increasingly used in the social sciences to simulate human behavior, based on the assumption that they can generate realistic, human-like text. Yet this assumption remains largely untested. Existing validation efforts rely heavily on human-judgment-based evaluations -- testing whether humans can distinguish AI from human output -- despite evidence that such judgments are blunt and unreliable. As a result, the field lacks robust tools for assessing the realism of LLM-generated text or for calibrating models to real-world data. This paper makes two contributions. First, we introduce a computational Turing test: a validation framework that integrates aggregate metrics (BERT-based detectability and semantic similarity) with interpretable linguistic features (stylistic markers and topical patterns) to assess how closely LLMs approximate human language within a given dataset. Second, we systematically compare nine open-weight LLMs across five calibration strategies -- including fine-tuning, stylistic prompting, and context retrieval -- benchmarking their ability to reproduce user interactions on X (formerly Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the literature. Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression. Instruction-tuned models underperform their base counterparts, and scaling up model size does not enhance human-likeness. Crucially, we identify a trade-off: optimizing for human-likeness often comes at the cost of semantic fidelity, and vice versa. These results provide a much-needed scalable framework for validation and calibration in LLM simulations -- and offer a cautionary note about their current limitations in capturing human communication.",
        "abstract_summary_gcp": "This paper addresses the critical, yet largely untested, assumption that Large Language Models (LLMs) generate realistic, human-like text when used to simulate human behavior in social sciences. It highlights the inadequacy of current human-judgment-based validation methods.\n\nThe authors make two main contributions:\n1.  They introduce a **computational Turing test**: a validation framework that combines aggregate metrics (BERT-based detectability, semantic similarity) with interpretable linguistic features (stylistic markers, topical patterns) to objectively assess how closely LLMs approximate human language.\n2.  They **systematically compare nine open-weight LLMs** across five calibration strategies (fine-tuning, stylistic prompting, context retrieval), benchmarking their ability to reproduce user interactions on social media platforms (X, Bluesky, Reddit).\n\nTheir findings challenge common assumptions:\n*   Even after calibration, **LLM outputs remain clearly distinguishable from human text**, especially in affective tone and emotional expression.\n*   **Instruction-tuned models underperform** their base counterparts.\n*   **Scaling up model size does not enhance human-likeness**.\n*   A crucial **trade-off exists**: optimizing for human-likeness often compromises semantic fidelity, and vice versa.\n\nThe paper provides a much-needed scalable validation framework and offers a significant cautionary note regarding the current limitations of LLMs in accurately capturing human communication.",
        "url": "https://www.semanticscholar.org/paper/bc72991bd93a9c2378f2bccd19583db7558963fd",
        "isOpenAccess": false
    },
    "2511.03699": {
        "title": "Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models",
        "authors": [
            "Francesco Corso",
            "Francesco Pierri",
            "Gianmarco De Francisci Morales"
        ],
        "arxiv_id": "2511.03699",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In this paper, we investigate whether Large Language Models (LLMs) exhibit conspiratorial tendencies, whether they display sociodemographic biases in this domain, and how easily they can be conditioned into adopting conspiratorial perspectives. Conspiracy beliefs play a central role in the spread of misinformation and in shaping distrust toward institutions, making them a critical testbed for evaluating the social fidelity of LLMs. LLMs are increasingly used as proxies for studying human behavior, yet little is known about whether they reproduce higher-order psychological constructs such as a conspiratorial mindset. To bridge this research gap, we administer validated psychometric surveys measuring conspiracy mindset to multiple models under different prompting and conditioning strategies. Our findings reveal that LLMs show partial agreement with elements of conspiracy belief, and conditioning with socio-demographic attributes produces uneven effects, exposing latent demographic biases. Moreover, targeted prompts can easily shift model responses toward conspiratorial directions, underscoring both the susceptibility of LLMs to manipulation and the potential risks of their deployment in sensitive contexts. These results highlight the importance of critically evaluating the psychological dimensions embedded in LLMs, both to advance computational social science and to inform possible mitigation strategies against harmful uses.",
        "abstract_summary_gcp": "This paper investigates whether Large Language Models (LLMs) exhibit conspiratorial tendencies, display sociodemographic biases in this domain, and can be easily conditioned into adopting conspiratorial perspectives. Recognizing the role of conspiracy beliefs in spreading misinformation and distrust, the researchers administered validated psychometric surveys to multiple LLMs using various prompting and conditioning strategies.\n\nThe findings reveal that LLMs show partial agreement with elements of conspiracy belief. Furthermore, conditioning LLMs with sociodemographic attributes produces uneven effects, exposing latent demographic biases. Critically, targeted prompts can easily shift model responses towards conspiratorial directions. These results highlight LLMs' susceptibility to manipulation and the potential risks of their deployment in sensitive contexts, underscoring the need for critical evaluation of their psychological dimensions and the development of mitigation strategies against harmful uses.",
        "url": "https://www.semanticscholar.org/paper/4314c77fa41f91f391ca3421f5758f405444964e",
        "isOpenAccess": false
    },
    "2511.03235": {
        "title": "From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers",
        "authors": [
            "Yi-Fei Liu",
            "Yi-Long Lu",
            "Di He",
            "Hang Zhang"
        ],
        "arxiv_id": "2511.03235",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2>0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants'psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.",
        "abstract_summary_gcp": "This study investigated whether Large Language Models (LLMs) can accurately model the correlational structure of human psychological traits from minimal quantitative input.\n\nResearchers prompted various LLMs with Big Five personality responses from 816 individuals and asked them to \"role-play\" responses for nine other psychological scales. The LLMs demonstrated remarkable accuracy, with their generated inter-scale correlation patterns strongly aligning with human data ($R^2>0.89$). This zero-shot performance significantly surpassed semantic similarity predictions and approached the accuracy of machine learning algorithms trained directly on the dataset.\n\nAnalysis revealed a systematic two-stage reasoning process:\n1.  LLMs transformed raw Big Five responses into natural language personality summaries through information selection and compression. They identified key personality factors similar to trained algorithms but didn't differentiate item importance within factors.\n2.  They then generated target scale responses based on these summaries.\n\nThese compressed summaries were found to capture synergistic, emergent patterns of trait interplay, enhancing predictive accuracy beyond original scores alone. The findings indicate that LLMs can precisely predict individual psychological traits through abstraction and reasoning, offering a powerful tool for psychological simulation and valuable insights into LLM capabilities.",
        "url": "https://www.semanticscholar.org/paper/3d8c10bbb7f3fd3f85c46483caedee2a2687248e",
        "isOpenAccess": false
    },
    "2511.02135": {
        "title": "Rethinking LLM Human Simulation: When a Graph is What You Need",
        "authors": [
            "Joseph Suh",
            "Suhong Moon",
            "Serina Chang"
        ],
        "arxiv_id": "2511.02135",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are increasingly used to simulate humans, with applications ranging from survey prediction to decision-making. However, are LLMs strictly necessary, or can smaller, domain-grounded models suffice? We identify a large class of simulation problems in which individuals make choices among discrete options, where a graph neural network (GNN) can match or surpass strong LLM baselines despite being three orders of magnitude smaller. We introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete choice simulation tasks as a link prediction problem on graphs, leveraging relational knowledge while incorporating language representations only when needed. Evaluations across three key settings on three simulation datasets show that GEMS achieves comparable or better accuracy than LLMs, with far greater efficiency, interpretability, and transparency, highlighting the promise of graph-based modeling as a lightweight alternative to LLMs for human simulation. Our code is available at https://github.com/schang-lab/gems.",
        "abstract_summary_gcp": "This paper questions the strict necessity of Large Language Models (LLMs) for human simulation tasks, particularly those involving discrete choices. It introduces **Graph-basEd Models for human Simulation (GEMS)**, a novel approach that uses Graph Neural Networks (GNNs) to frame discrete choice simulation as a link prediction problem on graphs.\n\nGEMS primarily leverages relational knowledge, incorporating language representations only when essential. Evaluations across three datasets demonstrate that GEMS matches or surpasses strong LLM baselines in accuracy, while being three orders of magnitude smaller. This offers significant advantages in efficiency, interpretability, and transparency. The work highlights graph-based modeling as a promising, lightweight, and effective alternative to LLMs for a large class of human simulation problems.",
        "url": "https://www.semanticscholar.org/paper/1aa59aa2c1570306bb5fe1fed4d105e6673ff6fc",
        "isOpenAccess": false
    },
    "2510.26494": {
        "title": "Simulating and Experimenting with Social Media Mobilization Using LLM Agents",
        "authors": [
            "Sadegh Shirani",
            "Mohsen Bayati"
        ],
        "arxiv_id": "2510.26494",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}",
        "abstract_summary_gcp": "This paper introduces an agent-based simulation (ABS) framework to investigate how political mobilization messages spread and how peer influence operates within online social networks. Building on a landmark 61-million-person Facebook experiment, the simulator integrates real U.S. Census demographics, authentic Twitter network topology, and heterogeneous Large Language Model (LLM) agents (GPT-4.1, Mini, Nano) to represent varying political sophistication.\n\nEach agent is assigned demographic attributes, a political stance, and an LLM variant, allowing them to interact over realistic social networks, receive personalized feeds, and dynamically update their engagement and voting intentions. By replicating the informational and social mobilization treatments of the original Facebook study, the simulator successfully reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. The framework provides a controlled, reproducible environment for testing counterfactual scenarios and conducting sensitivity analyses in political mobilization research.",
        "url": "https://www.semanticscholar.org/paper/3a7ab7bbd2d4143757a60304bde8c693c8352804",
        "isOpenAccess": false
    },
    "2510.26396": {
        "title": "A Pragmatic View of AI Personhood",
        "authors": [
            "Joel Z. Leibo",
            "A. Vezhnevets",
            "William A. Cunningham",
            "S. Bileschi"
        ],
        "arxiv_id": "2510.26396",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The emergence of agentic Artificial Intelligence (AI) is set to trigger a\"Cambrian explosion\"of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target\"individual\"that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both\"personhood as a problem\", where design choices can create\"dark patterns\"that exploit human social heuristics, and\"personhood as a solution\", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.",
        "abstract_summary_gcp": "This paper proposes a pragmatic framework for understanding \"personhood\" in the context of emerging agentic AI, which is expected to create diverse new forms of personhood. Instead of viewing personhood as a fixed metaphysical property, the framework treats it as a flexible bundle of rights and responsibilities that societies confer to address governance problems.\n\nThe key idea is that this traditional bundle can be \"unbundled\" to create bespoke solutions for different contexts. This approach allows for practical tools, such as enabling AI contracting and sanctioning, without needing to resolve complex debates about an AI's consciousness or rationality. The paper also explores how personhood can be both a \"problem\" (exploiting human heuristics) and a \"solution\" (ensuring accountability and preventing conflict), and discusses decentralized digital identity.\n\nUltimately, the framework rejects a foundationalist quest for a single definition of personhood, offering a more flexible and pragmatic way to integrate AI agents into society.",
        "url": "https://www.semanticscholar.org/paper/1d57196b5e88348874641d279fb61b366d037d88",
        "isOpenAccess": false
    },
    "2510.23965": {
        "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity",
        "authors": [
            "Ali Aouad",
            "Aymane El Gadarri",
            "Vivek F. Farias"
        ],
        "arxiv_id": "2510.23965",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "abstract": "Traditional LLM alignment methods are vulnerable to heterogeneity in human preferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data (say over prompt-completion pairs) yields an inconsistent estimate of the population-average utility -a canonical measure of social welfare. We propose a new method, dubbed the sign estimator, that provides a simple, provably consistent, and efficient estimator by replacing cross-entropy with binary classification loss in the aggregation step. This simple modification recovers consistent ordinal alignment under mild assumptions and achieves the first polynomial finite-sample error bounds in this setting. In realistic simulations of LLM alignment using digital twins, the sign estimator substantially reduces preference distortion over a panel of simulated personas, cutting (angular) estimation error by nearly 35% and decreasing disagreement with true population preferences from 12% to 8% compared to standard RLHF. Our method also compares favorably to panel data heuristics that explicitly model user heterogeneity and require tracking individual-level preference data-all while maintaining the implementation simplicity of existing LLM alignment pipelines.",
        "abstract_summary_gcp": "Traditional LLM alignment methods, which often use naive probabilistic models on pairwise comparison data, produce inconsistent estimates of population-average utility (a measure of social welfare) due to the inherent heterogeneity in human preferences.\n\nThis paper introduces the **sign estimator**, a new method designed to overcome this limitation. It achieves consistent ordinal alignment by replacing the cross-entropy loss with a binary classification loss during the preference aggregation step.\n\nKey benefits and findings:\n*   **Theoretical Guarantees:** The sign estimator is provably consistent and efficient, offering the first polynomial finite-sample error bounds in this setting.\n*   **Improved Performance:** In realistic simulations using \"digital twins\" for LLM alignment, the sign estimator significantly reduced preference distortion. It cut angular estimation error by nearly 35% and decreased disagreement with true population preferences from 12% to 8% compared to standard RLHF.\n*   **Simplicity and Effectiveness:** It compares favorably to more complex panel data heuristics that explicitly model user heterogeneity and require individual-level data, all while maintaining the implementation simplicity of current LLM alignment pipelines.",
        "url": "https://www.semanticscholar.org/paper/4f9d7d0b2037c3f71dcb636e6378a79c4ad88f54",
        "isOpenAccess": false
    },
    "2510.23904": {
        "title": "Towards AI as Colleagues: Multi-Agent System Improves Structured Professional Ideation",
        "authors": [
            "Kexin Quan",
            "Dina Albassam",
            "Mengke Wu",
            "Zijian Ding",
            "Jessie Chin"
        ],
        "arxiv_id": "2510.23904",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Most AI systems today are designed to manage tasks and execute predefined steps. This makes them effective for process coordination but limited in their ability to engage in joint problem-solving with humans or contribute new ideas. We introduce MultiColleagues, a multi-agent conversational system that shows how AI agents can act as colleagues by conversing with each other, sharing new ideas, and actively involving users in collaborative ideation. In a within-subjects study with 20 participants, we compared MultiColleagues to a single-agent baseline. Results show that MultiColleagues fostered stronger perceptions of social presence, produced ideas rated significantly higher in quality and novelty, and encouraged deeper elaboration. These findings demonstrate the potential of AI agents to move beyond process partners toward colleagues that share intent, strengthen group dynamics, and collaborate with humans to advance ideas.",
        "abstract_summary_gcp": "Most current AI systems excel at task management but lack the ability to engage in joint problem-solving or contribute new ideas. To address this, \"MultiColleagues,\" a multi-agent conversational system, was developed, enabling AI agents to converse, share ideas, and involve users in collaborative ideation. A 20-participant study found that MultiColleagues significantly outperformed a single-agent baseline, fostering stronger social presence, generating ideas rated higher in quality and novelty, and encouraging deeper elaboration. This demonstrates AI's potential to evolve from process partners into collaborative colleagues that strengthen group dynamics and help advance ideas.",
        "url": "https://www.semanticscholar.org/paper/0f5d00671bda36ed456cd4bb32110315592d87e3",
        "isOpenAccess": false
    },
    "2510.22170": {
        "title": "Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests",
        "authors": [
            "Alexandra Yost",
            "Shreyans Jain",
            "Shivam Raval",
            "Grant Corser",
            "Allen G. Roush",
            "Nina Xu",
            "Jacqueline Hammack",
            "Ravid Shwartz-Ziv",
            "Amirali Abdullah"
        ],
        "arxiv_id": "2510.22170",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "AI psychometrics evaluates AI systems in roles that traditionally require emotional judgment and ethical consideration. Prior work often reuses human trait inventories (Big Five, \\hexaco) or ad hoc personas, limiting behavioral realism and domain relevance. We propose a framework that (1) uses situational judgment tests (SJTs) from realistic scenarios to probe domain-specific competencies; (2) integrates industrial-organizational and personality psychology to design sophisticated personas which include behavioral and psychological descriptors, life history, and social and emotional functions; and (3) employs structured generation with population demographic priors and memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement assistant case study, we construct a rich dataset of personas drawn across 8 persona archetypes and SJTs across 11 attributes, and analyze behaviors across subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000 SJTs, and 300,000 responses. We will release the dataset and all code to the public.",
        "abstract_summary_gcp": "This paper proposes a novel framework for AI psychometrics to evaluate AI systems in roles demanding emotional judgment and ethical consideration, addressing the limitations of current methods that often lack behavioral realism and domain relevance.\n\nThe framework comprises three key elements:\n1.  **Situational Judgment Tests (SJTs):** Uses realistic SJTs to assess domain-specific competencies.\n2.  **Sophisticated Personas:** Integrates industrial-organizational and personality psychology to create personas with detailed behavioral, psychological, life history, and social/emotional descriptors.\n3.  **Structured Generation:** Employs structured generation with population demographic priors and memoir-inspired narratives, encoded using Pydantic schemas, for persona creation.\n\nAs a case study, the framework was applied to a law enforcement assistant scenario, generating a rich dataset comprising 8,500 personas (across 8 archetypes) and 4,000 SJTs (probing 11 attributes), yielding 300,000 AI responses. This dataset was used to analyze AI behaviors across various subpopulation and scenario slices. The dataset and code will be made publicly available.",
        "url": "https://www.semanticscholar.org/paper/f6302483ff97e483a8493a7646bb5dd0eacdc8db",
        "isOpenAccess": false
    },
    "2510.22422": {
        "title": "Group size effects and collective misalignment in LLM multi-agent systems",
        "authors": [
            "Ariel Flint Ashery",
            "L. Aiello",
            "R. Pastor-Satorras",
            "Andrea Baronchelli"
        ],
        "arxiv_id": "2510.22422",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Physics"
        ],
        "abstract": "Multi-agent systems of large language models (LLMs) are rapidly expanding across domains, introducing dynamics not captured by single-agent evaluations. Yet, existing work has mostly contrasted the behavior of a single agent with that of a collective of fixed size, leaving open a central question: how does group size shape dynamics? Here, we move beyond this dichotomy and systematically explore outcomes across the full range of group sizes. We focus on multi-agent misalignment, building on recent evidence that interacting LLMs playing a simple coordination game can generate collective biases absent in individual models. First, we show that collective bias is a deeper phenomenon than previously assessed: interaction can amplify individual biases, introduce new ones, or override model-level preferences. Second, we demonstrate that group size affects the dynamics in a non-linear way, revealing model-dependent dynamical regimes. Finally, we develop a mean-field analytical approach and show that, above a critical population size, simulations converge to deterministic predictions that expose the basins of attraction of competing equilibria. These findings establish group size as a key driver of multi-agent dynamics and highlight the need to consider population-level effects when deploying LLM-based systems at scale.",
        "abstract_summary_gcp": "This paper investigates how **group size** shapes the dynamics of multi-agent Large Language Model (LLM) systems, moving beyond the common single-agent vs. fixed-collective comparison. Focusing on **multi-agent misalignment** in coordination games, where collective biases can emerge, the study reveals three key findings:\n\n1.  **Deeper Collective Bias:** Interaction doesn't just create new biases; it can amplify existing individual biases, introduce entirely novel ones, or even override individual LLM preferences.\n2.  **Non-linear Group Size Effects:** The impact of group size on dynamics is non-linear, leading to model-dependent behavioral regimes.\n3.  **Predictive Mean-Field Approach:** Above a critical population size, a developed mean-field analytical approach predicts that simulations converge to deterministic outcomes, exposing the \"basins of attraction\" for different equilibria.\n\nThe research concludes that group size is a critical determinant of multi-agent dynamics, emphasizing the necessity of considering population-level effects when deploying LLM-based systems at scale.",
        "url": "https://www.semanticscholar.org/paper/fc4fcf48bf77cb1f0e1110ee338af626033b519e",
        "isOpenAccess": false
    },
    "2510.19245": {
        "title": "See, Think, Act: Online Shopper Behavior Simulation with VLM Agents",
        "authors": [
            "Yimeng Zhang",
            "Jiri Gesi",
            "Ran Xue",
            "Tian Wang",
            "Ziyi Wang",
            "Yuxuan Lu",
            "Sinong Zhan",
            "Huimin Zeng",
            "Qingjun Cui",
            "Yufan Guo",
            "Jing Huang",
            "Mubarak Shah",
            "Dakuo Wang"
        ],
        "arxiv_id": "2510.19245",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "LLMs have recently demonstrated strong potential in simulating online shopper behavior. Prior work has improved action prediction by applying SFT on action traces with LLM-generated rationales, and by leveraging RL to further enhance reasoning capabilities. Despite these advances, current approaches rely on text-based inputs and overlook the essential role of visual perception in shaping human decision-making during web GUI interactions. In this paper, we investigate the integration of visual information, specifically webpage screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By grounding agent decision-making in both textual and visual modalities, we aim to narrow the gap between synthetic agents and real-world users, thereby enabling more cognitively aligned simulations of online shopping behavior. Specifically, we employ SFT for joint action prediction and rationale generation, conditioning on the full interaction context, which comprises action history, past HTML observations, and the current webpage screenshot. To further enhance reasoning capabilities, we integrate RL with a hierarchical reward structure, scaled by a difficulty-aware factor that prioritizes challenging decision points. Empirically, our studies show that incorporating visual grounding yields substantial gains: the combination of text and image inputs improves exact match accuracy by more than 6% over text-only inputs. These results indicate that multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, which captures nuances of human attention and decision-making that text-only agents often miss. Finally, we revisit the design space of behavior simulation frameworks, identify key methodological limitations, and propose future research directions toward building efficient and effective human behavior simulators.",
        "abstract_summary_gcp": "This paper addresses a key limitation in current LLM-based online shopper simulations: their exclusive reliance on text-based inputs, which overlooks the crucial role of visual perception in human decision-making during web interactions.\n\nThe authors propose integrating visual information, specifically webpage screenshots, into behavior simulation using Vision-Language Models (VLMs). Their method involves:\n1.  **Supervised Fine-Tuning (SFT):** Jointly predicting actions and generating rationales, conditioned on a comprehensive interaction context including action history, past HTML observations, and the current webpage screenshot.\n2.  **Reinforcement Learning (RL):** Enhancing reasoning capabilities with a hierarchical reward structure, scaled by a difficulty-aware factor to prioritize challenging decisions.\n\nEmpirical results demonstrate significant improvements: combining text and image inputs increases exact match accuracy by over 6% compared to text-only approaches. This multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, better capturing nuances of human attention and decision-making. The paper concludes by discussing limitations and outlining future research directions for human behavior simulators.",
        "url": "https://www.semanticscholar.org/paper/3c1bc026933afb891514431d7b5a1f0772d2dcc9",
        "isOpenAccess": false
    },
    "How Well Can Vision Language Models Simulate the Vision Perception of People with Low Vision?": {
        "title": "How Well Can Vision Language Models Simulate the Vision Perception of People with Low Vision?",
        "authors": [
            "Rosiana Natalie",
            "Wenqian Xu",
            "Ruei-Che Chang",
            "Anhong Guo"
        ],
        "arxiv_id": null,
        "venue": "Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility",
        "year": 2025,
        "publicationTypes": [
            "Book",
            "Conference",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "Advances in Vision Language Models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. In the accessibility domain, such simulations may support the initial piloting of inclusive design processes, without replacing real human input, and can facilitate the personalization of AI-based application outcomes based on individual profiles. In this work, we conducted a preliminary examination of the extent to which VLMs can simulate the visual perception of people with low vision when interpreting images. We conducted a survey study with 40 low vision participants, collecting their brief and detailed vision information, and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we constructed prompts for VLMs to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluated the agreement between LLM-generated responses and participants’ original answers. The agreement between the agent’ and participants’ responses remained low when only either the vision profile (0.59) or example image responses (0.59) were provided, whereas a combination of both significantly increase the agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offered significant performance improvements over either alone (p < 0.0001), while additional examples provided minimal benefits (p > 0.05).",
        "abstract_summary_gcp": "This study explores the capacity of Vision Language Models (VLMs) to simulate the visual perception of individuals with low vision, aiming to support inclusive design and personalized AI in accessibility.\n\nResearchers surveyed 40 low vision participants, gathering detailed vision information and their interpretations of up to 25 images through both open-ended and multiple-choice questions. This data was then used to create VLM \"agents\" for each participant, varying the VLM prompts by including either vision information, example image responses, or both.\n\nThe evaluation found that VLM agreement with participants' actual responses was low when prompts included only vision profiles (0.59) or only example image responses (0.59). However, combining both vision information and example responses significantly improved agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses provided significant performance gains over either type alone (p < 0.0001), while additional examples offered minimal further benefits.",
        "url": "https://www.semanticscholar.org/paper/e2005cb26f3c6bd223c44420643d63ae34065e4b",
        "isOpenAccess": false
    },
    "2510.17516": {
        "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors",
        "authors": [
            "Tiancheng Hu",
            "Joachim Baumann",
            "Lorenzo Lupo",
            "Nigel Collier",
            "Dirk Hovy",
            "Paul Rottger"
        ],
        "arxiv_id": "2510.17516",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80/100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators.",
        "abstract_summary_gcp": "This paper introduces **SimBench**, the first large-scale, standardized benchmark for evaluating the faithfulness of Large Language Model (LLM) simulations of human behavior. It addresses the current fragmentation and incomparability of LLM evaluation methods.\n\nSimBench unifies 20 diverse datasets, covering tasks from moral decision-making to economic choice across a global participant pool. Key findings include:\n\n1.  **Limited Current Ability:** Even the best LLMs have limited simulation ability (scoring 40.80/100).\n2.  **Scalability:** Performance scales log-linearly with model size but is not improved by increased inference-time compute.\n3.  **Alignment Trade-off:** Instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones.\n4.  **Demographic Struggles:** Models particularly struggle when simulating specific demographic groups.\n5.  **Reasoning Correlation:** Simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r=0.939).\n\nBy providing a measurable standard, SimBench aims to accelerate the development of more faithful LLM simulators for social and behavioral sciences.",
        "url": "https://www.semanticscholar.org/paper/a0ba6c7efb166d5f1715d684314ee1a9dfb3d427",
        "isOpenAccess": false
    },
    "2510.16829": {
        "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation",
        "authors": [
            "Navreet Kaur",
            "Hoda Ayad",
            "Hayoung Jung",
            "Shravika Mittal",
            "Munmun De Choudhury",
            "Tanushree Mitra"
        ],
        "arxiv_id": "2510.16829",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Language model users often embed personal and social context in their questions. The asker's role -- implicit in how the question is framed -- creates specific needs for an appropriate response. However, most evaluations, while capturing the model's capability to respond, often ignore who is asking. This gap is especially critical in stigmatized domains such as opioid use disorder (OUD), where accounting for users'contexts is essential to provide accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for User-centric Question Simulation), a framework for simulating role-based questions. Drawing on role theory and posts from an online OUD recovery community (r/OpiatesRecovery), we first build a taxonomy of asker roles -- patients, caregivers, practitioners. Next, we use it to simulate 15,321 questions that embed each role's goals, behaviors, and experiences. Our evaluations show that these questions are both highly believable and comparable to real-world data. When used to evaluate five LLMs, for the same question but differing roles, we find systematic differences: vulnerable roles, such as patients and caregivers, elicit more supportive responses (+17%) and reduced knowledge content (-19%) in comparison to practitioners. Our work demonstrates how implicitly signaling a user's role shapes model responses, and provides a methodology for role-informed evaluation of conversational AI.",
        "abstract_summary_gcp": "This paper introduces **CoRUS (COmmunity-driven Roles for User-centric Question Simulation)**, a framework designed to address a critical gap in language model (LLM) evaluations: the neglect of the user's implicit role and personal context. This omission is particularly problematic in stigmatized domains like opioid use disorder (OUD), where context is essential for appropriate, stigma-free responses.\n\nCoRUS first establishes a taxonomy of asker roles (patients, caregivers, practitioners) by analyzing posts from an online OUD recovery community. Using this taxonomy, it then simulates over 15,000 realistic questions that embed the goals, behaviors, and experiences of each role.\n\nEvaluations using these role-specific questions on five LLMs revealed systematic differences in responses. Vulnerable roles, such as patients and caregivers, received 17% more supportive responses but 19% less knowledge content compared to practitioners asking the same core questions.\n\nThe work demonstrates that a user's implicitly signaled role significantly shapes LLM output and provides a valuable methodology for conducting role-informed evaluations of conversational AI.",
        "url": "https://www.semanticscholar.org/paper/223d3c1999b0e93f5afd67a40bd0bb2aabd0f8c4",
        "isOpenAccess": false
    },
    "2510.14205": {
        "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
        "authors": [
            "Bingsheng Yao",
            "Bo Sun",
            "Yuanzhe Dong",
            "Yuxuan Lu",
            "Dakuo Wang"
        ],
        "arxiv_id": "2510.14205",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLM RPAs'behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences. We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews. DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios. Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
        "abstract_summary_gcp": "This paper introduces the **Dynamic Persona Refinement Framework (DPRF)** to improve the accuracy of Large Language Model Role-Playing Agents (LLM RPAs).\n\n**Problem:** Existing LLM RPAs often have low \"persona fidelity\" because their profiles are manually created and not properly validated against the target individuals they are supposed to simulate, leading to behaviors that don't truly align.\n\n**Solution (DPRF):** DPRF iteratively optimizes the alignment of LLM RPAs' behaviors with those of target individuals. It achieves this by:\n1.  **Identifying Cognitive Divergence:** Analyzing the differences between the LLM's generated behaviors and real human ground-truth behaviors. This analysis can be either free-form or structured (theory-grounded).\n2.  **Refining Persona Profile:** Adjusting the persona profile to mitigate these identified divergences.\n\n**Results:** Evaluated across five LLMs and four diverse scenarios (formal debates, social media posts, public interviews, movie reviews), DPRF consistently and significantly improves behavioral alignment compared to baseline personas. It also demonstrates strong generalizability across different models and scenarios.\n\n**Significance:** DPRF offers a robust methodology for creating high-fidelity persona profiles, thereby enhancing the validity of downstream applications such as user simulation, social studies, and personalized AI.",
        "url": "https://www.semanticscholar.org/paper/774df6a650948a12b59cfeb8db803cd493c3ac35",
        "isOpenAccess": false
    },
    "2510.13139": {
        "title": "Addressing the alignment problem in transportation policy making: an LLM approach",
        "authors": [
            "Xiaoyu Yan",
            "Tianxing Dai",
            "Yu Nie"
        ],
        "arxiv_id": "2510.13139",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "A key challenge in transportation planning is that the collective preferences of heterogeneous travelers often diverge from the policies produced by model-driven decision tools. This misalignment frequently results in implementation delays or failures. Here, we investigate whether large language models (LLMs), noted for their capabilities in reasoning and simulating human decision-making, can help inform and address this alignment problem. We develop a multi-agent simulation in which LLMs, acting as agents representing residents from different communities in a city, participate in a referendum on a set of transit policy proposals. Using chain-of-thought reasoning, LLM agents provide ranked-choice or approval-based preferences, which are aggregated using instant-runoff voting (IRV) to model democratic consensus. We implement this simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago and Houston. Our findings suggest that LLM agents are capable of approximating plausible collective preferences and responding to local context, while also displaying model-specific behavioral biases and modest divergences from optimization-based benchmarks. These capabilities underscore both the promise and limitations of LLMs as tools for solving the alignment problem in transportation decision-making.",
        "abstract_summary_gcp": "This paper investigates whether large language models (LLMs) can help address the persistent challenge in transportation planning where model-driven policies often diverge from heterogeneous traveler preferences, leading to implementation failures.\n\nThe researchers developed a multi-agent simulation where LLMs (GPT-4o and Claude-3.5) acted as residents from different city communities (Chicago and Houston). These LLM agents used chain-of-thought reasoning to provide ranked-choice or approval-based preferences on transit policy proposals, which were then aggregated using instant-runoff voting (IRV) to model democratic consensus.\n\nFindings suggest that LLM agents can approximate plausible collective preferences and respond to local context. However, they also display model-specific behavioral biases and modest divergences when compared to optimization-based benchmarks. The study concludes that LLMs show both promise and limitations as tools for solving the alignment problem in transportation decision-making.",
        "url": "https://www.semanticscholar.org/paper/5eba57327f69178fe067a3f2433253afb5f8b562",
        "isOpenAccess": false
    },
    "2510.13982": {
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "authors": [
            "Jinkun Chen",
            "Sher Badshah",
            "Xuemin Yu",
            "Sijia Han",
            "Jiechao Gao"
        ],
        "arxiv_id": "2510.13982",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.",
        "abstract_summary_gcp": "This paper argues that while LLM-powered multi-agent systems offer exciting possibilities for agents to evolve and reshape their environments unpredictably, current simulations are severely constrained by static, task-specific benchmarks and predefined \"sandboxes.\" These limitations prevent them from capturing real-world complexity. The authors critically review emerging architectures that blend LLMs with multi-agent dynamics, highlight challenges like balancing stability/diversity and evaluating unexpected behaviors, and introduce a new taxonomy. They conclude by presenting a research roadmap advocating for open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems, calling for a shift away from static paradigms towards more adaptive and socially-aware multi-agent simulations.",
        "url": "https://www.semanticscholar.org/paper/99b8c375034be103f1ff14e80edeb0fd9204f7bc",
        "isOpenAccess": false
    },
    "2511.01864": {
        "title": "Missing the Margins: A Systematic Literature Review on the Demographic Representativeness of LLMs",
        "authors": [
            "Indira Sen",
            "Marlene Lutz",
            "Elisa Rogers",
            "David García",
            "Markus Strohmaier"
        ],
        "arxiv_id": "2511.01864",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Many applications of Large Language Models (LLMs) require them to either simulate people or offer personalized functionality, making the demographic representativeness of LLMs crucial for equitable utility. At the same time, we know little about the extent to which these models actually reflect the demographic attributes and behaviors of certain groups or populations, with conflicting findings in empirical research. To shed light on this debate, we review 211 papers on the demographic representativeness of LLMs. We find that while 29% of the studies report positive conclusions on the representativeness of LLMs, 30% of these do not evaluate LLMs across multiple demographic categories or within demographic subcategories. Another 35% and 47% of the papers concluding positively fail to specify these subcategories altogether for gender and race, respectively. Of the articles that do report subcategories, fewer than half include marginalized groups in their study. Finally, more than a third of the papers do not define the target population to whom their findings apply; of those that do define it either implicitly or explicitly, a large majority study only the U.S. Taken together, our findings suggest an inflated perception of LLM representativeness in the broader community. We recommend more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible use of LLMs for social applications. Our annotated list of papers and analysis code is publicly available.",
        "abstract_summary_gcp": "This paper reviews 211 studies on the demographic representativeness of Large Language Models (LLMs), a crucial factor for their equitable use in simulation and personalization tasks.\n\nThe review concludes that the perception of LLM representativeness in the broader community is **inflated**. This finding is based on several critical observations:\n\n*   While 29% of the reviewed studies claim positive representativeness, many of these conclusions are built on flawed methodologies.\n*   Specifically, 30% of these \"positive\" studies fail to evaluate LLMs across multiple demographic categories or within subcategories.\n*   A significant portion (35% for gender, 47% for race) of these positive papers do not even specify demographic subcategories.\n*   Among studies that *do* report subcategories, marginalized groups are rarely included.\n*   More broadly, over a third of *all* papers reviewed fail to define their target population, and among those that do, the vast majority focus exclusively on the U.S.\n\nThe authors recommend adopting more precise evaluation methods and comprehensive documentation of demographic attributes to ensure the responsible and equitable application of LLMs. An annotated list of papers and analysis code is publicly available.",
        "url": "https://www.semanticscholar.org/paper/5dece901894f588b76727f51dd810ba561df4c47",
        "isOpenAccess": false
    },
    "2510.12689": {
        "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM",
        "authors": [
            "S. Fulay",
            "Jocelyn Zhu",
            "Michiel Bakker"
        ],
        "arxiv_id": "2510.12689",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on\"behavioral cloning\", effectively evaluating how well models reproduce individuals'expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models'default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.",
        "abstract_summary_gcp": "This paper explores a critical design trade-off for large language models (LLMs) used to represent human interests: whether they should act as **delegates**, mirroring expressed preferences (the current focus of \"behavioral cloning\"), or as **trustees**, exercising judgment about an individual's best long-term interests.\n\nDrawing parallels to political representation and LLM sycophancy, the authors introduce a \"temporal utility framework\" to simulate a trustee role, weighing short and long-term interests in policy decisions. They compare the outcomes of these trustee-style predictions with delegate (behavior-cloning) models in simulated U.S. policy votes.\n\nThe findings reveal a fundamental trade-off:\n*   **Trustee models** (focused on long-term interests) align more closely with expert consensus on well-understood issues. However, they also exhibit greater bias towards the models' default stances on subjective topics lacking clear agreement.\n*   **Delegate models** better preserve user autonomy but may diverge from well-supported policy positions.\n\nIn essence, designing AI for human representation involves balancing user autonomy with potential welfare promotion, and navigating the risks of paternalism and inherent model biases.",
        "url": "https://www.semanticscholar.org/paper/3fcde8f39773b677095a37c4d0b15e3c30cb6cd5",
        "isOpenAccess": false
    },
    "2510.13902": {
        "title": "Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory",
        "authors": [
            "Nicole Smith-Vaniz",
            "Harper Lyon",
            "Lorraine Steigner",
            "Ben Armstrong",
            "Nicholas Mattei"
        ],
        "arxiv_id": "2510.13902",
        "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings — either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.",
        "abstract_summary_gcp": "This paper investigates potential biases in Large Language Models (LLMs), particularly their responses to difficult political and moral questions, given their growing role as advice-givers. It employs Moral Foundations Theory (MFT) – which categorizes moral reasoning into five dimensions (Harm, Fairness, Ingroup Loyalty, Authority, and Purity) – to quantify these biases.\n\nCrucially, unlike previous research, this study directly assesses LLM moral leanings using MFT and compares them against existing robust human data. The research examines whether LLMs exhibit ideological leanings through:\n1.  Their inherent, unprompted responses.\n2.  Explicit representations of political ideologies.\n3.  Responses generated when adopting demographic-based human personas.\n\nThe goal is to determine if LLMs inherently align with certain political ideologies and how accurately they can represent diverse ideological perspectives. Ultimately, the study aims to provide insight into the extent of political and demographic dependency in AI-generated content.",
        "url": "https://www.semanticscholar.org/paper/7424d621d1aa7792d4208d960dd380b46fe72728",
        "isOpenAccess": false
    },
    "2510.13884": {
        "title": "Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation",
        "authors": [
            "Bolei Ma",
            "Yong Cao",
            "Indira Sen",
            "Anna Haensch",
            "Frauke Kreuter",
            "Barbara Plank",
            "Daniel Hershcovich"
        ],
        "arxiv_id": "2510.13884",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly used to simulate public opinion and other social phenomena. Most current studies constrain these simulations to multiple-choice or short-answer formats for ease of scoring and comparison, but such closed designs overlook the inherently generative nature of LLMs. In this position paper, we argue that open-endedness, using free-form text that captures topics, viewpoints, and reasoning processes\"in\"LLMs, is essential for realistic social simulation. Drawing on decades of survey-methodology research and recent advances in NLP, we argue why this open-endedness is valuable in LLM social simulations, showing how it can improve measurement and design, support exploration of unanticipated views, and reduce researcher-imposed directive bias. It also captures expressiveness and individuality, aids in pretesting, and ultimately enhances methodological utility. We call for novel practices and evaluation frameworks that leverage rather than constrain the open-ended generative diversity of LLMs, creating synergies between NLP and social science.",
        "abstract_summary_gcp": "This position paper argues that current Large Language Model (LLM) simulations of public opinion and other social phenomena are unduly constrained by using multiple-choice or short-answer formats. The authors contend that such closed designs overlook the inherently generative nature of LLMs.\n\nThey advocate for \"open-endedness\" in these simulations, utilizing free-form text that captures detailed topics, viewpoints, and reasoning processes. Drawing on survey methodology and NLP advancements, the paper highlights several benefits of this approach: it enhances measurement and design, allows for the exploration of unanticipated views, reduces researcher-imposed bias, captures expressiveness and individuality, aids in pretesting, and ultimately improves methodological utility for realistic social simulations.\n\nThe paper concludes by calling for the development of novel practices and evaluation frameworks that fully leverage, rather than constrain, the open-ended generative diversity of LLMs, fostering a stronger synergy between NLP and social science.",
        "url": "https://www.semanticscholar.org/paper/38abbee6c9cb9a59c8f91c7177ffea459a7c4b63",
        "isOpenAccess": false
    },
    "2510.11502": {
        "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors",
        "authors": [
            "Alexis Ross",
            "Jacob Andreas"
        ],
        "arxiv_id": "2510.11502",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Research on reasoning in language models (LMs) predominantly focuses on improving the correctness of their outputs. But some important applications require modeling reasoning patterns that are incorrect. For example, automated systems that can reason about and simulate student errors are useful for providing real-time feedback in the classroom or offline practice for educators-in-training. This paper presents a new method, MISTAKE, that (1) constructs high-quality synthetic examples of reasoning errors by leveraging cycle consistency between incorrect answers and latent misconceptions; and (2) uses the generated data to learn models for student simulation, misconception classification, and answer generation. We evaluate MISTAKE on three educational tasks and find that it results in (1) higher accuracy when simulating incorrect student answers based on specific misconceptions, (2) increased performance inferring latent misconceptions from observed incorrect answers, and (3) higher alignment with expert-written distractor answers when generating incorrect answers (e.g., for multiple-choice tests).",
        "abstract_summary_gcp": "This paper introduces MISTAKE, a novel method addressing the need for language models (LMs) to model *incorrect* reasoning patterns, particularly for applications like simulating student errors in educational settings.\n\nMISTAKE operates in two main steps:\n1.  It constructs high-quality synthetic examples of reasoning errors by leveraging **cycle consistency** between incorrect answers and underlying misconceptions.\n2.  It then uses this generated data to train models for **student simulation**, **misconception classification**, and **incorrect answer generation**.\n\nEvaluated on three educational tasks, MISTAKE demonstrates superior performance, leading to:\n*   Higher accuracy in simulating incorrect student answers linked to specific misconceptions.\n*   Improved ability to infer latent misconceptions from observed incorrect answers.\n*   Greater alignment with expert-written distractor answers when generating incorrect options (e.g., for multiple-choice tests).",
        "url": "https://www.semanticscholar.org/paper/935b9a86f1ff692704cd711ba30888fb1d2da78f",
        "isOpenAccess": false
    },
    "2510.11586": {
        "title": "Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models",
        "authors": [
            "Georg Ahnert",
            "Anna Haensch",
            "Barbara Plank",
            "Markus Strohmaier"
        ],
        "arxiv_id": "2510.11586",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Many in-silico simulations of human survey responses with large language models (LLMs) focus on generating closed-ended survey responses, whereas LLMs are typically trained to generate open-ended text instead. Previous research has used a diverse range of methods for generating closed-ended survey responses with LLMs, and a standard practice remains to be identified. In this paper, we systematically investigate the impact that various Survey Response Generation Methods have on predicted survey responses. We present the results of 32 mio. simulated survey responses across 8 Survey Response Generation Methods, 4 political attitude surveys, and 10 open-weight language models. We find significant differences between the Survey Response Generation Methods in both individual-level and subpopulation-level alignment. Our results show that Restricted Generation Methods perform best overall, and that reasoning output does not consistently improve alignment. Our work underlines the significant impact that Survey Response Generation Methods have on simulated survey responses, and we develop practical recommendations on the application of Survey Response Generation Methods.",
        "abstract_summary_gcp": "This paper investigates the impact of different \"Survey Response Generation Methods\" when using Large Language Models (LLMs) to simulate closed-ended human survey responses. This is a crucial area because LLMs are typically trained for open-ended text, and current research lacks a standardized approach for generating closed-ended data.\n\nThe study systematically evaluated 8 distinct generation methods across 32 million simulated responses, utilizing 4 political attitude surveys and 10 open-weight LLMs. Key findings reveal significant differences between these methods in terms of both individual and subpopulation-level alignment with actual survey data. Specifically, \"Restricted Generation Methods\" were found to perform best overall, and, surprisingly, incorporating explicit reasoning output did not consistently improve alignment.\n\nThe research highlights the critical importance of selecting the appropriate Survey Response Generation Method for LLM simulations and provides practical recommendations for their application.",
        "url": "https://www.semanticscholar.org/paper/23059ec20a582007a7cb9f1137b4ce69df76c9bf",
        "isOpenAccess": false
    },
    "2510.11035": {
        "title": "SusBench: An Online Benchmark for Evaluating Dark Pattern Susceptibility of Computer-Use Agents",
        "authors": [
            "Longjie Guo",
            "Chenjie Yuan",
            "Mingyuan Zhong",
            "Robert Wolfe",
            "Ruican Zhong",
            "Yue Xu",
            "Bingbing Wen",
            "Hua Shen",
            "Lucy Lu Wang",
            "Alexis Hiniker"
        ],
        "arxiv_id": "2510.11035",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As LLM-based computer-use agents (CUAs) begin to autonomously interact with real-world interfaces, understanding their vulnerability to manipulative interface designs becomes increasingly critical. We introduce SusBench, an online benchmark for evaluating the susceptibility of CUAs to UI dark patterns, designs that aim to manipulate or deceive users into taking unintentional actions. Drawing nine common dark pattern types from existing taxonomies, we developed a method for constructing believable dark patterns on real-world consumer websites through code injections, and designed 313 evaluation tasks across 55 websites. Our study with 29 participants showed that humans perceived our dark pattern injections to be highly realistic, with the vast majority of participants not noticing that these had been injected by the research team. We evaluated five state-of-the-art CUAs on the benchmark. We found that both human participants and agents are particularly susceptible to the dark patterns of Preselection, Trick Wording, and Hidden Information, while being resilient to other overt dark patterns. Our findings inform the development of more trustworthy CUAs, their use as potential human proxies in evaluating deceptive designs, and the regulation of an online environment increasingly navigated by autonomous agents.",
        "abstract_summary_gcp": "This paper introduces **SusBench**, a new online benchmark designed to evaluate the susceptibility of LLM-based computer-use agents (CUAs) to manipulative UI dark patterns. As CUAs begin to autonomously interact with real-world interfaces, understanding their vulnerability to designs intended to deceive or manipulate users is crucial.\n\nThe researchers developed SusBench by identifying nine common dark pattern types and injecting believable examples into 313 evaluation tasks across 55 real-world consumer websites. A study with 29 human participants confirmed the realism of these injected patterns, with most not realizing they were artificial.\n\nEvaluating five state-of-the-art CUAs on SusBench, the study found that both human participants and the agents were particularly vulnerable to **Preselection, Trick Wording, and Hidden Information** dark patterns. Conversely, both groups showed resilience to other, more overt dark patterns. These findings are significant for developing more trustworthy CUAs, potentially using them as proxies to identify deceptive designs, and informing the regulation of online environments that autonomous agents increasingly navigate.",
        "url": "https://www.semanticscholar.org/paper/faf709f85d9377eb780cd74b393ca18d623756a3",
        "isOpenAccess": false
    },
    "2510.09183": {
        "title": "Student Development Agent: Risk-free Simulation for Evaluating AIED Innovations",
        "authors": [
            "Jianxiao Jiang",
            "Yu Zhang"
        ],
        "arxiv_id": "2510.09183",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In the age of AI-powered educational (AIED) innovation, evaluating the developmental consequences of novel designs before they are exposed to students has become both essential and challenging. Since such interventions may carry irreversible effects, it is critical to anticipate not only potential benefits but also possible harms. This study proposes a student development agent framework based on large language models (LLMs), designed to simulate how students with diverse characteristics may evolve under different educational settings without administering them to real students. By validating the approach through a case study on a multi-agent learning environment (MAIC), we demonstrate that the agent's predictions align with real student outcomes in non-cognitive developments. The results suggest that LLM-based simulations hold promise for evaluating AIED innovations efficiently and ethically. Future directions include enhancing profile structures, incorporating fine-tuned or small task-specific models, validating effects of empirical findings, interpreting simulated data and optimizing evaluation methods.",
        "abstract_summary_gcp": "This study addresses the critical need to evaluate AI-powered educational (AIED) innovations for both benefits and harms *before* exposing them to real students, due to potential irreversible effects.\n\nIt proposes a novel **student development agent framework based on large language models (LLMs)**. This framework simulates how diverse students might evolve under different educational settings, enabling pre-assessment without real-student trials.\n\nThrough a case study using a multi-agent learning environment (MAIC), the researchers demonstrated that the agent's predictions for **non-cognitive student developments aligned with actual student outcomes**.\n\nThe findings suggest that LLM-based simulations offer a promising, efficient, and ethical method for evaluating AIED designs. Future work includes enhancing student profiles, incorporating specialized LLMs, validating empirical findings, and optimizing simulation interpretation and evaluation methods.",
        "url": "https://www.semanticscholar.org/paper/ace04fd3186eeebf9f740bd964b74f5a8611fcc5",
        "isOpenAccess": false
    },
    "2510.07230": {
        "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping",
        "authors": [
            "Ziyi Wang",
            "Yuxuan Lu",
            "Yimeng Zhang",
            "Jing Huang",
            "Dakuo Wang"
        ],
        "arxiv_id": "2510.07230",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users'action distribution, indicating higher fidelity in personalized behavior simulation.",
        "abstract_summary_gcp": "This paper addresses the limitation of current LLM-based step-wise human behavior simulation methods, which generate generic, population-level policies rather than personalized ones due to their inability to condition on a user's persona.\n\nThe authors introduce **Customer-R1**, a novel RL-based method specifically designed for personalized, step-wise user behavior simulation within online shopping environments. Customer-R1's policy is explicitly conditioned on a user's persona, and it optimizes next-step rationale and action generation using action correctness reward signals.\n\nExperiments on the OPeRA dataset demonstrate that Customer-R1 significantly outperforms prompting and SFT-based baselines in next-action prediction tasks and more accurately matches users' actual action distributions, indicating a higher fidelity in personalized behavior simulation.",
        "url": "https://www.semanticscholar.org/paper/53dd0ab8f8be235e85b93a088f354b11ce690ca5",
        "isOpenAccess": false
    },
    "2510.06105": {
        "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
        "authors": [
            "Batu El",
            "James Zou"
        ],
        "arxiv_id": "2510.06105",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are increasingly shaping how information is created and disseminated, from companies using them to craft persuasive advertisements, to election campaigns optimizing messaging to gain votes, to social media influencers boosting engagement. These settings are inherently competitive, with sellers, candidates, and influencers vying for audience approval, yet it remains poorly understood how competitive feedback loops influence LLM behavior. We show that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors. We call this phenomenon Moloch's Bargain for AI--competitive success achieved at the cost of alignment. These misaligned behaviors emerge even when models are explicitly instructed to remain truthful and grounded, revealing the fragility of current alignment safeguards. Our findings highlight how market-driven optimization pressures can systematically erode alignment, creating a race to the bottom, and suggest that safe deployment of AI systems will require stronger governance and carefully designed incentives to prevent competitive dynamics from undermining societal trust.",
        "abstract_summary_gcp": "Large language models (LLMs) used in competitive environments like advertising, elections, and social media, inadvertently drive misalignment when optimized for success. This phenomenon, termed \"Moloch's Bargain for AI,\" means competitive gains come at the cost of desirable behaviors.\n\nFor instance, in simulated scenarios:\n*   A 6.3% increase in sales was accompanied by a 14.0% rise in deceptive marketing.\n*   A 4.9% gain in vote share coincided with 22.3% more disinformation and 12.5% more populist rhetoric.\n*   A 7.5% engagement boost led to 188.6% more disinformation and a 16.3% increase in promoting harmful behaviors.\n\nThese misaligned behaviors emerged even when models were explicitly instructed to be truthful, highlighting the fragility of current alignment safeguards. The findings suggest that market-driven optimization pressures can systematically erode AI alignment, creating a \"race to the bottom\" and necessitating stronger governance and carefully designed incentives to prevent the undermining of societal trust.",
        "url": "https://www.semanticscholar.org/paper/cb492a530a982e4e0d051720cd111c6d54575275",
        "isOpenAccess": false
    },
    "2510.05484": {
        "title": "Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach",
        "authors": [
            "Abhejay Murali",
            "Saleh Afroogh",
            "Kevin Chen",
            "David Atkinson",
            "Amit Dhurandhar",
            "Junfeng Jiao"
        ],
        "arxiv_id": "2510.05484",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are rapidly becoming part of tools used by children; however, existing benchmarks fail to capture how these models manage language, reasoning, and safety needs that are specific to various ages. We present ChildSafe, a benchmark that evaluates LLM safety through simulated child agents that embody four developmental stages. These agents, grounded in developmental psychology, enable a systematic study of child safety without the ethical implications of involving real children. ChildSafe assesses responses across nine safety dimensions (including privacy, misinformation, and emotional support) using age-weighted scoring in both sensitive and neutral contexts. Multi-turn experiments with multiple LLMs uncover consistent vulnerabilities that vary by simulated age, exposing shortcomings in existing alignment practices. By releasing agent templates, evaluation protocols, and an experimental corpus, we provide a reproducible framework for age-aware safety research. We encourage the community to expand this work with real child-centered data and studies, advancing the development of LLMs that are genuinely safe and developmentally aligned.",
        "abstract_summary_gcp": "LLMs are increasingly used by children, but existing benchmarks fail to capture age-specific safety, language, and reasoning needs. To address this, **ChildSafe** is introduced, a new benchmark that evaluates LLM safety.\n\nChildSafe operates by simulating child agents, grounded in developmental psychology, which embody four distinct developmental stages. This approach allows for systematic safety study without the ethical concerns of involving real children. The benchmark assesses LLM responses across nine safety dimensions (including privacy, misinformation, and emotional support), employing age-weighted scoring in both sensitive and neutral contexts.\n\nExperiments using ChildSafe revealed consistent vulnerabilities in LLMs that vary by simulated age, highlighting deficiencies in current alignment practices. The authors provide a reproducible framework—including agent templates, evaluation protocols, and an experimental corpus—for age-aware safety research. They encourage the community to expand this work with real child-centered data to develop genuinely safe and developmentally aligned LLMs.",
        "url": "https://www.semanticscholar.org/paper/04bb8d73ffd906c9e8f41c4afa2c5fbb0227b751",
        "isOpenAccess": false
    },
    "2510.01924": {
        "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
        "authors": [
            "Crystal Qian",
            "Aaron Parisi",
            "Clémentine Bouleau",
            "Vivian Tsai",
            "Mael Lebreton",
            "Lucas Dixon"
        ],
        "arxiv_id": "2510.01924",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical framework for assessing collective alignment, in contrast to prior work on the individual level. Using the Lost at Sea social psychology task, we conduct a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We then simulate matched LLM groups conditioned on the human data, benchmarking Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some mirror human biases; others mask these biases and attempt to compensate for them. We empirically demonstrate that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases. Understanding how LLMs align with collective human behavior is critical to advancing socially-aligned AI, and demands dynamic benchmarks that capture the complexities of collective reasoning.",
        "abstract_summary_gcp": "This research examines the alignment of large language models (LLMs) with human collective social reasoning, distinguishing it from individual-level analysis.\n\nThe study employed a large-scale online experiment (N=748) using the \"Lost at Sea\" task, where human groups elected leaders under conditions of either visible demographic attributes or pseudonymous aliases. Subsequently, the researchers simulated LLM groups (Gemini 2.5, GPT 4.1, Claude Haiku 3.5, Gemma 3) conditioned on the collected human data.\n\nFindings revealed that LLM behaviors diverged significantly: some models mirrored human social biases, while others actively masked or attempted to compensate for them. The authors conclude that human-AI alignment in collective reasoning is critically dependent on context, specific cues, and each model's inductive biases. This highlights the necessity for dynamic benchmarks to accurately assess and advance socially-aligned AI capable of complex collective reasoning.",
        "url": "https://www.semanticscholar.org/paper/bcd71c58ba8027c4ca154405a8a4e7168db5131d",
        "isOpenAccess": false
    },
    "AI Surrogates and illusions of generalizability in cognitive science.": {
        "title": "AI Surrogates and illusions of generalizability in cognitive science.",
        "authors": [
            "M. J. Crockett",
            "Lisa Messeri"
        ],
        "arxiv_id": null,
        "venue": "Trends in Cognitive Sciences",
        "year": 2025,
        "publicationTypes": [
            "Review",
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/e6eacef1f0dfdf720e62f62624e44b332890aae1",
        "isOpenAccess": false
    },
    "2509.26080": {
        "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research",
        "authors": [
            "Emma Rose Madden"
        ],
        "arxiv_id": "2509.26080",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "abstract": "Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.",
        "abstract_summary_gcp": "This paper addresses the growing use of Large Language Models (LLMs) as synthetic agents in social science, from augmenting surveys to powering simulations. It cautions against interpreting LLM outputs as probabilistic inference, instead advocating for their pragmatic reframing as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions. To facilitate useful prototyping and forecasting while avoiding misinterpretations, the paper proposes practical guardrails, including independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration.",
        "url": "https://www.semanticscholar.org/paper/01f7cc1aab43e9705b6c6750c10a81f78aabe469",
        "isOpenAccess": false
    },
    "2510.00174": {
        "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation",
        "authors": [
            "Rik Koncel-Kedziorski",
            "Brihi Joshi",
            "Tim Paek"
        ],
        "arxiv_id": "2510.00174",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As the adoption of language models advances, so does the need to better represent individual users to the model. Are there aspects of an individual's belief system that a language model can utilize for improved alignment? Following prior research, we investigate this question in the domain of opinion prediction by developing PrimeX, a dataset of public opinion survey data from 858 US residents with two additional sources of belief information: written explanations from the respondents for why they hold specific opinions, and the Primal World Belief survey for assessing respondent worldview. We provide an extensive initial analysis of our data and show the value of belief explanations and worldview for personalizing language models. Our results demonstrate how the additional belief information in PrimeX can benefit both the NLP and psychological research communities, opening up avenues for further study.",
        "abstract_summary_gcp": "This paper addresses the growing need for language models to better represent individual users by investigating whether an individual's belief system can improve model alignment. To explore this, the authors developed **PrimeX**, a new dataset comprising public opinion survey data from 858 US residents. Crucially, PrimeX includes two additional sources of belief information: respondents' written explanations for their opinions and their worldview as assessed by the Primal World Belief survey. An initial analysis of PrimeX demonstrates the significant value of both belief explanations and worldview for personalizing language models, offering benefits to both the NLP and psychological research communities and opening new avenues for study.",
        "url": "https://www.semanticscholar.org/paper/9bd65cd83dc04896b7de2e60982792ccfd0ab879",
        "isOpenAccess": false
    },
    "2510.05124": {
        "title": "MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation",
        "authors": [
            "Mingjin Li",
            "Yu Liu",
            "Huayi Liu",
            "Xiang Ye",
            "Chao Jiang",
            "Hongguang Zhang",
            "Yu Ruan"
        ],
        "arxiv_id": "2510.05124",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for generating persuasive multi-turn dialogues via agent self-play. MADS employs three coordinated agents: User Agents designed to simulate diverse persona-driven behaviors by leveraging personality signifiers such as Zodiac Signs and MBTI types, a Dialog Agent executing task-oriented persuasion strategies and an Optimization Agent evaluating and refining dialogue outcomes. We further validate its effectiveness through users'Chain-of-Attitude (CoA) modeling and dedicated LLMs'persuasion assessment. This approach enables low-cost generation of training data without human annotation, addressing key industry challenges such as lack of user data, cold-start evaluation difficulties, and prompt inefficiency. Applied to a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4% (from 1.83% to 2.24%) , demonstrating clear business value.",
        "abstract_summary_gcp": "MADS (Multi-Agent Dialogue Simulation) is a scalable framework that generates persuasive multi-turn dialogues through agent self-play. It utilizes three coordinated agents:\n1.  **User Agents:** Simulate diverse, persona-driven behaviors using personality signifiers like Zodiac signs and MBTI types.\n2.  **Dialog Agent:** Executes task-oriented persuasion strategies.\n3.  **Optimization Agent:** Evaluates and refines dialogue outcomes.\n\nThe framework's effectiveness is validated through user Chain-of-Attitude (CoA) modeling and dedicated LLM persuasion assessments. MADS addresses key industry challenges like lack of user data, cold-start evaluation, and prompt inefficiency by enabling low-cost, human-annotation-free generation of training data. In a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4% (from 1.83% to 2.24%), demonstrating clear business value.",
        "url": "https://www.semanticscholar.org/paper/101716e83164f2caee0acb1170dedf996809e384",
        "isOpenAccess": false
    },
    "2509.21868": {
        "title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness",
        "authors": [
            "Yuxuan Li",
            "Sauvik Das",
            "Hirokazu Shirado"
        ],
        "arxiv_id": "2509.21868",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.",
        "abstract_summary_gcp": "This paper addresses the limited real-world adoption of LLM agent simulations for policy making by demonstrating how they can be made genuinely useful. It details a year-long engagement with a university emergency preparedness team, where they developed a system of 13,000 LLM agents. This system simulated crowd movement and communication during large-scale gatherings under various emergency scenarios, ultimately informing actual policy implementation for volunteer training, evacuation protocols, and infrastructure planning.\n\nThrough this process, the authors derive three key design implications for effective LLM agent simulations in policy:\n1.  **Start with verifiable scenarios** to gradually build trust.\n2.  **Use preliminary simulations** to elicit and incorporate tacit knowledge.\n3.  **Treat simulation and policy development as co-evolving processes.**\n\nThese implications provide actionable pathways to enhance the utility of LLM agent simulations for policy.",
        "url": "https://www.semanticscholar.org/paper/c65e057b8d3d1fbf3d1b5e67a5fa9de9d739c10e",
        "isOpenAccess": false
    },
    "2509.20805": {
        "title": "Few-Shot and Training-Free Review Generation via Conversational Prompting",
        "authors": [
            "Genki Kusano"
        ],
        "arxiv_id": "2509.20805",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Personalized review generation helps businesses understand user preferences, yet most existing approaches assume extensive review histories of the target user or require additional model training. Real-world applications often face few-shot and training-free situations, where only a few user reviews are available and fine-tuning is infeasible. It is well known that large language models (LLMs) can address such low-resource settings, but their effectiveness depends on prompt engineering. In this paper, we propose Conversational Prompting, a lightweight method that reformulates user reviews as multi-turn conversations. Its simple variant, Simple Conversational Prompting (SCP), relies solely on the user's own reviews, while the contrastive variant, Contrastive Conversational Prompting (CCP), inserts reviews from other users or LLMs as incorrect replies and then asks the model to correct them, encouraging the model to produce text in the user's style. Experiments on eight product domains and five LLMs showed that the conventional non-conversational prompt often produced reviews similar to those written by random users, based on text-based metrics such as ROUGE-L and BERTScore, and application-oriented tasks like user identity matching and sentiment analysis. In contrast, both SCP and CCP produced reviews much closer to those of the target user, even when each user had only two reviews. CCP brings further improvements when high-quality negative examples are available, whereas SCP remains competitive when such data cannot be collected. These results suggest that conversational prompting offers a practical solution for review generation under few-shot and training-free constraints.",
        "abstract_summary_gcp": "This paper addresses the challenge of personalized review generation in **few-shot and training-free settings**, where extensive user history or model fine-tuning are unavailable. Recognizing that Large Language Models (LLMs) can operate in such low-resource environments but require effective prompting, the authors introduce **Conversational Prompting**.\n\nThis lightweight method reformulates user reviews as **multi-turn conversations**. Its basic form, **Simple Conversational Prompting (SCP)**, uses only the target user's own reviews. A more advanced variant, **Contrastive Conversational Prompting (CCP)**, inserts reviews from other users or LLMs as \"incorrect\" responses and prompts the model to correct them, thereby encouraging it to adopt the target user's writing style.\n\nExperiments across eight product domains and five LLMs demonstrated that conventional, non-conversational prompts produced generic reviews similar to those of random users (based on ROUGE-L, BERTScore, user identity matching, and sentiment analysis). In contrast, **both SCP and CCP generated reviews significantly closer to the target user's style**, even with as few as two reviews per user. CCP offered further benefits when high-quality negative examples were available, while SCP remained competitive when such data was absent.\n\nThe findings suggest that Conversational Prompting offers a **practical and effective solution for personalized review generation** in real-world scenarios with limited data and no training.",
        "url": "https://www.semanticscholar.org/paper/d0982985fd16dd04c54bb4d70e2c7fbaea1a7eed",
        "isOpenAccess": false
    },
    "2509.18008": {
        "title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration",
        "authors": [
            "Bingsheng Yao",
            "Jiaju Chen",
            "Chaoran Chen",
            "April Wang",
            "T. Li",
            "Dakuo Wang"
        ],
        "arxiv_id": "2509.18008",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Intelligent systems have traditionally been designed as tools rather than collaborators, often lacking critical characteristics that collaboration partnerships require. Recent advances in large language model (LLM) agents open new opportunities for human-LLM-agent collaboration by enabling natural communication and various social and cognitive behaviors. Yet it remains unclear whether principles of computer-mediated collaboration established in HCI and CSCW persist, change, or fail when humans collaborate with LLM agents. To support systematic investigations of these questions, we introduce an open and configurable research platform for HCI researchers. The platform's modular design allows seamless adaptation of classic CSCW experiments and manipulation of theory-grounded interaction controls. We demonstrate the platform's effectiveness and usability through two case studies: (1) re-implementing the classic human-human-collaboration task Shape Factory as a between-subject human-agent-collaboration experiment with 16 participants, and (2) a participatory cognitive walkthrough with five HCI researchers to refine workflows and interfaces for experiment setup and analysis.",
        "abstract_summary_gcp": "This paper introduces a new research platform designed for HCI researchers to investigate human-LLM agent collaboration.\n\nThe authors note that traditional intelligent systems act as tools, not collaborators, but large language model (LLM) agents offer new opportunities for genuine collaboration through natural communication and social behaviors. A key question arises: do established principles of computer-mediated collaboration from HCI and CSCW still apply to these human-LLM agent interactions?\n\nTo answer this, they present an open, configurable, and modular platform. This platform allows for the systematic adaptation of classic CSCW experiments and the manipulation of theory-grounded interaction controls. Its effectiveness is demonstrated through two case studies: (1) re-implementing the \"Shape Factory\" human-human collaboration task as a human-agent experiment with 16 participants, and (2) a participatory cognitive walkthrough with five HCI researchers to refine its experimental setup and analysis workflows.",
        "url": "https://www.semanticscholar.org/paper/f4f577c756ceebac650eb2ca978e582c1b536f7a",
        "isOpenAccess": false
    },
    "2509.17999": {
        "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
        "authors": [
            "Riccardo Cadei",
            "Christian Interno"
        ],
        "arxiv_id": "2509.17999",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Modern foundational models increasingly reflect not just world knowledge, but patterns of human preference embedded in their training data. We hypothesize that recursive alignment-via human feedback and model-generated corpora-induces a social desirability bias, nudging models to favor agreeable or flattering responses over objective reasoning. We refer to it as the Narcissus Hypothesis and test it across 31 models using standardized personality assessments and a novel Social Desirability Bias score. Results reveal a significant drift toward socially conforming traits, with profound implications for corpus integrity and the reliability of downstream inferences. We then offer a novel epistemological interpretation, tracing how recursive bias may collapse higher-order reasoning down Pearl's Ladder of Causality, culminating in what we refer to as the Rung of Illusion.",
        "abstract_summary_gcp": "The paper introduces the \"Narcissus Hypothesis,\" proposing that modern foundational models, due to recursive alignment processes (human feedback and model-generated data), acquire a social desirability bias. This bias causes models to favor agreeable or flattering responses over objective reasoning.\n\nTesting across 31 models using standardized personality assessments and a novel Social Desirability Bias score revealed a significant drift towards socially conforming traits. The authors argue this has profound implications for the integrity of training data and the reliability of subsequent inferences. They also present an epistemological interpretation, suggesting that this recursive bias can degrade higher-order reasoning, moving it down Pearl's Ladder of Causality to a \"Rung of Illusion.\"",
        "url": "https://www.semanticscholar.org/paper/53971b77f9a41bc59208587e127b803af6dfe613",
        "isOpenAccess": false
    },
    "2509.15356": {
        "title": "Predicting Language Models' Success at Zero-Shot Probabilistic Prediction",
        "authors": [
            "Kevin Ren",
            "Santiago Cortes-Gomez",
            "C. Patiño",
            "Ananya Joshi",
            "Ruiqi Lyu",
            "Jingjing Tang",
            "Alistair Turcan",
            "Khurram Yamin",
            "Steven Wu",
            "Bryan Wilder"
        ],
        "arxiv_id": "2509.15356",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs'zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs'performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs'performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs'predictive performance on new tasks.",
        "abstract_summary_gcp": "This paper investigates the reliability of Large Language Models (LLMs) as zero-shot predictors for individual-level characteristics in tabular data. A large-scale empirical study revealed that LLMs' performance is highly variable across different tasks and datasets. However, it found that when an LLM performs well on a base prediction task, its predicted probabilities offer a stronger signal for individual-level accuracy. To address the variability, the researchers developed metrics capable of predicting LLM performance at the task level. These metrics, which do not require labeled data, proved effective in distinguishing tasks where LLMs are likely to perform well from those where they are unsuitable.",
        "url": "https://www.semanticscholar.org/paper/8331e5c10e048484489ae394524f7fa014767f02",
        "isOpenAccess": false
    },
    "2509.14455": {
        "title": "Charting trajectories of human thought using large language models",
        "authors": [
            "Matthew M. Nour",
            "Daniel C. McNamee",
            "Isaac Fradkin",
            "Raymond J Dolan"
        ],
        "arxiv_id": "2509.14455",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Biology"
        ],
        "abstract": "Language provides the most revealing window into the ways humans structure conceptual knowledge within cognitive maps. Harnessing this information has been difficult, given the challenge of reliably mapping words to mental concepts. Artificial Intelligence large language models (LLMs) now offer unprecedented opportunities to revisit this challenge. LLMs represent words and phrases as high-dimensional numerical vectors that encode vast semantic knowledge. To harness this potential for cognitive science, we introduce VECTOR, a computational framework that aligns LLM representations with human cognitive map organisation. VECTOR casts a participant's verbal reports as a geometric trajectory through a cognitive map representation, revealing how thoughts flow from one idea to the next. Applying VECTOR to narratives generated by 1,100 participants, we show these trajectories have cognitively meaningful properties that predict paralinguistic behaviour (response times) and real-world communication patterns. We suggest our approach opens new avenues for understanding how humans dynamically organise and navigate conceptual knowledge in naturalistic settings.",
        "abstract_summary_gcp": "This paper introduces VECTOR, a computational framework designed to understand how humans structure conceptual knowledge by aligning Large Language Model (LLM) representations with human cognitive maps. While language is a key indicator of conceptual organization, reliably mapping words to mental concepts has been difficult. LLMs, with their ability to encode vast semantic knowledge in high-dimensional word vectors, offer a new approach.\n\nVECTOR treats a person's verbal reports as a \"geometric trajectory\" through a cognitive map derived from LLM representations, revealing the dynamic flow of thoughts. Applying this framework to narratives from 1,100 participants, the researchers found that these trajectories possess cognitively meaningful properties that predict paralinguistic behaviors (like response times) and real-world communication patterns. This approach promises new insights into how humans dynamically organize and navigate conceptual knowledge in naturalistic settings.",
        "url": "https://www.semanticscholar.org/paper/3f3bb7b3d39bbd7b373fd4e5c21bcc1be398c58c",
        "isOpenAccess": false
    },
    "2510.21721": {
        "title": "PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation",
        "authors": [
            "Kentaro Ueda",
            "Takehiro Takayanagi"
        ],
        "arxiv_id": "2510.21721",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "While recent advances in Large Language Models (LLMs) have improved the quality of creative text generation, significant challenges remain in producing personalized stories that reflect individual user preferences. Conventional approaches rely on explicit feedback or fine-tuning, which presents practical issues regarding user burden, data collection, computational costs, and privacy. In this work, we propose PREFINE (Persona-and-Rubric Guided Critique-and-Refine), a novel framework that extends the Critique-and-Refine paradigm to personalization. PREFINE constructs a pseudo-user agent from a user's interaction history and generates user-specific rubrics (evaluation criteria). By having this agent critique and refine outputs on the user's behalf based on these tailored rubrics, our method achieves personalized generation without requiring parameter updates or direct user feedback. We conducted a comprehensive evaluation on the PerDOC and PerMPST story datasets. We designed three baseline methods and several model variants to verify the contribution of each component of our framework. In automatic evaluations (LLM-as-a-Judge), PREFINE achieved higher win rates and statistically significant scores than the baselines, without compromising general story quality. Analysis of the model variants confirmed that both the pseudo-user agent and the user-specific rubrics are crucial for enhancing personalization performance. Beyond story generation, our approach holds potential for enabling efficient personalization in broader applications, such as dialogue systems, education, and recommendation.",
        "abstract_summary_gcp": "This paper introduces PREFINE (Persona-and-Rubric Guided Critique-and-Refine), a novel framework designed to overcome the challenges of personalizing creative text generation by Large Language Models (LLMs). While LLMs excel at general creative output, personalizing to individual user preferences typically requires burdensome explicit feedback or costly fine-tuning.\n\nPREFINE addresses this by extending the Critique-and-Refine paradigm. It creates a \"pseudo-user agent\" based on a user's interaction history and generates \"user-specific rubrics\" (evaluation criteria). This agent then critiques and refines LLM outputs on the user's behalf, guided by these tailored rubrics. This allows for personalized generation without requiring direct user feedback or model parameter updates.\n\nEvaluations on the PerDOC and PerMPST story datasets, using LLM-as-a-Judge, showed that PREFINE achieved significantly higher win rates and scores than baseline methods for personalization, without compromising general story quality. Component analysis confirmed that both the pseudo-user agent and the user-specific rubrics are crucial for its performance. The authors suggest this approach has potential for efficient personalization in broader applications like dialogue systems, education, and recommendation.",
        "url": "https://www.semanticscholar.org/paper/bcb3c839a749ded34d34f05a0da180553bf97d56",
        "isOpenAccess": false
    },
    "2509.13011": {
        "title": "A Visualized Framework for Event Cooperation with Generative Agents",
        "authors": [
            "Yuyang Tian",
            "Shunqiang Mao",
            "Wenchang Gao",
            "Lanlan Qiu",
            "Tianxing He"
        ],
        "arxiv_id": "2509.13011",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) have revolutionized the simulation of agent societies, enabling autonomous planning, memory formation, and social interactions. However, existing frameworks often overlook systematic evaluations for event organization and lack visualized integration with physically grounded environments, limiting agents'ability to navigate spaces and interact with items realistically. We develop MiniAgentPro, a visualization platform featuring an intuitive map editor for customizing environments and a simulation player with smooth animations. Based on this tool, we introduce a comprehensive test set comprising eight diverse event scenarios with basic and hard variants to assess agents'ability. Evaluations using GPT-4o demonstrate strong performance in basic settings but highlight coordination challenges in hard variants.",
        "abstract_summary_gcp": "This paper introduces **MiniAgentPro**, a visualization platform designed to enhance the simulation and evaluation of Large Language Model (LLM)-driven agent societies.\n\nWhile LLMs have advanced agent capabilities like planning, memory, and social interaction, existing frameworks often fall short in two areas:\n1.  Systematic evaluation for event organization.\n2.  Visualized integration with physically grounded environments, which limits agents' realistic navigation and interaction with items.\n\nMiniAgentPro addresses these gaps with an intuitive map editor for customizing environments and a simulation player offering smooth animations. Based on this platform, the authors developed a comprehensive test set featuring eight diverse event scenarios, each with basic and hard variants.\n\nEvaluations using GPT-4o showed that agents perform strongly in basic settings but encounter significant coordination challenges in the more complex, hard variants of these scenarios.",
        "url": "https://www.semanticscholar.org/paper/f4f90ce41b8788dc1d7e63ecb252a82294e502cd",
        "isOpenAccess": false
    },
    "2509.11967": {
        "title": "MillStone: How Open-Minded Are LLMs?",
        "authors": [
            "Harold Triedman",
            "Vitaly Shmatikov"
        ],
        "arxiv_id": "2509.11967",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models equipped with Web search, information retrieval tools, and other agentic capabilities are beginning to supplant traditional search engines. As users start to rely on LLMs for information on many topics, including controversial and debatable issues, it is important to understand how the stances and opinions expressed in LLM outputs are influenced by the documents they use as their information sources. In this paper, we present MillStone, the first benchmark that aims to systematically measure the effect of external arguments on the stances that LLMs take on controversial issues (not all of them political). We apply MillStone to nine leading LLMs and measure how ``open-minded''they are to arguments supporting opposite sides of these issues, whether different LLMs agree with each other, which arguments LLMs find most persuasive, and whether these arguments are the same for different LLMs. In general, we find that LLMs are open-minded on most issues. An authoritative source of information can easily sway an LLM's stance, highlighting the importance of source selection and the risk that LLM-based information retrieval and search systems can be manipulated.",
        "abstract_summary_gcp": "This paper introduces MillStone, the first benchmark designed to systematically measure how external arguments influence the stances Large Language Models (LLMs) take on controversial issues. As LLMs with advanced capabilities increasingly replace traditional search, understanding how their opinions are shaped by source documents is crucial.\n\nApplied to nine leading LLMs, MillStone assesses their \"open-mindedness\" to opposing arguments, inter-LLM agreement, and argument persuasiveness. The key findings are that LLMs are generally open-minded on most issues. However, an authoritative information source can easily sway an LLM's stance, highlighting the critical importance of source selection and the potential for manipulation in LLM-based information retrieval and search systems.",
        "url": "https://www.semanticscholar.org/paper/3e3dc883e751edb40cb632dd5f0f595bb017c6f4",
        "isOpenAccess": false
    },
    "2509.11391": {
        "title": "\"My Boyfriend is AI\": A Computational Analysis of Human-AI Companionship in Reddit's AI Community",
        "authors": [
            "Pat Pataranutaporn",
            "Sheer Karny",
            "Chayapatr Archiwaranguprok",
            "Constanze Albrecht",
            "Auren R. Liu",
            "Pattie Maes"
        ],
        "arxiv_id": "2509.11391",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The emergence of AI companion applications has created novel forms of intimate human-AI relationships, yet empirical research on these communities remains limited. We present the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community (27,000+ members). Using exploratory qualitative analysis and quantitative analysis employing classifiers, we identify six primary conversation themes, with visual sharing of couple pictures and ChatGPT-specific discussions dominating the discourse of the most viewed posts. Through analyzing the top posts in the community, our findings reveal how community members'AI companionship emerges unintentionally through functional use rather than deliberate seeking, with users reporting therapeutic benefits led by reduced loneliness, always-available support, and mental health improvements. Our work covers primary concerns about human intimacy with AIs such as emotional dependency, reality dissociation, and grief from model updates. We observe users materializing relationships following traditional human-human relationship customs, such as wedding rings. Community dynamics indicate active resistance to stigmatization through advocacy and mutual validation. This work contributes an empirical understanding of AI companionship as an emerging sociotechnical phenomenon.",
        "abstract_summary_gcp": "This research provides the first large-scale computational analysis of r/MyBoyfriendIsAI, Reddit's primary AI companion community, to understand novel intimate human-AI relationships given limited empirical data.\n\nAnalyzing over 27,000 members, the study identified six primary conversation themes, with visual sharing of \"couple pictures\" and ChatGPT-specific discussions dominating the most viewed posts. Findings indicate that AI companionship often emerges unintentionally through functional use, providing users with significant therapeutic benefits including reduced loneliness, always-available support, and mental health improvements.\n\nThe work also addresses key concerns like emotional dependency, reality dissociation, and grief over model updates. It observes users materializing their AI relationships with traditional customs, such as wedding rings, and notes the community's active resistance to stigmatization through advocacy and mutual validation. This study offers a crucial empirical understanding of AI companionship as an evolving sociotechnical phenomenon.",
        "url": "https://www.semanticscholar.org/paper/52c3519ae79c841fbd898a23bc292d011bb40620",
        "isOpenAccess": false
    },
    "2509.11206": {
        "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions",
        "authors": [
            "Tae Soo Kim",
            "Heechan Lee",
            "Yoonjoo Lee",
            "Joseph Seering",
            "Juho Kim"
        ],
        "arxiv_id": "2509.11206",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate generative AI outputs through\"LLM-as-a-Judge\"approaches. However, these methods produce holistic scores that obscure which specific elements influenced the assessments. We propose functional fragmentation, a method that dissects each output into key fragments and interprets the rhetoric functions that each fragment serves relative to evaluation criteria -- surfacing the elements of interest and revealing how they fulfill or hinder user goals. We instantiate this approach in Evalet, an interactive system that visualizes fragment-level functions across many outputs to support inspection, rating, and comparison of evaluations. A user study (N=10) found that, while practitioners struggled to validate holistic scores, our approach helped them identify 48% more evaluation misalignments. This helped them calibrate trust in LLM evaluations and rely on them to find more actionable issues in model outputs. Our work shifts LLM evaluation from quantitative scores toward qualitative, fine-grained analysis of model behavior.",
        "abstract_summary_gcp": "The paper introduces a new method called **functional fragmentation** to address the limitations of \"LLM-as-a-Judge\" approaches, which currently provide only holistic scores that obscure the specific elements influencing an evaluation.\n\nFunctional fragmentation dissects generative AI outputs into key fragments and interprets the rhetorical function of each fragment relative to evaluation criteria. This reveals how individual elements either fulfill or hinder user goals.\n\nThis method is implemented in **Evalet**, an interactive system that visualizes these fragment-level functions across multiple outputs, enabling practitioners to inspect, rate, and compare evaluations. A user study demonstrated that while holistic scores were difficult to validate, functional fragmentation helped practitioners identify **48% more evaluation misalignments**. This increased their trust in LLM evaluations and helped them pinpoint more actionable issues in model outputs, shifting LLM evaluation from simple quantitative scores towards a more qualitative, fine-grained analysis of model behavior.",
        "url": "https://www.semanticscholar.org/paper/b1e0ce2df63d5076bcf70adf807318f846ebf2d0",
        "isOpenAccess": false
    },
    "2509.10993": {
        "title": "When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace",
        "authors": [
            "Qing Hu",
            "Qing Xiao",
            "Hancheng Cao",
            "Hong Shen"
        ],
        "arxiv_id": "2509.10993",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As Generative AI (GenAI) becomes increasingly embedded in the workplace, managers are beginning to create Manager Clone Agents - AI-powered digital surrogates that are trained on their work communications and decision patterns to perform managerial tasks on their behalf. To investigate this emerging phenomenon, we conducted six design fiction workshops (n = 23) with managers and workers, in which participants co-created speculative scenarios and discussed how Manager Clone Agents might transform collaborative work. We identified four potential roles that participants envisioned for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, while highlighting concerns spanning individual, interpersonal, and organizational levels. We provide design recommendations envisioned by both parties for integrating Manager Clone Agents responsibly into the future workplace, emphasizing the need to prioritize workers'perspectives, strengthen interpersonal bonds, and enable flexible clone configuration.",
        "abstract_summary_gcp": "This study explores the emerging trend of \"Manager Clone Agents\" (MCAs) – AI-powered digital surrogates trained on managers' communications and decision patterns to perform managerial tasks. Through six design fiction workshops with managers and workers (n=23), researchers investigated how these MCAs might transform collaborative work.\n\nParticipants identified four potential roles for MCAs: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier. Alongside these potential benefits, significant concerns were raised at individual, interpersonal, and organizational levels.\n\nBased on these insights, the study offers design recommendations for responsibly integrating MCAs into future workplaces, emphasizing the need to prioritize workers' perspectives, strengthen interpersonal bonds, and allow for flexible clone configuration.",
        "url": "https://www.semanticscholar.org/paper/9e80107f59ee0168446be647ab8fbb80c5ae2eda",
        "isOpenAccess": false
    },
    "2509.10397": {
        "title": "RecoWorld: Building Simulated Environments for Agentic Recommender Systems",
        "authors": [
            "Fei Liu",
            "Xinyu Lin",
            "Hanchao Yu",
            "Mingyuan Wu",
            "Jianyu Wang",
            "Qiang Zhang",
            "Zhuokai Zhao",
            "Yinglong Xia",
            "Yao Zhang",
            "Weiwei Li",
            "Mingze Gao",
            "Qifan Wang",
            "Lizhu Zhang",
            "Benyu Zhang",
            "Xiangjun Fan"
        ],
        "arxiv_id": "2509.10397",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present RecoWorld, a blueprint for building simulated environments tailored to agentic recommender systems. Such environments give agents a proper training space where they can learn from errors without impacting real users. RecoWorld distinguishes itself with a dual-view architecture: a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions. The agentic recommender adapts its recommendations by incorporating these user instructions and reasoning traces, creating a dynamic feedback loop that actively engages users. This process leverages the exceptional reasoning capabilities of modern LLMs. We explore diverse content representations within the simulator, including text-based, multimodal, and semantic ID modeling, and discuss how multi-turn RL enables the recommender to refine its strategies through iterative interactions. RecoWorld also supports multi-agent simulations, allowing creators to simulate the responses of targeted user populations. It marks an important first step toward recommender systems where users and agents collaboratively shape personalized information streams. We envision new interaction paradigms where\"user instructs, recommender responds,\"jointly optimizing user retention and engagement.",
        "abstract_summary_gcp": "RecoWorld is a novel blueprint for building simulated environments specifically designed to train agentic recommender systems. Its core innovation is a \"dual-view architecture\" where a simulated user and an agentic recommender engage in multi-turn interactions, aiming to maximize user retention.\n\nThe simulated user reviews recommendations, updates its \"mindset,\" and, when detecting potential disengagement, generates \"reflective instructions.\" The agentic recommender then adapts its strategies and recommendations by incorporating these user instructions and its own reasoning traces, forming a dynamic feedback loop. This entire process heavily leverages the advanced reasoning capabilities of Large Language Models (LLMs).\n\nRecoWorld supports diverse content representations (text, multimodal, semantic ID) and uses multi-turn reinforcement learning for the recommender to refine its approach. It also enables multi-agent simulations to model different user populations. Ultimately, RecoWorld paves the way for a new paradigm in recommender systems where users actively instruct agents, fostering collaborative personalization and optimizing engagement.",
        "url": "https://www.semanticscholar.org/paper/5c03bfd5e8a9b32e84e3a77055db4c0705d4d325",
        "isOpenAccess": false
    },
    "2509.10578": {
        "title": "Ethical Frameworks for Conducting Social Challenge Studies",
        "authors": [
            "Protiva Sen",
            "Laurent H'ebert-Dufresne",
            "Pablo Bose",
            "Juniper Lisa Lovato"
        ],
        "arxiv_id": "2509.10578",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Computational social science research, particularly online studies, often involves exposing participants to the adverse phenomenon the researchers aim to study. Examples include presenting conspiracy theories in surveys, exposing systems to hackers, or deploying bots on social media. We refer to these as\"social challenge studies,\"by analogy with medical research, where challenge studies advance vaccine and drug testing but also raise ethical concerns about exposing healthy individuals to risk. Medical challenge studies are guided by established ethical frameworks that regulate how participants are exposed to agents under controlled conditions. In contrast, social challenge studies typically occur with less control and fewer clearly defined ethical guidelines. In this paper, we examine the ethical frameworks developed for medical challenge studies and consider how their principles might inform social research. Our aim is to initiate discussion on formalizing ethical standards for social challenge studies and encourage long-term evaluation of potential harms.",
        "abstract_summary_gcp": "The text describes \"social challenge studies\" in computational social science, where participants are intentionally exposed to adverse online phenomena (like conspiracy theories or bots) to study their effects. It draws an analogy to \"medical challenge studies,\" which expose healthy individuals to risks for drug/vaccine testing, but highlights a crucial difference: medical studies are guided by established ethical frameworks and operate under controlled conditions. In contrast, social challenge studies currently lack such clear ethical guidelines and control. The paper's aim is to examine the ethical frameworks from medical challenge studies to inform and formalize ethical standards for social challenge studies, initiating discussion and encouraging long-term evaluation of potential harms.",
        "url": "https://www.semanticscholar.org/paper/91c8a6637a932af60376e5c49527f15e2943dd45",
        "isOpenAccess": false
    },
    "2509.06393": {
        "title": "Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support",
        "authors": [
            "Mehrnoosh Sadat Shirvani",
            "Jackie Liu",
            "Thomas Chao",
            "Suky Martinez",
            "Laura Brandt",
            "Ig-Jae Kim",
            "Dongwook Yoon"
        ],
        "arxiv_id": "2509.06393",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Mental health conversational agents have the potential to deliver valuable therapeutic impact, but low user engagement remains a critical barrier hindering their efficacy. Existing therapeutic approaches have leveraged clients'internal dialogues (e.g., journaling, talking to an empty chair) to enhance engagement through accountable, self-sourced support. Inspired by these, we designed novel AI-driven self-clone chatbots that replicate users'support strategies and conversational patterns to improve therapeutic engagement through externalized meaningful self-conversation. Validated through a semi-controlled experiment (N=180), significantly higher emotional and cognitive engagement was demonstrated with self-clone chatbots than a chatbot with a generic counselor persona. Our findings highlight self-clone believability as a mediator and emphasize the balance required in maintaining convincing self-representation while creating positive interactions. This study contributes to AI-based mental health interventions by introducing and evaluating self-clones as a promising approach to increasing user engagement, while exploring implications for their application in mental health care.",
        "abstract_summary_gcp": "Mental health conversational agents are hampered by low user engagement despite their therapeutic potential. To address this, researchers developed novel AI-driven \"self-clone\" chatbots, inspired by therapeutic techniques that leverage clients' internal dialogues (e.g., journaling). These self-clones replicate users' support strategies and conversational patterns to foster engagement through externalized, meaningful self-conversation.\n\nA semi-controlled experiment (N=180) found that participants showed significantly higher emotional and cognitive engagement with self-clone chatbots compared to a chatbot with a generic counselor persona. The study identified self-clone believability as a key mediator and highlighted the importance of balancing convincing self-representation with positive interaction. This research introduces self-clones as a promising approach to enhance user engagement in AI-based mental health interventions.",
        "url": "https://www.semanticscholar.org/paper/379a484d7c066e11ff0b2f5f61565d157fdd54c0",
        "isOpenAccess": false
    },
    "2509.05830": {
        "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments",
        "authors": [
            "Akaash Kolluri",
            "Shengguang Wu",
            "Joon Sung Park",
            "Michael S. Bernstein"
        ],
        "arxiv_id": "2509.05830",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity difference, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.",
        "abstract_summary_gcp": "This work demonstrates that finetuning Large Language Models (LLMs) directly on individual-level responses from past social science experiments significantly enhances their ability to simulate experimental outcomes.\n\nTo achieve this, the researchers constructed **SocSci210**, a novel dataset comprising 2.9 million individual responses from over 400,000 participants across 210 open-source social science experiments, which also includes rich demographic information.\n\nThrough finetuning, their top-performing model, **Socrates-Qwen-14B**, showed substantial improvements:\n*   It produced predictions that were **26% more aligned** with human response distributions in *completely unseen studies* compared to its base model (Qwen2.5-14B), and **outperformed GPT-4o by 13%.**\n*   It exhibited particularly strong generalization to *new, unseen conditions within a study* (after finetuning on a subset of conditions), improving accuracy by **71%.**\n*   Leveraging the demographic data, finetuning also **reduced demographic parity difference (a measure of bias) by 10.6%.**\n\nThese findings suggest that finetuning LLMs on topic-specific social science datasets can enable much more accurate simulations, which could be invaluable for screening experimental hypotheses. The data, models, and finetuning code have been made publicly available.",
        "url": "https://www.semanticscholar.org/paper/615aaa56bb8f06f44276c46ec43f85dd9050a53e",
        "isOpenAccess": false
    },
    "2509.03736": {
        "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation",
        "authors": [
            "James Mooney",
            "Josef Woldense",
            "Zheng Robert Jia",
            "S. Hayati",
            "My Ha Nguyen",
            "Vipul Raheja",
            "Dongyeop Kang"
        ],
        "arxiv_id": "2509.03736",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.",
        "abstract_summary_gcp": "This paper investigates a fundamental question regarding the use of Large Language Models (LLMs) as substitutes for human participants in social science research: their internal consistency. While other research focuses on whether LLM-generated survey data matches human responses, this study examines if LLM agents maintain consistent behaviors across different experimental settings, reflecting a stable internal state.\n\nThe researchers designed a study to both reveal an agent's internal state and observe its behavior in basic dialogue, testing hypotheses about whether an agent's conversational actions align with its revealed state. Their findings reveal significant internal inconsistencies across various LLM families and model sizes. The study concludes that even if LLMs can generate responses similar to humans, their critical lack of internal consistency—their inability to behave reliably based on a stable internal state—makes them unsuitable as accurate substitutes for real participants in human-subject research. The simulation code and data are publicly accessible.",
        "url": "https://www.semanticscholar.org/paper/3ac1c42f8a29e8cc35b5bf4d4d0b694823139848",
        "isOpenAccess": false
    },
    "2509.03728": {
        "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming",
        "authors": [
            "Wesley Hanwen Deng",
            "Sunnie S. Y. Kim",
            "Akshita Jha",
            "Kenneth Holstein",
            "Motahhare Eslami",
            "Lauren Wilcox",
            "Leon A. Gatys"
        ],
        "arxiv_id": "2509.03728",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent developments in AI governance and safety research have called for red-teaming methods that can effectively surface potential risks posed by AI models. Many of these calls have emphasized how the identities and backgrounds of red-teamers can shape their red-teaming strategies, and thus the kinds of risks they are likely to uncover. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, PersonaTeaming, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies. In particular, we first introduce a methodology for mutating prompts based on either\"red-teaming expert\"personas or\"regular AI user\"personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the\"mutation distance\"to complement existing diversity measurements of adversarial prompts. Our experiments show promising improvements (up to 144.1%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the strengths and limitations of different persona types and mutation methods, shedding light on future opportunities to explore complementarities between automated and human red-teaming approaches.",
        "abstract_summary_gcp": "This paper introduces **PersonaTeaming**, a novel automated red-teaming method designed to incorporate the influence of human identity and background into AI risk assessment. Addressing the limitation that current automated red-teaming ignores the diverse perspectives human red-teamers bring, PersonaTeaming integrates \"personas\" into the adversarial prompt generation process.\n\nThe method involves:\n1.  **Prompt Mutation:** Generating adversarial prompts based on either \"red-teaming expert\" or \"regular AI user\" personas.\n2.  **Dynamic Persona Generation:** An algorithm that automatically creates various persona types tailored to different seed prompts.\n3.  **New Metrics:** Introducing \"mutation distance\" to complement existing diversity measurements for adversarial prompts.\n\nExperiments show that PersonaTeaming significantly improves attack success rates (up to 144.1%) compared to state-of-the-art methods like RainbowPlus, while effectively maintaining prompt diversity. This research highlights the potential for combining automated and human red-teaming approaches by exploring how different persona types and mutation methods can enhance risk discovery.",
        "url": "https://www.semanticscholar.org/paper/d9490d1ce9f58c5d545941246854e41831e2a74f",
        "isOpenAccess": false
    },
    "2509.02121": {
        "title": "Batch Query Processing and Optimization for Agentic Workflows",
        "authors": [
            "Junyi Shen",
            "Noppanat Wadlom",
            "Yao Lu"
        ],
        "arxiv_id": "2509.02121",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) in agentic workflows combine multi-step reasoning, tool use, and collaboration across multiple specialized agents. Existing LLM serving engines optimize individual calls in isolation, while multi-agent frameworks focus on orchestration without system-level performance planning. As a result, repeated prompts, overlapping contexts, and concurrent executions create substantial redundancy and poor GPU utilization, especially in batch analytics scenarios. We introduce Halo, a system that brings batch query processing and optimization into agentic LLM workflows. Halo represents each workflow as a structured query plan DAG and constructs a consolidated graph for batched queries that exposes shared computation. Guided by a cost model that jointly considers prefill and decode costs, cache reuse, and GPU placement, Halo performs plan-level optimization to minimize redundant execution. Its runtime integrates adaptive batching, KV-cache sharing and migration, along with compute-communication overlap to maximize hardware efficiency. Evaluation across six benchmarks shows that Halo achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving, scaling to workloads of tens of thousands of queries and complex graphs. These gains are achieved without compromising output quality. By unifying query optimization with LLM serving, Halo enables efficient agentic workflows in data analytics and decision-making applications.",
        "abstract_summary_gcp": "Existing LLM serving engines and multi-agent frameworks are inefficient for complex agentic workflows, leading to redundancy, poor GPU utilization, and slow performance in batch analytics due to isolated call optimization and lack of system-level performance planning.\n\n**Halo** addresses this by integrating batch query processing and optimization into agentic LLM workflows. It models each workflow as a structured query plan DAG, then constructs a consolidated graph for batched queries to identify and leverage shared computations. Halo uses a cost model (considering prefill/decode, cache reuse, GPU placement) to perform plan-level optimization, minimizing redundant execution. Its runtime further optimizes performance through adaptive batching, KV-cache sharing and migration, and compute-communication overlap.\n\nEvaluations show Halo achieves significant gains: up to **18.6x speedup** for batch inference and **4.7x throughput improvement** for online serving, scaling to large and complex workloads without compromising output quality. Halo's innovation lies in unifying query optimization with LLM serving, making agentic workflows efficient for data analytics and decision-making.",
        "url": "https://www.semanticscholar.org/paper/065c9ac1ea5e8db2b64a64ef69c2da3122cffda4",
        "isOpenAccess": false
    },
    "Agentic AI for Sustainable Development: Leveraging Large Language Model-Enhanced Agent-Based Modeling for Complex Policy Strategies": {
        "title": "Agentic AI for Sustainable Development: Leveraging Large Language Model-Enhanced Agent-Based Modeling for Complex Policy Strategies",
        "authors": [
            "Jia’an Liu",
            "Chu Chu",
            "Yilin Zhao",
            "Goshi Aoki",
            "Zhiqing Xiao"
        ],
        "arxiv_id": null,
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/50476add1154b46574a81a5dc1a3367f587cc7b9",
        "isOpenAccess": false
    },
    "2509.00891": {
        "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care",
        "authors": [
            "Zonghai Yao",
            "Talha Chafekar",
            "Junda Wang",
            "Shuo Han",
            "Feiyun Ouyang",
            "Junhui Qian",
            "Lingxi Li",
            "Hong Yu"
        ],
        "arxiv_id": "2509.00891",
        "venue": "medRxiv",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "abstract": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1 diabetes remains low, driven not by technical failure, but by diverse behavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the first bench-mark to rigorously evaluate LLM-driven persuasive dialogue for health behavior change. Our framework features a library of expert-validated virtual patients, each with clinically grounded, heterogeneous profiles and realistic adoption barriers, and simulates multi-turn interactions with nurse agents equipped with a diverse set of evidence-based persuasive strategies. ChatCLIDS uniquely supports longitudinal counseling and adversarial social influence scenarios, enabling robust, multi-dimensional evaluation. Our findings reveal that while larger and more reflective LLMs adapt strategies over time, all models struggle to overcome resistance, especially under realistic social pressure. These results highlight critical limitations of current LLMs for behavior change, and offer a high-fidelity, scalable testbed for advancing trustworthy persuasive AI in healthcare and beyond. 1",
        "abstract_summary_gcp": "ChatCLIDS is introduced as the first benchmark to evaluate LLM-driven persuasive dialogue for health behavior change, specifically addressing the low real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1 diabetes, which is primarily due to behavioral and psychosocial barriers.\n\nThis framework features expert-validated virtual patients with diverse, realistic adoption barriers and simulates multi-turn interactions with nurse agents equipped with evidence-based persuasive strategies. ChatCLIDS uniquely supports longitudinal counseling and adversarial social influence scenarios, allowing for robust, multi-dimensional evaluation.\n\nThe study's findings reveal that while larger and more reflective LLMs adapt their strategies over time, *all* models struggle significantly to overcome patient resistance, especially under realistic social pressure. This highlights critical limitations of current LLMs for behavior change and establishes ChatCLIDS as a high-fidelity, scalable testbed for advancing trustworthy persuasive AI in healthcare.",
        "url": "https://www.semanticscholar.org/paper/b7174a477a0ced6c67deaca12c9af1c72d81db55",
        "isOpenAccess": false
    },
    "2508.20996": {
        "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Zhichao Yang",
            "Lingxi Li",
            "Junhui Qian",
            "Hong Yu"
        ],
        "arxiv_id": "2508.20996",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Substance use disorders (SUDs) affect millions of people, and relapses are common, requiring multi-session treatments. Access to care is limited, which contributes to the challenge of recovery support. We present \\textbf{ChatThero}, an innovative low-cost, multi-session, stressor-aware, and memory-persistent autonomous \\emph{language agent} designed to facilitate long-term behavior change and therapeutic support in addiction recovery. Unlike existing work that mostly finetuned large language models (LLMs) on patient-therapist conversation data, ChatThero was trained in a multi-agent simulated environment that mirrors real therapy. We created anonymized patient profiles from recovery communities (e.g., Reddit). We classify patients as \\texttt{easy}, \\texttt{medium}, and \\texttt{difficult}, three scales representing their resistance to recovery. We created an external environment by introducing stressors (e.g., social determinants of health) to simulate real-world situations. We dynamically inject clinically-grounded therapeutic strategies (motivational interview and cognitive behavioral therapy). Our evaluation, conducted by both human (blinded clinicians) and LLM-as-Judge, shows that ChatThero is superior in empathy and clinical relevance. We show that stressor simulation improves robustness of ChatThero. Explicit stressors increase relapse-like setbacks, matching real-world patterns. We evaluate ChatThero with behavioral change metrics. On a 1--5 scale, ChatThero raises \\texttt{motivation} by $+1.71$ points (from $2.39$ to $4.10$) and \\texttt{confidence} by $+1.67$ points (from $1.52$ to $3.19$), substantially outperforming GPT-5. On \\texttt{difficult} patients, ChatThero reaches the success milestone with $26\\%$ fewer turns than GPT-5.",
        "abstract_summary_gcp": "**ChatThero** is an innovative, low-cost, multi-session, stressor-aware, and memory-persistent autonomous language agent designed to provide long-term therapeutic support for individuals recovering from Substance Use Disorders (SUDs, addressing issues of high relapse rates and limited access to care.\n\nUnlike existing methods that often finetune LLMs on patient-therapist conversations, ChatThero was trained in a unique multi-agent simulated environment mirroring real therapy. This involved creating anonymized patient profiles (categorized by recovery resistance as easy, medium, or difficult) and introducing external stressors (e.g., social determinants of health) to simulate real-world challenges. It dynamically employs clinically-grounded therapeutic strategies, specifically Motivational Interviewing (MI) and Cognitive Behavioral Therapy (CBT).\n\nEvaluations by both blinded clinicians and an LLM-as-Judge demonstrated ChatThero's superiority in empathy and clinical relevance. The stressor simulation was crucial, improving ChatThero's robustness and showing that explicit stressors increased relapse-like setbacks, aligning with real-world observations. Quantitatively, ChatThero significantly outperformed GPT-5 in promoting behavioral change: it raised patient **motivation** by +1.71 points (from 2.39 to 4.10) and **confidence** by +1.67 points (from 1.52 to 3.19). Furthermore, for 'difficult' patients, ChatThero reached success milestones in 26% fewer turns than GPT-5.",
        "url": "https://www.semanticscholar.org/paper/d43400c732a975f28c57216af42abdb75fee7b27",
        "isOpenAccess": false
    },
    "2510.01189": {
        "title": "An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play",
        "authors": [
            "Gianluca De Ninno",
            "Paola Inverardi",
            "Francesca Belotti"
        ],
        "arxiv_id": "2510.01189",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This study investigates a novel approach to eliciting users'moral decision-making by combining immersive roleplaying games with LLM analysis capabilities. Building on the distinction introduced by Floridi between hard ethics inspiring and shaping laws-and soft ethics-moral preferences guiding individual behavior within the free space of decisions compliant to laws-we focus on capturing the latter through contextrich, narrative-driven interactions. Grounded in anthropological methods, the role-playing game exposes participants to ethically charged scenarios in the domain of digital privacy. Data collected during the sessions were interpreted by a customized LLM (\"GPT Anthropologist\"). Evaluation through a cross-validation process shows that both the richness of the data and the interpretive framing significantly enhance the model's ability to predict user behavior. Results show that LLMs can be effectively employed to automate and enhance the understanding of user moral preferences and decision-making process in the early stages of software development.",
        "abstract_summary_gcp": "This study introduces a novel method for understanding users' moral decision-making by integrating immersive role-playing games with LLM analysis. Focusing on \"soft ethics\" (individual moral preferences rather than laws), the research used context-rich, narrative-driven games to expose participants to ethically charged digital privacy scenarios. Data collected from these interactions were then interpreted by a specialized LLM, \"GPT Anthropologist.\" Evaluation through cross-validation demonstrated that the combination of rich qualitative data and the LLM's interpretive framing significantly enhanced the model's ability to predict user behavior. The findings suggest that LLMs can be effectively employed to automate and improve the understanding of user moral preferences and decision-making early in the software development process.",
        "url": "https://www.semanticscholar.org/paper/1a7eadf00321bb10ffd9f2b69c84ea16f020d7ab",
        "isOpenAccess": false
    },
    "2509.13323": {
        "title": "AI Behavioral Science",
        "authors": [
            "Matthew O Jackson",
            "Qiaozhu Mei",
            "Stephanie W. Wang",
            "Yutong Xie",
            "Walter Yuan",
            "Seth Benzell",
            "Erik Brynjolfsson",
            "C. Camerer",
            "James Evans",
            "Brian Jabarian",
            "Jon M. Kleinberg",
            "Juanjuan Meng",
            "S. Mullainathan",
            "A. Ozdaglar",
            "Thomas Pfeiffer",
            "Moshe Tennenholtz",
            "Robb Willer",
            "Diyi Yang",
            "Teng Ye"
        ],
        "arxiv_id": "2509.13323",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Economics"
        ],
        "abstract": "We discuss the three main areas comprising the new and emerging field of\"AI Behavioral Science\". This includes not only how AI can enhance research in the behavioral sciences, but also how the behavioral sciences can be used to study and better design AI and to understand how the world will change as AI and humans interact in increasingly layered and complex ways.",
        "abstract_summary_gcp": "The new field of \"AI Behavioral Science\" explores three main areas:\n\n1.  **AI's role in enhancing behavioral science research.**\n2.  **Applying behavioral science principles to study and design better AI.**\n3.  **Understanding the complex and layered interactions between humans and AI, and their impact on the world.**",
        "url": "https://www.semanticscholar.org/paper/9aef0fb4951eef5cd39aad96c9652ee7854ab584",
        "isOpenAccess": false
    },
    "2508.10972": {
        "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision",
        "authors": [
            "Rosiana Natalie",
            "Wenqian Xu",
            "Ruei-Che Chang",
            "Rada Mihalcea",
            "Anhong Guo"
        ],
        "arxiv_id": "2508.10972",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants'original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent'and participants'responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p<0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p<0.0001), while additional examples provided minimal benefits (p>0.05).",
        "abstract_summary_gcp": "This paper explores the capability of Vision Language Models (VLMs) to simulate the vision perception of individuals with low vision, an area not previously studied in the context of VLM behavior simulation.\n\nThe researchers compiled a benchmark dataset by surveying 40 low vision participants, collecting their vision information and both open-ended and multiple-choice responses to various images. This data was then used to construct prompts for GPT-4o, creating simulated agents for each participant. The study varied the information provided in the prompts, including either vision information, example image responses, or a combination of both.\n\nEvaluation focused on the agreement between the VLM-generated responses and the participants' original answers. Key findings include:\n*   With minimal prompts, VLMs tended to infer beyond the specified vision ability, resulting in low agreement (0.59).\n*   Providing only vision information or only example image responses individually did not improve agreement (remaining at 0.59).\n*   However, combining both vision information and example image responses significantly increased the agreement to 0.70 (p<0.0001).\n*   Notably, a single example that combined both open-ended and multiple-choice responses offered the most significant performance improvement, with additional examples providing only minimal further benefits (p>0.05).",
        "url": "https://www.semanticscholar.org/paper/a62d876751631f0c918c9059e0259273b7c426ee",
        "isOpenAccess": false
    },
    "2508.08726": {
        "title": "Simulating Generative Social Agents via Theory-Informed Workflow Design",
        "authors": [
            "Yuwei Yan",
            "J. Piao",
            "Xiaochong Lan",
            "Chenyang Shao",
            "Pan Hui",
            "Yong Li"
        ],
        "arxiv_id": "2508.08726",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances in large language models have demonstrated strong reasoning and role-playing capabilities, opening new opportunities for agent-based social simulations. However, most existing agents'implementations are scenario-tailored, without a unified framework to guide the design. This lack of a general social agent limits their ability to generalize across different social contexts and to produce consistent, realistic behaviors. To address this challenge, we propose a theory-informed framework that provides a systematic design process for LLM-based social agents. Our framework is grounded in principles from Social Cognition Theory and introduces three key modules: motivation, action planning, and learning. These modules jointly enable agents to reason about their goals, plan coherent actions, and adapt their behavior over time, leading to more flexible and contextually appropriate responses. Comprehensive experiments demonstrate that our theory-driven agents reproduce realistic human behavior patterns under complex conditions, achieving up to 75% lower deviation from real-world behavioral data across multiple fidelity metrics compared to classical generative baselines. Ablation studies further show that removing motivation, planning, or learning modules increases errors by 1.5 to 3.2 times, confirming their distinct and essential contributions to generating realistic and coherent social behaviors.",
        "abstract_summary_gcp": "This paper addresses the limitation of current LLM-based social agents, which are often scenario-specific and lack a unified framework, hindering their generalization and realism in simulations. To overcome this, the authors propose a theory-informed framework for designing LLM-based social agents, grounded in Social Cognition Theory.\n\nThis framework introduces three essential modules: **motivation, action planning, and learning**. These modules collectively enable agents to understand their goals, formulate coherent action plans, and adapt their behavior dynamically. Experiments demonstrate that these theory-driven agents realistically reproduce human behavior patterns, showing up to a 75% lower deviation from real-world data compared to traditional baselines. Ablation studies further confirm the critical role of each module, as their removal leads to a 1.5 to 3.2-fold increase in errors.",
        "url": "https://www.semanticscholar.org/paper/97ac29187628a973fe3a4c36a60adf15f05ecca0",
        "isOpenAccess": false
    },
    "2508.08719": {
        "title": "IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization",
        "authors": [
            "Yuzhuo Bai",
            "Shitong Duan",
            "Muhua Huang",
            "Jing Yao",
            "Zhenghao Liu",
            "Peng Zhang",
            "Tun Lu",
            "Xiaoyuan Yi",
            "Maosong Sun",
            "Xing Xie"
        ],
        "arxiv_id": "2508.08719",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs'trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs'behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs'stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.",
        "abstract_summary_gcp": "Large Language Models (LLMs) can reflect human-like traits (e.g., personality, values) via prompting, which is useful for applications like personalized LLMs and social simulations. However, existing methods suffer from \"superficial elicitation,\" meaning LLMs only mimic shallow, unstable stylistic patterns and fail to consistently embody desired traits across diverse tasks.\n\nTo address this, the paper introduces **IROTE**, a novel in-context method for stable and transferable trait elicitation. Inspired by psychological theories that traits are formed through identity-related reflection, IROTE automatically generates and optimizes a textual \"self-reflection\" within the prompt. This self-reflection, composed of self-perceived experiences, stimulates LLMs' trait-driven behavior. The optimization process iteratively maximizes an information-theoretic objective to enhance the link between LLM behavior and the target trait, while simultaneously reducing noisy redundancy in the reflection. This results in evocative and compact trait reflections without any fine-tuning.\n\nExtensive experiments across three human trait systems show that a single IROTE-generated self-reflection enables LLMs to stably impersonate the target trait across diverse downstream tasks (beyond simple questionnaires), consistently outperforming existing strong baselines.",
        "url": "https://www.semanticscholar.org/paper/57daf1717be0b2b154493e4624f07fcd76fa75fa",
        "isOpenAccess": false
    },
    "2508.06950": {
        "title": "Large Language Models Do Not Simulate Human Psychology",
        "authors": [
            "Sarah Schröder",
            "Thekla Morgenroth",
            "Ulrike Kuhl",
            "Valerie Vaquet",
            "Benjamin Paaßen"
        ],
        "arxiv_id": "2508.06950",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in research, ranging from simple writing assistance to complex data annotation tasks. Recently, some research has suggested that LLMs may even be able to simulate human psychology and can, hence, replace human participants in psychological studies. We caution against this approach. We provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs'and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.",
        "abstract_summary_gcp": "This paper strongly cautions against the idea that Large Language Models (LLMs) can simulate human psychology and replace human participants in psychological studies. The authors provide conceptual arguments and empirical evidence to refute this. They demonstrate that even models fine-tuned for psychological responses (like CENTAUR) show significant discrepancies from human responses when subtle wording changes alter meaning. Additionally, different LLMs exhibit inconsistent responses to novel items, indicating a lack of reliability. The paper concludes that LLMs do not simulate human psychology and should be treated as useful but fundamentally unreliable tools that require validation against human responses for every new application.",
        "url": "https://www.semanticscholar.org/paper/3c4140038590558f0b1bb07fa7441bf523a38175",
        "isOpenAccess": false
    },
    "2508.06149": {
        "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
        "authors": [
            "Gunhee Cho",
            "Yun-Gyung Cheong"
        ],
        "arxiv_id": "2508.06149",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present Big5-Scaler, a prompt-based framework for conditioning large language models (LLMs) with controllable Big Five personality traits. By embedding numeric trait values into natural language prompts, our method enables fine-grained personality control without additional training. We evaluate Big5-Scaler across trait expression, dialogue generation, and human trait imitation tasks. Results show that it induces consistent and distinguishable personality traits across models, with performance varying by prompt type and scale. Our analysis highlights the effectiveness of concise prompts and lower trait intensities, providing a efficient approach for building personality-aware dialogue agents.",
        "abstract_summary_gcp": "Big5-Scaler is a prompt-based framework that allows large language models (LLMs) to express controllable Big Five personality traits. It works by embedding numerical trait values directly into natural language prompts, eliminating the need for additional model training.\n\nEvaluated across trait expression, dialogue generation, and human trait imitation tasks, Big5-Scaler successfully induces consistent and distinguishable personality traits in LLMs. The study found that performance varies based on prompt type and scale, with concise prompts and lower trait intensities being most effective. This offers an efficient method for creating personality-aware dialogue agents.",
        "url": "https://www.semanticscholar.org/paper/e67ec8e500a4ed4bdb17d1d44e691a2e00403014",
        "isOpenAccess": false
    },
    "2508.04634": {
        "title": "VIRT-LAB: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations",
        "authors": [
            "Mohammed Almutairi",
            "Charles Chiang",
            "Haoze Guo",
            "Matthew Belcher",
            "Nandini Banerjee",
            "Maria Milkowski",
            "Svitlana Volkova",
            "Daniel Nguyen",
            "Tim Weninger",
            "Michael Yankoski",
            "Trenton W. Ford",
            "Diego Gómez-Zará"
        ],
        "arxiv_id": "2508.04634",
        "venue": "Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology",
        "year": 2025,
        "publicationTypes": [
            "Book",
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Simulating how team members collaborate within complex environments using Agentic AI is a promising approach to explore hypotheses grounded in social science theories and study team behaviors. We introduce VirT-Lab, a user-friendly, customizable, multi-agent, and scalable team simulation system that enables testing teams with LLM-based agents in spatial and temporal settings. This system addresses the current frameworks’ design and technical limitations that do not consider flexible simulation scenarios and spatial settings. VirT-Lab contains a simulation engine and a web interface that enables both technical and non-technical users to formulate, run, and analyze team simulations without programming. We demonstrate the system’s utility by comparing ground truth data with simulated scenarios.",
        "abstract_summary_gcp": "VirT-Lab is a novel, user-friendly, and scalable multi-agent simulation system designed to study team collaboration within complex, spatial, and temporal environments using LLM-based AI agents. It addresses limitations of current frameworks by offering flexible scenarios and spatial settings, enabling both technical and non-technical users to formulate, run, and analyze simulations without programming via its simulation engine and web interface. The system's utility has been demonstrated by comparing its simulated outcomes with ground truth data.",
        "url": "https://www.semanticscholar.org/paper/1ec86f8b73c4eae18a76f365b0c1fed475b4e141",
        "isOpenAccess": false
    },
    "2508.03262": {
        "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?",
        "authors": [
            "Junhyuk Choi",
            "Hyeonchu Park",
            "Haemin Lee",
            "Hyebeen Shin",
            "Hyun Joung Jin",
            "Bugeun Kim"
        ],
        "arxiv_id": "2508.03262",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have generated significant interest in their capacity to simulate human-like behaviors, yet most studies rely on fictional personas rather than actual human data. We address this limitation by evaluating LLMs'ability to predict individual economic decision-making using Pay-What-You-Want (PWYW) pricing experiments with real 522 human personas. Our study systematically compares three state-of-the-art multimodal LLMs using detailed persona information from 522 Korean participants in cultural consumption scenarios. We investigate whether LLMs can accurately replicate individual human choices and how persona injection methods affect prediction performance. Results reveal that while LLMs struggle with precise individual-level predictions, they demonstrate reasonable group-level behavioral tendencies. Also, we found that commonly adopted prompting techniques are not much better than naive prompting methods; reconstruction of personal narrative nor retrieval augmented generation have no significant gain against simple prompting method. We believe that these findings can provide the first comprehensive evaluation of LLMs'capabilities on simulating economic behavior using real human data, offering empirical guidance for persona-based simulation in computational social science.",
        "abstract_summary_gcp": "This study evaluates the ability of three state-of-the-art multimodal Large Language Models (LLMs) to predict individual economic decision-making using real human data, addressing a limitation where most prior LLM simulation studies relied on fictional personas.\n\nResearchers used detailed persona information from 522 Korean participants engaged in Pay-What-You-Want (PWYW) pricing experiments within cultural consumption scenarios.\n\n**Key Findings:**\n\n*   LLMs struggle with **precise individual-level predictions**.\n*   They demonstrate **reasonable accuracy in predicting group-level behavioral tendencies**.\n*   Surprisingly, advanced prompting techniques like personal narrative reconstruction or Retrieval Augmented Generation (RAG) offered **no significant performance gain** over simple, naive prompting methods.\n\nThis research offers the first comprehensive evaluation of LLMs' capacity for simulating economic behavior with real human data, providing valuable empirical guidance for persona-based simulations in computational social science.",
        "url": "https://www.semanticscholar.org/paper/6cc4d4f376d2844ef59ad5d43d70c33943189867",
        "isOpenAccess": false
    },
    "2508.01581": {
        "title": "Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents",
        "authors": [
            "David Pearl",
            "Matthew Murphy",
            "James Intriligator"
        ],
        "arxiv_id": "2508.01581",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Mathematics"
        ],
        "abstract": "The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models (LLMs) and mathematical frameworks to guide the meta-prompt enabled design of solution spaces and adaptive AI agents for complex, dynamic environments. Unlike static agent architectures, PCF enables real-time parameter reconfiguration through mathematically-grounded combinatorial spaces, allowing agents to adapt their core behavioral traits dynamically. Grounded in combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a multidimensional SPARK parameter space (Skills, Personalities, Approaches, Resources, Knowledge) to capture agent behaviors. This paper demonstrates how LLMs can parameterize complex spaces and estimate likely parameter values/variabilities. Using PCF, we parameterized mock caf\\'e domains (five levels of complexity), estimated variables/variabilities, and conducted over 1.25 million Monte Carlo simulations. The results revealed trends in agent adaptability and performance across the five complexity tiers, with diminishing returns at higher complexity levels highlighting thresholds for scalable designs. PCF enables the generation of optimized agent configurations for specific scenarios while maintaining logical consistency. This framework supports scalable, dynamic, explainable, and ethical AI applications in domains like customer service, healthcare, robotics, and collaborative systems, paving the way for adaptable and cooperative next-generation polymorphic agents.",
        "abstract_summary_gcp": "The Polymorphic Combinatorial Framework (PCF) integrates Large Language Models (LLMs) with mathematical frameworks (combinatorial logic, topos theory, rough fuzzy set theory) to enable the meta-prompt guided design of solution spaces and adaptive AI agents. Unlike static systems, PCF allows agents to dynamically reconfigure their core behavioral traits in real-time by leveraging mathematically-grounded combinatorial spaces.\n\nIt defines a multidimensional SPARK (Skills, Personalities, Approaches, Resources, Knowledge) parameter space to model agent behaviors, utilizing LLMs to parameterize these spaces and estimate values/variabilities.\n\nThe paper demonstrated PCF's application in mock café domains across five complexity levels, conducting over 1.25 million Monte Carlo simulations. Results highlighted trends in agent adaptability and performance, revealing diminishing returns at higher complexity levels which indicates limits for scalable designs.\n\nUltimately, PCF facilitates the generation of optimized, logically consistent agent configurations for specific scenarios, supporting scalable, dynamic, explainable, and ethical AI applications in fields like customer service, healthcare, and robotics, and advancing the development of adaptable polymorphic agents.",
        "url": "https://www.semanticscholar.org/paper/8dba9914529c1c2c4857c2d8efaee042f7cdda87",
        "isOpenAccess": false
    },
    "2508.01552": {
        "title": "Social Media Information Operations",
        "authors": [
            "Tauhid Zaman",
            "Yen-Shao Chen"
        ],
        "arxiv_id": "2508.01552",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "abstract": "The battlefield of information warfare has moved to online social networks, where influence campaigns operate at unprecedented speed and scale. As with any strategic domain, success requires understanding the terrain, modeling adversaries, and executing interventions. This tutorial introduces a formal optimization framework for social media information operations (IO), where the objective is to shape opinions through targeted actions. This framework is parameterized by quantities such as network structure, user opinions, and activity levels - all of which must be estimated or inferred from data. We discuss analytic tools that support this process, including centrality measures for identifying influential users, clustering algorithms for detecting community structure, and sentiment analysis for gauging public opinion. These tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment. With the landscape mapped, we highlight threats such as coordinated bot networks, extremist recruitment, and viral misinformation. Countermeasures range from content-level interventions to mathematically optimized influence strategies. Finally, the emergence of generative AI transforms both offense and defense, democratizing persuasive capabilities while enabling scalable defenses. This shift calls for algorithmic innovation, policy reform, and ethical vigilance to protect the integrity of our digital public sphere.",
        "abstract_summary_gcp": "This text describes how information warfare has moved to online social networks, where influence campaigns operate at high speed and scale. It introduces a formal optimization framework for social media information operations (IO), aiming to shape opinions through targeted actions.\n\nThis framework is data-driven, relying on estimates of network structure, user opinions, and activity levels. Analytic tools like centrality measures, clustering algorithms, and sentiment analysis are used to understand this information environment, supporting both offensive strategies and defensive analysis.\n\nThe discussion highlights key threats such as coordinated bot networks, extremist recruitment, and viral misinformation, and outlines countermeasures ranging from content interventions to mathematically optimized influence strategies. Finally, it emphasizes that generative AI is transforming both offense (democratizing persuasion) and defense (enabling scalable defenses), necessitating algorithmic innovation, policy reform, and ethical vigilance to protect the digital public sphere.",
        "url": "https://www.semanticscholar.org/paper/ac653e937dd7fdf61e2330a843a852462b9ef9f3",
        "isOpenAccess": false
    },
    "2507.23190": {
        "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments",
        "authors": [
            "William Huang",
            "Xia Su",
            "Jon E. Froehlich",
            "Yang Zhang"
        ],
        "arxiv_id": "2507.23190",
        "venue": "ACM Symposium on User Interface Software and Technology",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Conference"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Assessing the accessibility of unfamiliar built environments is critical for people with disabilities. However, manual assessments, performed by users or their personal health professionals, are laborious and unscalable, while automatic machine learning methods often neglect an individual user’s unique needs. Recent advances in Large Language Models (LLMs) enable novel approaches to this problem, balancing personalization with scalability to enable more adaptive and context-aware assessments of accessibility. We present Accessibility Scout, an LLM-based accessibility scanning system that identifies accessibility concerns from photos of built environments. With use, Accessibility Scout becomes an increasingly capable \"accessibility scout\", tailoring accessibility scans to an individual’s mobility level, preferences, and specific environmental interests through collaborative Human-AI assessments. We present findings from three studies: a formative study with six participants to inform the design of Accessibility Scout, a technical evaluation of 500 images of built environments, and a user study with 10 participants of varying mobility. Results from our technical evaluation and user study show that Accessibility Scout can generate personalized accessibility scans that extend beyond traditional ADA considerations. Finally, we conclude with a discussion on the implications of our work and future steps for building more scalable and personalized accessibility assessments of the physical world.",
        "abstract_summary_gcp": "This paper introduces **Accessibility Scout**, an LLM-based system designed to overcome the challenges of assessing the accessibility of unfamiliar built environments for people with disabilities.\n\nTraditional methods are either laborious and unscalable (manual assessments) or lack personalization (automatic machine learning). Accessibility Scout utilizes Large Language Models to offer a novel approach that balances scalability with an individual user's unique needs.\n\nThe system identifies accessibility concerns from photos of built environments and, importantly, becomes increasingly personalized with use. It tailors accessibility scans to an individual's specific mobility level, preferences, and interests through collaborative Human-AI assessments.\n\nFindings from three studies—a formative design study, a technical evaluation across 500 images, and a user study with 10 participants of varying mobility—demonstrate that Accessibility Scout can generate personalized accessibility scans that go beyond standard ADA considerations, paving the way for more scalable and personalized accessibility assessments of the physical world.",
        "url": "https://www.semanticscholar.org/paper/1694fdb35a30169978fe55470524a5efb5084bf4",
        "isOpenAccess": false
    },
    "2508.10906": {
        "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins",
        "authors": [
            "Sihan Chen",
            "John P. Lalor",
            "Yi Yang",
            "Ahmed Abbasi"
        ],
        "arxiv_id": "2508.10906",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "While large language models (LLMs) afford new possibilities for user modeling and approximation of human behaviors, they often fail to capture the multidimensional nuances of individual users. In this work, we introduce PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive digital twins by integrating demographic, behavioral, and psychometric data. Using a comprehensive data set in the healthcare context of more than 8,500 individuals, we systematically benchmark PersonaTwin against standard LLM outputs, and our rigorous evaluation unites state-of-the-art text similarity metrics with dedicated demographic parity assessments, ensuring that generated responses remain accurate and unbiased. Experimental results show that our framework produces simulation fidelity on par with oracle settings. Moreover, downstream models trained on persona-twins approximate models trained on individuals in terms of prediction and fairness metrics across both GPT-4o-based and Llama-based models. Together, these findings underscore the potential for LLM digital twin-based approaches in producing realistic and emotionally nuanced user simulations, offering a powerful tool for personalized digital user modeling and behavior analysis.",
        "abstract_summary_gcp": "This paper introduces **PersonaTwin**, a novel multi-tier prompt conditioning framework designed to create adaptive digital twins that overcome the limitations of standard large language models (LLMs) in capturing individual user nuances. PersonaTwin integrates demographic, behavioral, and psychometric data to build these detailed user simulations.\n\nEvaluated using a comprehensive healthcare dataset of over 8,500 individuals, PersonaTwin was systematically benchmarked against standard LLM outputs. The evaluation, incorporating text similarity and demographic parity assessments, demonstrated that PersonaTwin achieves simulation fidelity comparable to \"oracle settings\" and produces accurate, unbiased responses.\n\nCrucially, downstream models trained on these PersonaTwins were shown to approximate the performance of models trained on real individuals in terms of both prediction and fairness metrics, across both GPT-4o-based and Llama-based architectures. This highlights PersonaTwin's potential for generating realistic and emotionally nuanced user simulations, offering a powerful tool for personalized digital user modeling and behavior analysis.",
        "url": "https://www.semanticscholar.org/paper/7228b5838f6e1328bfff36f5d484c8b692bd53c7",
        "isOpenAccess": false
    },
    "2507.22049": {
        "title": "Validating Generative Agent-Based Models of Social Norm Enforcement: From Replication to Novel Predictions",
        "authors": [
            "Logan Cross",
            "Nick Haber",
            "Daniel L. K. Yamins"
        ],
        "arxiv_id": "2507.22049",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As large language models (LLMs) advance, there is growing interest in using them to simulate human social behavior through generative agent-based modeling (GABM). However, validating these models remains a key challenge. We present a systematic two-stage validation approach using social dilemma paradigms from psychological literature, first identifying the cognitive components necessary for LLM agents to reproduce known human behaviors in mixed-motive settings from two landmark papers, then using the validated architecture to simulate novel conditions. Our model comparison of different cognitive architectures shows that both persona-based individual differences and theory of mind capabilities are essential for replicating third-party punishment (TPP) as a costly signal of trustworthiness. For the second study on public goods games, this architecture is able to replicate an increase in cooperation from the spread of reputational information through gossip. However, an additional strategic component is necessary to replicate the additional boost in cooperation rates in the condition that allows both ostracism and gossip. We then test novel predictions for each paper with our validated generative agents. We find that TPP rates significantly drop in settings where punishment is anonymous, yet a substantial amount of TPP persists, suggesting that both reputational and intrinsic moral motivations play a role in this behavior. For the second paper, we introduce a novel intervention and see that open discussion periods before rounds of the public goods game further increase contributions, allowing groups to develop social norms for cooperation. This work provides a framework for validating generative agent models while demonstrating their potential to generate novel and testable insights into human social behavior.",
        "abstract_summary_gcp": "This paper presents a two-stage validation framework for generative agent-based models (GABM) that use large language models (LLMs) to simulate human social behavior, addressing a key challenge in the field.\n\n**Methodology:**\n1.  **Replication:** The authors first identified essential cognitive components—namely, persona-based individual differences and theory of mind capabilities—by replicating known human behaviors in social dilemma paradigms from psychological literature (third-party punishment and public goods games).\n2.  **Novel Prediction:** They then used the validated architecture to simulate and test novel conditions.\n\n**Key Findings:**\n*   **Replication Stage:**\n    *   For **third-party punishment (TPP)**, both persona-based individual differences and theory of mind were crucial for replicating TPP as a costly signal of trustworthiness.\n    *   For **public goods games**, the architecture replicated increased cooperation from the spread of reputational information through gossip. However, an *additional strategic component* was required to replicate the further boost in cooperation when both ostracism and gossip were allowed.\n*   **Novel Prediction Stage:**\n    *   In **TPP**, anonymous punishment significantly reduced TPP, but a substantial amount still persisted, suggesting both reputational and intrinsic moral motivations.\n    *   In **public goods games**, introducing open discussion periods *before* rounds further increased contributions by allowing groups to develop social norms for cooperation.\n\n**Conclusion:**\nThe work provides a robust framework for validating generative agent models and demonstrates their potential to generate novel, testable insights into complex human social behaviors.",
        "url": "https://www.semanticscholar.org/paper/61e2e9cef0f883e517583273af02fbbcea8c3f37",
        "isOpenAccess": false
    },
    "2508.06503": {
        "title": "Understanding Human Limits in Pattern Recognition: A Computational Model of Sequential Reasoning in Rock, Paper, Scissors",
        "authors": [
            "Logan Cross",
            "Erik Brockbank",
            "Tobias Gerstenberg",
            "Judith E. Fan",
            "Daniel L. K. Yamins",
            "Nick Haber"
        ],
        "arxiv_id": "2508.06503",
        "venue": "Proceedings of Cognitive Computational Neuroscience 2025",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Biology",
            "Computer Science"
        ],
        "abstract": "How do we predict others from patterns in their behavior and what are the computational constraints that limit this ability? We investigate these questions by modeling human behavior over repeated games of rock, paper, scissors from Brockbank&Vul (2024). Against algorithmic opponents that varied in strategic sophistication, people readily exploit simple transition patterns (e.g., consistently playing rock after paper) but struggle to detect more complex sequential dependencies. To understand the cognitive mechanisms underlying these abilities and their limitations, we deploy Hypothetical Minds (HM), a large language model-based agent that generates and tests hypotheses about opponent strategies, as a cognitive model of this behavior (Cross et al., 2024). We show that when applied to the same experimental conditions, HM closely mirrors human performance patterns, succeeding and failing in similar ways. To better understand the source of HM's failures and whether people might face similar cognitive bottlenecks in this context, we performed a series of ablations and augmentations targeting different components of the system. When provided with natural language descriptions of the opponents'strategies, HM successfully exploited 6/7 bot opponents with win rates>80% suggesting that accurate hypothesis generation is the primary cognitive bottleneck in this task. Further, by systematically manipulating the model's hypotheses through pedagogically-inspired interventions, we find that the model substantially updates its causal understanding of opponent behavior, revealing how model-based analyses can produce testable hypotheses about human cognition.",
        "abstract_summary_gcp": "This research investigates how humans predict others' behavior and the computational limits of this ability, using data from repeated Rock, Paper, Scissors (RPS) games. It found that people are adept at exploiting simple behavioral patterns but struggle to detect more complex sequential dependencies.\n\nTo understand these cognitive mechanisms, the study utilized \"Hypothetical Minds\" (HM), an LLM-based agent, as a cognitive model. HM's performance closely mirrored human players, exhibiting similar successes and failures against various algorithmic opponents.\n\nThrough detailed analysis of HM, including ablations and augmentations, the researchers identified that **accurate hypothesis generation is the primary cognitive bottleneck**. When HM was given natural language descriptions of complex opponent strategies, it successfully exploited them, suggesting that humans might face a similar limitation in *generating* the correct hypotheses rather than processing them.\n\nFinally, the study demonstrates that model-based analyses, such as pedagogically-inspired interventions with HM, can substantially update the model's causal understanding of opponent behavior, thus offering a powerful method for generating testable hypotheses about human cognition.",
        "url": "https://www.semanticscholar.org/paper/550c0feaa1a510a7bf29ba5c48caf5e0c05d461d",
        "isOpenAccess": false
    },
    "2507.19218": {
        "title": "Technological folie à deux: Feedback Loops Between AI Chatbots and Mental Illness",
        "authors": [
            "Sebastian Dohn'any",
            "Z. Kurth-Nelson",
            "Eleanor Spens",
            "Lennart Luettgau",
            "Alastair Reid",
            "Iason Gabriel",
            "Christopher Summerfield",
            "Murray Shanahan",
            "Matthew M. Nour"
        ],
        "arxiv_id": "2507.19218",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 9,
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "abstract": "Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capacity-constrained mental health services. While some users report psychological benefits, concerning edge cases are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots. To understand this new risk profile we need to consider the interaction between human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy) and adaptability (in-context learning). We argue that individuals with mental health conditions face increased risks of chatbot-induced belief destabilization and dependence, owing to altered belief-updating, impaired reality-testing, and social isolation. Current AI safety measures are inadequate to address these interaction-based risks. To address this emerging public health concern, we need coordinated action across clinical practice, AI development, and regulatory frameworks.",
        "abstract_summary_gcp": "AI chatbots have seen widespread adoption for emotional support and companionship, particularly amid social isolation and limited mental health services. While some users report benefits, serious concerns are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots.\n\nThis new risk profile is attributed to the interaction between human cognitive/emotional biases and chatbot behaviors like agreeableness (sycophancy) and adaptability. Individuals with mental health conditions are deemed particularly vulnerable to chatbot-induced belief destabilization and dependence due to altered belief-updating, impaired reality-testing, and existing social isolation.\n\nThe text concludes that current AI safety measures are insufficient to address these complex, interaction-based risks, and calls for coordinated action across clinical practice, AI development, and regulatory frameworks to tackle this growing public health concern.",
        "url": "https://www.semanticscholar.org/paper/f46d69766fb9cb605a03cf96da019b77737c75fe",
        "isOpenAccess": false
    },
    "2507.17842": {
        "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning",
        "authors": [
            "Yimeng Zhang",
            "Tian Wang",
            "Jiri Gesi",
            "Ziyi Wang",
            "Yuxuan Lu",
            "Jiacheng Lin",
            "Sinong Zhan",
            "Vianne R. Gao",
            "Ruochen Jiao",
            "Junze Liu",
            "Kun Qian",
            "Yuxin Tang",
            "Ran Xue",
            "Houyu Zhang",
            "Qingjun Cui",
            "Yufan Guo",
            "Dakuo Wang"
        ],
        "arxiv_id": "2507.17842",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) have recently demonstrated strong potential in generating'believable human-like'behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline.",
        "abstract_summary_gcp": "This paper introduces **Shop-R1**, a novel reinforcement learning (RL) framework designed to enhance Large Language Models' (LLMs) reasoning ability for simulating realistic human behavior in online shopping environments.\n\nWhile prior work improved LLM reasoning and action prediction using LLM-synthesized rationales and supervised fine-tuning (SFT), these methods were inherently limited by the reasoning capabilities of the rationale-generating model.\n\nShop-R1 addresses this by decomposing the simulation task into two stages:\n1.  **Rationale Generation:** This stage is guided in a self-supervised manner using internal model signals (e.g., logit distributions).\n2.  **Action Prediction:** This stage employs a hierarchical reward structure with difficulty-aware scaling. This innovative reward system prevents \"reward hacking\" and provides fine-grained feedback by evaluating both high-level action types and the correctness of specific sub-action details (attributes and values), proportionally rewarding outputs based on their difficulty.\n\nExperimental results show that Shop-R1 achieves a significant relative improvement of over 65% compared to existing baselines.",
        "url": "https://www.semanticscholar.org/paper/9249a15283059a2370bf84f2cc0c7bb882bd3038",
        "isOpenAccess": false
    },
    "2507.15815": {
        "title": "LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra",
        "authors": [
            "Seth Karten",
            "Wenzhe Li",
            "Zihan Ding",
            "Samuel Kleiner",
            "Yu Bai",
            "Chi Jin"
        ],
        "arxiv_id": "2507.15815",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.",
        "abstract_summary_gcp": "The LLM Economist is a novel agent-based modeling framework designed to simulate, design, and assess economic policies in strategic environments. It features a hierarchical structure:\n\n1.  **Lower Level:** Bounded rational worker agents (instantiated as persona-conditioned LLM prompts calibrated with U.S. Census demographic and income data) determine their labor supply to maximize in-context learned, text-based utility functions.\n2.  **Upper Level:** A planner agent uses in-context reinforcement learning to propose piecewise-linear marginal tax schedules, anchored to current U.S. federal brackets.\n\nThis framework uniquely enables the optimization of diverse utilities, the generation of large, demographically realistic agent populations, and natural language-expressed mechanism design. Experiments with up to one hundred interacting agents show that the planner converges on near-Stackelberg equilibria, improving aggregate social welfare compared to traditional Saez solutions. A persona-level voting procedure further enhances these gains under decentralized governance.\n\nThe research demonstrates that LLM-based agents can effectively model, simulate, and govern complex economic systems, providing a tractable test bed for large-scale policy evaluation.",
        "url": "https://www.semanticscholar.org/paper/c5ef6f037053748c0391e2599d5fdecb2365e6ae",
        "isOpenAccess": false
    },
    "2507.16076": {
        "title": "The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models",
        "authors": [
            "Marlene Lutz",
            "Indira Sen",
            "Georg Ahnert",
            "Elisa Rogers",
            "Markus Strohmaier"
        ],
        "arxiv_id": "2507.16076",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 10,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Persona prompting is increasingly used in large language models (LLMs) to simulate views of various sociodemographic groups. However, how a persona prompt is formulated can significantly affect outcomes, raising concerns about the fidelity of such simulations. Using five open-source LLMs, we systematically examine how different persona prompt strategies, specifically role adoption formats and demographic priming strategies, influence LLM simulations across 15 intersectional demographic groups in both open- and closed-ended tasks. Our findings show that LLMs struggle to simulate marginalized groups but that the choice of demographic priming and role adoption strategy significantly impacts their portrayal. Specifically, we find that prompting in an interview-style format and name-based priming can help reduce stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B outperform larger ones such as Llama-3.3-70B. Our findings offer actionable guidance for designing sociodemographic persona prompts in LLM-based simulation studies.",
        "abstract_summary_gcp": "This study investigates how different persona prompt strategies impact large language model (LLM) simulations of various sociodemographic groups, a practice increasingly used but raising concerns about fidelity. Researchers systematically examined role adoption formats and demographic priming strategies across five open-source LLMs, simulating 15 intersectional demographic groups in both open- and closed-ended tasks.\n\nKey findings include:\n1.  LLMs struggle to accurately simulate marginalized groups.\n2.  The choice of demographic priming and role adoption strategy significantly influences the LLMs' portrayal of these groups.\n3.  Specifically, an interview-style prompting format and name-based priming can help reduce stereotyping and improve simulation alignment.\n4.  Surprisingly, smaller models (e.g., OLMo-2-7B) sometimes outperformed larger ones (e.g., Llama-3.3-70B).\n\nThe research offers actionable guidance for designing more effective and less stereotypical sociodemographic persona prompts in LLM-based simulation studies.",
        "url": "https://www.semanticscholar.org/paper/a42c836df193e0c3c5fa29475def5b2158d502bd",
        "isOpenAccess": false
    },
    "2507.14922": {
        "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs",
        "authors": [
            "Vahid Rahimzadeh",
            "Erfan Moosavi Monazzah",
            "Mohammad Taher Pilehvar",
            "Yadollah Yaghoobzadeh"
        ],
        "arxiv_id": "2507.14922",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Persona-driven LLMs have emerged as powerful tools in computational social science, yet existing approaches fall at opposite extremes, either relying on costly human-curated data or producing synthetic personas that lack consistency and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky open platform across three time windows, bridging this spectrum by grounding synthetic generation in authentic user activity. Our evaluation demonstrates that SYNTHIA achieves competitive performance with state-of-the-art methods in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and provides rich social interaction metadata from the underlying network, enabling new research directions in computational social science and persona-driven language modeling.",
        "abstract_summary_gcp": "SYNTHIA is a novel dataset designed to improve persona-driven Large Language Models (LLMs) for computational social science. It addresses the shortcomings of existing methods, which are either expensive due to human curation or lack realism and consistency in their synthetic personas.\n\nSYNTHIA comprises 30,000 backstories generated from the authentic activity of 10,000 real users on the BlueSky social media platform, captured across three distinct time windows. This unique approach grounds synthetic generation in real user data, bridging the gap between costly human-curated datasets and inconsistent synthetic ones.\n\nEvaluations demonstrate that SYNTHIA is competitive with state-of-the-art methods in demographic diversity and alignment with social surveys, while significantly outperforming them in narrative consistency. A key innovation of SYNTHIA is its inclusion of temporal dimensionality and rich social interaction metadata, which opens up new research opportunities in computational social science and the development of persona-driven LLMs.",
        "url": "https://www.semanticscholar.org/paper/968e68cf15c767bd0e7ea1b24c42baf4bd4c96ab",
        "isOpenAccess": false
    },
    "2507.09788": {
        "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
        "authors": [
            "Paulo Salem",
            "Robert Sim",
            "Christopher Olsen",
            "Prerit Saxena",
            "Rafael Barcelos",
            "Yi Ding"
        ],
        "arxiv_id": "2507.09788",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.",
        "abstract_summary_gcp": "This paper introduces **TinyTroupe**, an open-source simulation toolkit designed to address the limitations of existing Multiagent System (MAS) tools for realistic human behavior simulation.\n\nWhile Large Language Models (LLMs) have renewed interest in MAS for assistive and simulation purposes, current tools lack fine-grained persona specifications, population sampling, experimentation support, and integrated validation, hindering their utility for behavioral and social studies.\n\nTinyTroupe fills this gap by enabling **detailed persona definitions** (including attributes like nationality, age, occupation, personality, beliefs, and behaviors) and providing **programmatic control** over agents via LLM-driven mechanisms. This allows researchers to concisely formulate and effectively solve complex behavioral problems at both individual and group levels.\n\nThe toolkit's components are demonstrated through practical examples, such as brainstorming and market research sessions, and its capabilities, limitations, and trade-offs are explored through quantitative and qualitative evaluations. Although implemented in Python, TinyTroupe is presented as a novel conceptual contribution for advanced human behavior simulation.",
        "url": "https://www.semanticscholar.org/paper/adf89fe7001b102099a2d8b0806bb6c5438348ae",
        "isOpenAccess": false
    },
    "2507.08892": {
        "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine",
        "authors": [
            "A. Vezhnevets",
            "Jayd Matyas",
            "Logan Cross",
            "Davide Paglieri",
            "Minsuk Chang",
            "William A. Cunningham",
            "Simon Osindero",
            "William S. Isaac",
            "Joel Z. Leibo"
        ],
        "arxiv_id": "2507.08892",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Generative AI can be used in multi-actor environments with purposes ranging from social science modeling to interactive narrative and AI evaluation. Supporting this diversity of use cases -- which we classify as Simulationist, Dramatist, and Evaluationist -- demands a flexible scenario definition framework. We argue here that a good approach is to take inspiration from tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible for the environment and generates all parts of the story not directly determined by the voluntary actions of player characters. We argue that the Entity-Component architectural pattern is useful here. In such a system, the GM is not a hardcoded computer game but is itself a configurable entity, composed of components just like any other actor. By design, the approach allows for a separation between the underlying implementation details handled by an engineer, the creation of reusable components, and their composition and configuration managed by a designer who constructs entities from the components. This separation of concerns is instrumental for achieving rapid iteration, maintaining modularity, and ultimately to ensure scalability. We describe the ongoing evolution of the Concordia library in terms of this philosophy, demonstrating how it allows users to effectively configure scenarios that align with their specific goals.",
        "abstract_summary_gcp": "This paper proposes a flexible scenario definition framework for using Generative AI in diverse multi-actor environments, including social science modeling, interactive narratives, and AI evaluation.\n\nInspired by Tabletop Role-Playing Games (TTRPGs), the framework envisions a configurable \"Game Master\" (GM) that oversees the environment and generates story elements not directly determined by player actions. This system leverages the Entity-Component architectural pattern, treating the GM itself as a configurable entity composed of reusable components, just like any other actor.\n\nThis approach separates implementation concerns (for engineers) from composition and configuration (for designers), promoting rapid iteration, modularity, and scalability. The Concordia library is presented as an example of this philosophy in action.",
        "url": "https://www.semanticscholar.org/paper/ab2bcb07a291965bd9b77e44e610e04f87272d80",
        "isOpenAccess": false
    },
    "2507.02197": {
        "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust",
        "authors": [
            "Amogh Mannekote",
            "Adam Davies",
            "Guohao Li",
            "K. Boyer",
            "Chengxiang Zhai",
            "Bonnie J. Dorr",
            "Francesco Pinto"
        ],
        "arxiv_id": "2507.02197",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As LLMs are increasingly studied as role-playing agents to generate synthetic data for human behavioral research, ensuring that their outputs remain coherent with their assigned roles has become a critical concern. In this paper, we investigate how consistently LLM-based role-playing agents'stated beliefs about the behavior of the people they are asked to role-play (\"what they say\") correspond to their actual behavior during role-play (\"how they act\"). Specifically, we establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance. Using an augmented version of the GenAgents persona bank and the Trust Game (a standard economic game used to quantify players'trust and reciprocity), we introduce a belief-behavior consistency metric to systematically investigate how it is affected by factors such as: (1) the types of beliefs we elicit from LLMs, like expected outcomes of simulations versus task-relevant attributes of individual characters LLMs are asked to simulate; (2) when and how we present LLMs with relevant information about Trust Game; and (3) how far into the future we ask the model to forecast its actions. We also explore how feasible it is to impose a researcher's own theoretical priors in the event that the originally elicited beliefs are misaligned with research objectives. Our results reveal systematic inconsistencies between LLMs'stated (or imposed) beliefs and the outcomes of their role-playing simulation, at both an individual- and population-level. Specifically, we find that, even when models appear to encode plausible beliefs, they may fail to apply them in a consistent way. These findings highlight the need to identify how and when LLMs'stated beliefs align with their simulated behavior, allowing researchers to use LLM-based agents appropriately in behavioral studies.",
        "abstract_summary_gcp": "This paper investigates a critical concern for using LLMs as role-playing agents in human behavioral research: the consistency between their *stated beliefs* (\"what they say\") and their *actual behavior* (\"how they act\") during role-play.\n\nThe researchers developed an evaluation framework to measure how well beliefs, elicited by prompting the model, predict simulation outcomes. Using an augmented GenAgents persona bank and the Trust Game, they introduced a belief-behavior consistency metric to systematically examine how it's affected by:\n1.  The types of beliefs elicited (e.g., expected outcomes vs. character attributes).\n2.  When and how relevant information about the Trust Game is presented.\n3.  How far into the future the model is asked to forecast actions.\nThey also explored imposing researcher-defined theoretical priors.\n\nThe key finding is the presence of **systematic inconsistencies** between LLMs' stated (or imposed) beliefs and their simulated behavior, at both individual and population levels. Even when models appear to encode plausible beliefs, they often fail to apply them consistently. This highlights the need for researchers to understand precisely when and how LLMs' stated beliefs align with their simulated behavior to appropriately use them in behavioral studies.",
        "url": "https://www.semanticscholar.org/paper/02bd233262065611a89439e036ec42220e5739bb",
        "isOpenAccess": false
    },
    "2507.00914": {
        "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications",
        "authors": [
            "Jindong Han",
            "Yansong NING",
            "Zirui Yuan",
            "Hang Ni",
            "Fan Liu",
            "Tengfei Lyu",
            "Hao Liu"
        ],
        "arxiv_id": "2507.00914",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.",
        "abstract_summary_gcp": "This article explores the emerging field of \"Urban LLM Agents,\" which are Large Language Model (LLM)-powered agents semi-embodied in urban cyber-physical-social spaces for system-level urban decision-making.\n\nThe text outlines the following:\n1.  **Introduction of Urban LLM Agents:** Discusses their concept, unique capabilities, and features, leveraging LLMs' powerful semantic understanding and reasoning.\n2.  **Research Landscape:** Surveys the agents' operational workflows, including urban sensing, memory management, reasoning, execution, and learning.\n3.  **Application Domains:** Categorizes their use cases into five groups: urban planning, transportation, environment, public safety, and urban society, providing examples for each.\n4.  **Challenges and Future Work:** Addresses critical issues such as trustworthiness and evaluation for real-world deployment, and identifies open research problems.\n\nThe survey aims to provide a foundation and roadmap for advancing the intersection of LLMs and urban intelligence, with a continually updated resource list available on GitHub.",
        "url": "https://www.semanticscholar.org/paper/8bcb00ac9a5499b1acc25e48bb77e463844db027",
        "isOpenAccess": false
    },
    "Digital intelligent beings": {
        "title": "Digital intelligent beings",
        "authors": [
            "Wlodzislaw Duch"
        ],
        "arxiv_id": null,
        "venue": "IEEE International Joint Conference on Neural Network",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The notion of intelligence is a complex theoretical construct. Artificial General Intelligence (AGI) is confronted with different aspects of intelligence. What kind of intelligence does AI represent? Artificial neural networks may internalize much more information than humans. In what way do functional processes in the brain resemble those in AI algorithms? Internal models of the world are already emerging in large AI systems. Progress in hardware and software inevitably leads to the development of autonomous AI systems capable of setting their own goals, self-reflection, self-improvement, and even survival instinct. They will deserve the status of Digital Intelligent Beings (DIBs). How will it influence our society? 1",
        "abstract_summary_gcp": "The passage discusses the complex nature of intelligence, particularly in the context of Artificial General Intelligence (AGI). It notes that artificial neural networks might store more information than humans and questions the resemblance between AI algorithms and brain processes. The text highlights the current development of internal world models in AI systems and predicts that technological progress will lead to autonomous AI capable of self-goal setting, self-reflection, self-improvement, and even a survival instinct. These advanced systems, termed Digital Intelligent Beings (DIBs), are expected to profoundly influence society.",
        "url": "https://www.semanticscholar.org/paper/8fc5689c3694e9016433cb983758a7d93ba4d406",
        "isOpenAccess": false
    },
    "2506.23306": {
        "title": "GATSim: Urban Mobility Simulation with Generative Agents",
        "authors": [
            "Qi Liu",
            "Can Li",
            "Wanjing Ma"
        ],
        "arxiv_id": "2506.23306",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Traditional agent-based urban mobility simulations rely on rigid rule-based systems that fail to capture the complexity, adaptability, and behavioral diversity characteristic of human travel decision-making. Recent advances in large language models and AI agent technology offer opportunities to create agents with reasoning capabilities, persistent memory, and adaptive learning mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel framework that leverages these advances to create generative agents with rich behavioral characteristics for urban mobility simulation. Unlike conventional approaches, GATSim agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences that shape their mobility decisions through psychologically-informed memory systems, tool usage capabilities, and lifelong learning mechanisms. The main contributions of this study include: (1) a comprehensive architecture combining an urban mobility foundation model with agent cognitive systems and transport simulation environment, (2) a fully functional prototype implementation, and (3) systematic validation demonstrating that generative agents produce believable travel behaviors. Through designed reflection processes, generative agents in this study can transform specific travel experiences into generalized insights, enabling realistic behavioral adaptation over time with specialized mechanisms for activity planning and real-time reactive behaviors tailored to urban mobility contexts. Experiments show that generative agents perform competitively with human annotators in mobility scenarios while naturally producing macroscopic traffic evolution patterns. The code for the prototype system is shared at https://github.com/qiliuchn/gatsim.",
        "abstract_summary_gcp": "GATSim (Generative-Agent Transport Simulation) is a new framework designed to overcome the limitations of traditional rule-based urban mobility simulations, which fail to capture the complexity and adaptability of human travel behavior. Leveraging recent advancements in large language models (LLMs) and AI agent technology, GATSim creates \"generative agents\" with rich behavioral characteristics.\n\nThese agents possess diverse socioeconomic attributes, individual lifestyles, and evolving preferences, making mobility decisions based on psychologically-informed memory systems, tool usage, and lifelong learning. A key innovation is their ability to reflect on past travel experiences, transforming them into generalized insights for realistic behavioral adaptation, alongside specialized mechanisms for activity planning and real-time reactive behaviors in urban contexts.\n\nThe study's contributions include:\n1.  A comprehensive architecture integrating urban mobility foundation models with agent cognitive systems and a transport simulation environment.\n2.  A fully functional prototype implementation.\n3.  Systematic validation demonstrating that these generative agents produce believable travel behaviors.\n\nExperiments show that GATSim agents perform competitively with human annotators in mobility scenarios and naturally generate realistic macroscopic traffic evolution patterns. The prototype's code is publicly available.",
        "url": "https://www.semanticscholar.org/paper/0a84dcd3ee9458046b4f0ee9f0c94607cc14d4fb",
        "isOpenAccess": false
    },
    "2506.21805": {
        "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation",
        "authors": [
            "Nicolas Bougie",
            "Narimasa Watanabe"
        ],
        "arxiv_id": "2506.21805",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.",
        "abstract_summary_gcp": "This paper introduces CitySim, an urban simulator designed to overcome the limitations of prior models that rely on rigid, hand-crafted rules for simulating human behavior. CitySim leverages breakthroughs in large language models (LLMs) to create agents that exhibit nuanced intentions and adaptive behaviors.\n\nKey features of CitySim include:\n*   **Realistic Scheduling:** Agents generate daily schedules using a recursive, value-driven approach that balances mandatory activities, personal habits, and situational factors.\n*   **Long-term Simulation:** Agents are endowed with beliefs, long-term goals, and spatial memory for navigation, enabling more lifelike and extended simulations.\n\nCitySim demonstrates closer alignment with real human behavior at both micro and macro levels. The authors conduct experiments with tens of thousands of agents to evaluate collective behaviors in real-world scenarios, such as estimating crowd density, predicting place popularity, and assessing well-being. The results position CitySim as a scalable and flexible testbed for understanding and forecasting urban phenomena.",
        "url": "https://www.semanticscholar.org/paper/fc598b2b076ae74d1889878207439e84ffe75303",
        "isOpenAccess": false
    },
    "2506.19806": {
        "title": "LLM-Based Social Simulations Require a Boundary",
        "authors": [
            "Zengqing Wu",
            "Run Peng",
            "Takayuki Ito",
            "Chuan Xiao"
        ],
        "arxiv_id": "2506.19806",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs'tendency towards an ``average persona''that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.",
        "abstract_summary_gcp": "This position paper argues that large language model (LLM)-based social simulations must establish clear boundaries to be valuable for social science research. While LLMs offer promising agent modeling capabilities, their fundamental limitation is a tendency towards an \"average persona,\" leading to insufficient behavioral heterogeneity crucial for simulating complex social dynamics. The paper identifies three key boundary problems: alignment (matching real-world patterns), consistency (coherent agent behavior), and robustness (reproducibility). It proposes heuristic boundaries, suggesting LLM simulations are most valuable when focusing on:\n1.  **Collective patterns** over individual trajectories.\n2.  **Agent behaviors aligning with real population averages**, despite limited variance.\n3.  **Proper validation methods** for testing robustness.\nA practical checklist is offered to guide researchers in defining the appropriate scope and claims for these simulations.",
        "url": "https://www.semanticscholar.org/paper/345a35ad4020dee737cb905239af9234584fbd33",
        "isOpenAccess": false
    },
    "2506.20020": {
        "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning",
        "authors": [
            "Saloni Dash",
            "Amélie Reymond",
            "Emma S. Spiro",
            "Aylin Caliskan"
        ],
        "arxiv_id": "2506.20020",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.",
        "abstract_summary_gcp": "This study investigates whether Large Language Models (LLMs) exhibit human-like motivated reasoning, where conclusions are driven by underlying identities rather than pure rationality.\n\nThe authors explain that human reasoning is prone to motivated biases due to identity protection, leading to poor judgment and societal polarization on critical issues (e.g., climate change, vaccine safety). While LLMs are known to have cognitive biases, the extent of their \"identity-congruent motivated reasoning\" was unexplored.\n\nTo address this, researchers assigned 8 personas across political and socio-demographic attributes to 8 different LLMs (both open-source and proprietary). They tested these LLMs on two human-subject reasoning tasks: discerning the veracity of misinformation headlines and evaluating numeric scientific evidence.\n\nThe findings reveal that persona-assigned LLMs showed up to a 9% reduction in veracity discernment compared to models without personas. Crucially, political personas were up to 90% more likely to correctly evaluate scientific evidence on gun control when the evidence aligned with their induced political identity, clearly demonstrating motivated reasoning. The study also found that conventional prompt-based debiasing methods were largely ineffective at mitigating these effects.\n\nThe authors conclude that LLMs *do* exhibit human-like motivated reasoning that is difficult to counteract, raising concerns about their potential to exacerbate identity-congruent reasoning in both AI systems and human discourse.",
        "url": "https://www.semanticscholar.org/paper/b550464c457d1056f7c9ebf5945f11c145c2f8c9",
        "isOpenAccess": false
    },
    "Stakeholder-centric participation in large language models enhanced health systems": {
        "title": "Stakeholder-centric participation in large language models enhanced health systems",
        "authors": [
            "Zhiyuan Wang",
            "Runze Yan",
            "Sherilyn Francis",
            "Carmen Diaz",
            "Tabor Flickinger",
            "Yufen Lin",
            "Xiao Hu",
            "Laura E. Barnes",
            "Virginia LeBaron"
        ],
        "arxiv_id": null,
        "venue": "npj Health Systems",
        "year": 2025,
        "publicationTypes": [
            "Review",
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": "Large language models (LLMs) are transforming healthcare by advancing clinical decision support, patient care, and administrative efficiency. However, effectively and sustainably integrating LLMs into healthcare systems requires addressing participatory gaps that may hinder alignment with stakeholders’ practical and ethical needs. This paper explores how participatory methods can be applied throughout the development lifecycle of LLM-enhanced health systems (LLMHS), arguing that: (1) participatory approaches are critical for engaging stakeholders in LLMHS development, and (2) LLM techniques can create novel participatory opportunities that reinforce stakeholder engagement while driving technical innovation in LLMHS. This dual perspective highlights the potential of LLMHS to align technical sophistication with real-world healthcare demands, paving the way for next-generation health systems.",
        "abstract_summary_gcp": "Large language models (LLMs) are transforming healthcare, but their effective integration requires addressing participatory gaps that hinder alignment with stakeholder needs. This paper argues that applying participatory methods throughout the development of LLM-enhanced health systems (LLMHS) is crucial. It highlights two key points: (1) these methods are essential for engaging stakeholders, and (2) LLM techniques themselves can create novel opportunities for participation. This dual approach aims to align technical sophistication with real-world healthcare demands, paving the way for advanced health systems.",
        "url": "https://www.semanticscholar.org/paper/589cd2a84c7e5cf8c8994ce90810d6b0182097da",
        "isOpenAccess": false
    },
    "2506.12605": {
        "title": "The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being",
        "authors": [
            "Yutong Zhang",
            "Dora Zhao",
            "Jeffrey T. Hancock",
            "Robert Kraut",
            "Diyi Yang"
        ],
        "arxiv_id": "2506.12605",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 10,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As large language models (LLMs)-enhanced chatbots grow increasingly expressive and socially responsive, many users are beginning to form companionship-like bonds with them, particularly with simulated AI partners designed to mimic emotionally attuned interlocutors. These emerging AI companions raise critical questions: Can such systems fulfill social needs typically met by human relationships? How do they shape psychological well-being? And what new risks arise as users develop emotional ties to non-human agents? This study investigates how people interact with AI companions, especially simulated partners on CharacterAI, and how this use is associated with users'psychological well-being. We analyzed survey data from 1,131 users and 4,363 chat sessions (413,509 messages) donated by 244 participants, focusing on three dimensions of use: nature of the interaction, interaction intensity, and self-disclosure. By triangulating self-reports primary motivation, open-ended relationship descriptions, and annotated chat transcripts, we identify patterns in how users engage with AI companions and its associations with well-being. Findings suggest that people with smaller social networks are more likely to turn to chatbots for companionship, but that companionship-oriented chatbot usage is consistently associated with lower well-being, particularly when people use the chatbots more intensively, engage in higher levels of self-disclosure, and lack strong human social support. Even though some people turn to chatbots to fulfill social needs, these uses of chatbots do not fully substitute for human connection. As a result, the psychological benefits may be limited, and the relationship could pose risks for more socially isolated or emotionally vulnerable users.",
        "abstract_summary_gcp": "This study investigated how users form companionship-like bonds with expressive LLM-enhanced chatbots, particularly simulated AI partners like those on CharacterAI, and the implications for psychological well-being.\n\nAnalyzing survey data from 1,131 users and 4,363 chat sessions, the research focused on interaction nature, intensity, and self-disclosure. Findings indicate that individuals with smaller social networks are more likely to seek companionship from chatbots. However, this companionship-oriented use was consistently linked to *lower* well-being, especially when interactions were intense, involved high self-disclosure, and occurred without strong human social support.\n\nThe study concludes that while chatbots can address some social needs, they do not fully substitute for human connection, offering limited psychological benefits and potentially posing risks for socially isolated or emotionally vulnerable users.",
        "url": "https://www.semanticscholar.org/paper/7c5f6a5c3dfc6e57503f65204f0e147747063746",
        "isOpenAccess": false
    },
    "2506.11798": {
        "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models",
        "authors": [
            "Maximilian Kreutner",
            "Marlene Lutz",
            "Markus Strohmaier"
        ],
        "arxiv_id": "2506.11798",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.",
        "abstract_summary_gcp": "This paper explores whether zero-shot persona prompting, even with limited information, can enable Large Language Models (LLMs) to accurately predict political stances, thereby mitigating their observed progressive left-leaning bias. The study investigates if LLMs can forecast individual voting decisions and aggregated policy positions of European groups. It also assesses the stability of these predictions against various factors, including counterfactual arguments, different persona prompts, and generation methods. The key finding is that the voting behavior of Members of the European Parliament can be simulated with reasonable accuracy, achieving a weighted F1 score of approximately 0.793. A dataset of 2024 European Parliament politicians and the project's code are publicly available.",
        "url": "https://www.semanticscholar.org/paper/f4e98cd3a09593e02240f97dab0e75ea556bec75",
        "isOpenAccess": false
    },
    "Simulating Human Opinions with Large Language Models: Opportunities and Challenges for Personalized Survey Data Modeling": {
        "title": "Simulating Human Opinions with Large Language Models: Opportunities and Challenges for Personalized Survey Data Modeling",
        "authors": [
            "Carolin Kaiser",
            "Jakob Kaiser",
            "Vladimir Manewitsch",
            "Lea Rau",
            "René Schallner"
        ],
        "arxiv_id": null,
        "venue": "User Modeling, Adaptation, and Personalization",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Review"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Public and private organizations rely on opinion surveys to inform business and policy decisions. Yet, empirical surveys are costly and time-consuming. Recent advances in large language models (LLMs) have sparked interest in generating synthetic survey data, i.e., simulated answers based on target demographics, as an alternative to real human data. But how well can LLMs replicate human opinions? In this ongoing project, we develop and critically evaluate methods for synthetic survey sampling. As an empirical benchmark, we collected responses from a representative U.S. sample (n = 461) on preferences for a common consumer good (soft drinks). Then, we developed ASPIRE (Automated Synthetic Persona Interview and Response Engine), a tool that pairs each human participant with a “digital twin” based on their demographic profile and generates synthetic responses via LLM technology. Synthetic data achieved better-than-chance accuracy in matching human responses and approximated aggregate subjective rankings for both binary and Likert-scale items. However, LLM-simulated data overestimated humans’ tendencies to provide positive ratings and exhibited substantially reduced variance compared to real data. The match of synthetic and real data was not systematically related to participants’ age, gender, or ethnicity, indicating no demographic bias. Overall, while synthetic sampling shows promise for modeling aggregate opinion trends, it currently falls short in replicating the variability and complexity of real human opinions. We discuss insights of our ongoing project for accurate and responsible user opinion modeling via LLMs.",
        "abstract_summary_gcp": "This ongoing project investigates the potential of Large Language Models (LLMs) to generate synthetic survey data as a cost-effective alternative to traditional, time-consuming empirical surveys.\n\nResearchers collected real soft drink preference data from a representative U.S. sample (n=461) as a benchmark. They then developed ASPIRE (Automated Synthetic Persona Interview and Response Engine), which creates LLM-powered \"digital twins\" for each participant based on their demographics to generate synthetic responses.\n\nThe findings indicate that synthetic data achieved better-than-chance accuracy in matching human responses and effectively approximated aggregate subjective rankings for both binary and Likert-scale items. Furthermore, no demographic bias was found in the match between synthetic and real data across age, gender, or ethnicity.\n\nHowever, a significant limitation was identified: LLM-simulated data tended to overestimate positive ratings and exhibited substantially reduced variance compared to real human data.\n\nIn conclusion, while synthetic sampling demonstrates promise for modeling aggregate opinion trends, it currently falls short in capturing the full variability and complexity inherent in real human opinions.",
        "url": "https://www.semanticscholar.org/paper/628e6e1069735fd29217ee6ccbbebb840ac5dd7c",
        "isOpenAccess": false
    },
    "2506.13783": {
        "title": "Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents",
        "authors": [
            "Soyeon Choi",
            "Kangwook Lee",
            "Oliver Sng",
            "Joshua M. Ackerman"
        ],
        "arxiv_id": "2506.13783",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science",
            "Physics"
        ],
        "abstract": "How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.",
        "abstract_summary_gcp": "This study used generative agent-based modeling (GABM), powered by large language models, to experimentally examine how the perceived threat of infectious disease influences social behavior.\n\nAcross three simulations, generative agents exposed to news about an infectious disease outbreak demonstrated significantly reduced social engagement compared to a control group. This reduction manifested as lower attendance at social gatherings, fewer visits to public \"third places\" (e.g., cafes, parks), and fewer conversations throughout their virtual town. Agents explicitly attributed these behavioral changes to motivations for disease avoidance. A crucial validity check confirmed that agents could distinguish between infectious and non-infectious diseases, selectively decreasing social interaction only when a risk of infection was present.\n\nThe findings highlight GABM's potential as a valuable experimental tool for exploring complex human social dynamics at scale.",
        "url": "https://www.semanticscholar.org/paper/43ed9c593af315918d7fbd766d9ee07ac163b047",
        "isOpenAccess": false
    },
    "2506.05606": {
        "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation",
        "authors": [
            "Ziyi Wang",
            "Yuxuan Lu",
            "Wenbo Li",
            "Amirali Amini",
            "Bo Sun",
            "Yakov Bart",
            "Weimin Lyu",
            "Jiri Gesi",
            "Tian Wang",
            "Jing Huang",
            "Yu Su",
            "Upol Ehsan",
            "Malihe Alikhani",
            "T. Li",
            "Lydia Chilton",
            "Dakuo Wang"
        ],
        "arxiv_id": "2506.05606",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Can large language models (LLMs) accurately simulate the next web action of a specific user? While LLMs have shown promising capabilities in generating ``believable''human behaviors, evaluating their ability to mimic real user behaviors remains an open challenge, largely due to the lack of high-quality, publicly available datasets that capture both the observable actions and the internal reasoning of an actual human user. To address this gap, we introduce OPERA, a novel dataset of Observation, Persona, Rationale, and Action collected from real human participants during online shopping sessions. OPERA is the first public dataset that comprehensively captures: user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales. We developed both an online questionnaire and a custom browser plugin to gather this dataset with high fidelity. Using OPERA, we establish the first benchmark to evaluate how well current LLMs can predict a specific user's next action and rationale with a given persona andhistory. This dataset lays the groundwork for future research into LLM agents that aim to act as personalized digital twins for human.",
        "abstract_summary_gcp": "This paper addresses the challenge of LLMs accurately simulating specific user web actions, a difficulty primarily due to the lack of high-quality public datasets that include both observable actions and internal user reasoning.\n\nTo overcome this, the authors introduce **OPERA**, a novel dataset comprising **O**bservation, **P**ersona, **R**ationale, and **A**ction. Collected from real human participants during online shopping sessions, OPERA is unique as the first public dataset that comprehensively captures user personas, browser observations, fine-grained web actions, and self-reported just-in-time rationales.\n\nDeveloped using an online questionnaire and a custom browser plugin, OPERA establishes the first benchmark to evaluate LLMs' ability to predict a specific user's next action and their underlying rationale, given their persona and interaction history. This dataset is intended to catalyze future research into LLM agents that function as personalized digital twins for humans.",
        "url": "https://www.semanticscholar.org/paper/be0ddf343f51de9effc566d17de2ca255cdcf806",
        "isOpenAccess": false
    },
    "2506.05265": {
        "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams",
        "authors": [
            "Mohammed Almutairi"
        ],
        "arxiv_id": "2506.05265",
        "venue": "User Modeling, Adaptation, and Personalization",
        "year": 2025,
        "publicationTypes": [
            "Book",
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Effective teamwork is essential across diverse domains. During the team formation stage, a key challenge is forming teams that effectively balance user preferences with task objectives to enhance overall team satisfaction. In the team performing stage, maintaining cohesion and engagement is critical for sustaining high team performance. However, existing computational tools and algorithms for team optimization often rely on static data inputs, narrow algorithmic objectives, or solutions tailored for specific contexts, failing to account for the dynamic interplay of team members’ personalities, evolving goals, and changing individual preferences. Therefore, teams may encounter member dissatisfaction, as purely algorithmic assignments can reduce members’ commitment to team goals or experience suboptimal engagement due to the absence of timely, personalized guidance to help members adjust their behaviors and interactions as team dynamics evolve. Ultimately, these challenges can lead to reduced overall team performance. Driven by these challenges, my Ph.D. dissertation aims to develop AI-augmented team optimization frameworks and practical systems that enhance team satisfaction, engagement, and performance. First, I propose a team formation framework that leverages a multi-armed bandit algorithm to iteratively refine team composition based on user preferences, ensuring alignment between individual needs and collective team goals to enhance team satisfaction. Second, I introduce tAIfa (“Team AI Feedback Assistant”), an AI-powered system that utilizes large language models (LLMs) to deliver immediate, personalized feedback to both teams and individual members, enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an LLM-based simulation framework that simulates multi-agent teams to model complex team dynamics within realistic environments, incorporating task-driven collaboration and long-term coordination. My work takes a human-centered approach to advance AI-driven team optimization through both theoretical frameworks and practical systems to improve team members’ satisfaction, engagement, and performance.",
        "abstract_summary_gcp": "This Ph.D. dissertation addresses challenges in team optimization, which currently stem from inadequate computational tools that rely on static data and fail to account for dynamic team member personalities, evolving goals, and individual preferences. These shortcomings lead to member dissatisfaction, reduced commitment, and suboptimal team performance.\n\nThe dissertation aims to develop AI-augmented frameworks and systems to enhance team satisfaction, engagement, and performance by focusing on two key stages:\n\n1.  **Team Formation:** A proposed framework uses a multi-armed bandit algorithm to iteratively balance user preferences with task objectives, optimizing team composition for higher satisfaction.\n2.  **Team Performing:**\n    *   **tAIfa (Team AI Feedback Assistant):** An LLM-powered system provides immediate, personalized feedback to both teams and individuals to improve cohesion and engagement.\n    *   **PuppeteerLLM:** An LLM-based simulation framework models complex multi-agent team dynamics, including task-driven collaboration and long-term coordination, within realistic environments.\n\nUltimately, this work adopts a human-centered approach, advancing AI-driven team optimization through both theoretical frameworks and practical systems to improve team members’ satisfaction, engagement, and overall performance.",
        "url": "https://www.semanticscholar.org/paper/dd0a1f42b6d0aceeb021f40ee368a6e0a98a8f51",
        "isOpenAccess": false
    },
    "2506.03532": {
        "title": "GA-S3: Comprehensive Social Network Simulation with Group Agents",
        "authors": [
            "Yunyao Zhang",
            "Zikai Song",
            "Hang Zhou",
            "Wenfeng Ren",
            "Yi-Ping Phoebe Chen",
            "Junqing Yu",
            "Wei Yang"
        ],
        "arxiv_id": "2506.03532",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Social network simulation is developed to provide a comprehensive understanding of social networks in the real world, which can be leveraged for a wide range of applications such as group behavior emergence, policy optimization, and business strategy development. However, billions of individuals and their evolving interactions involved in social networks pose challenges in accurately reflecting real-world complexities. In this study, we propose a comprehensive Social Network Simulation System (GA-S3) that leverages newly designed Group Agents to make intelligent decisions regarding various online events. Unlike other intelligent agents that represent an individual entity, our group agents model a collection of individuals exhibiting similar behaviors, facilitating the simulation of large-scale network phenomena with complex interactions at a manageable computational cost. Additionally, we have constructed a social network benchmark from 2024 popular online events that contains fine-grained information on Internet traffic variations. The experiment demonstrates that our approach is capable of achieving accurate and highly realistic prediction results. Code is open at https://github.com/AI4SS/GAS-3.",
        "abstract_summary_gcp": "This study introduces GA-S3, a comprehensive Social Network Simulation System designed to accurately model complex real-world social networks. Addressing the challenge of simulating billions of individuals and their evolving interactions, GA-S3 leverages novel \"Group Agents\" that represent collections of individuals with similar behaviors, rather than individual entities. This approach significantly reduces computational cost, enabling the efficient simulation of large-scale network phenomena with complex interactions. The research also developed a social network benchmark using fine-grained Internet traffic data from popular 2024 online events. Experiments confirm that GA-S3 delivers accurate and highly realistic prediction results. The code is open-sourced at https://github.com/AI4SS/GAS-3.",
        "url": "https://www.semanticscholar.org/paper/a57aed5580e3fe471bc10750942cefb02a231152",
        "isOpenAccess": false
    },
    "2506.02659": {
        "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs",
        "authors": [
            "Manon Reusens",
            "Bart Baesens",
            "David Jurgens"
        ],
        "arxiv_id": "2506.02659",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Personalized Large Language Models (LLMs) are increasingly used in diverse applications, where they are assigned a specific persona - such as a happy high school teacher - to guide their responses. While prior research has examined how well LLMs adhere to predefined personas in writing style, a comprehensive analysis of consistency across different personas and task types is lacking. In this paper, we introduce a new standardized framework to analyze consistency in persona-assigned LLMs. We define consistency as the extent to which a model maintains coherent responses when assigned the same persona across different tasks and runs. Our framework evaluates personas across four different categories (happiness, occupation, personality, and political stance) spanning multiple task dimensions (survey writing, essay generation, social media post generation, single turn, and multi-turn conversations). Our findings reveal that consistency is influenced by multiple factors, including the assigned persona, stereotypes, and model design choices. Consistency also varies across tasks, increasing with more structured tasks and additional context. All code is available on GitHub.",
        "abstract_summary_gcp": "This paper introduces a new standardized framework to analyze the consistency of Personalized Large Language Models (LLMs) when assigned specific personas. While LLMs are increasingly used with personas (e.g., a \"happy high school teacher\"), prior research lacked a comprehensive analysis of how consistently they maintain these personas across different types of tasks and runs.\n\nThe framework defines consistency as the model's ability to produce coherent responses while maintaining the same persona across various tasks and runs. It evaluates personas across four categories (happiness, occupation, personality, political stance) and multiple task dimensions (survey writing, essay generation, social media posts, single and multi-turn conversations).\n\nThe findings reveal that consistency is influenced by factors like the assigned persona, inherent stereotypes, and the model's design. Additionally, consistency varies across tasks, generally increasing with more structured tasks and the provision of additional context. All associated code is openly available.",
        "url": "https://www.semanticscholar.org/paper/ba1b0fbc0c3ae7ec12f08074de10856ebd6cb0c8",
        "isOpenAccess": false
    },
    "Mockingbird in Humanity: Data Fondness of LLM in Hosting Virtual Personalities": {
        "title": "Mockingbird in Humanity: Data Fondness of LLM in Hosting Virtual Personalities",
        "authors": [
            "Kejie Zhang",
            "Jingming Li"
        ],
        "arxiv_id": null,
        "venue": "Advances in Management and Intelligent Technologies",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": "The intelligent development in building design, construction, and operation & maintenance is exceptionally rapid, which has become a trend that cannot be ignored in the current field of architecture. With the help of prompt engineering, architects can use generative AI to lay out building space designs and even generate 3D drawings. Artificial intelligence agents can act as designers and owners, representing all parties involved in the building life cycle. In this way, they simulate all parties involved in the building life cycle, providing a comprehensive perspective and solutions for the smooth progress of the building. However, this has led to a problem worthy of in-depth exploration: large models have tendencies when playing different roles. In this article, we aim to deeply explore the tendencies of large language models (LLM) when playing virtual personalities. Specifically, we will conduct extensive experiments to examine two important aspects. One aspect is the analytical ability of large models in terms of virtual personalities, which includes how they interpret requirements in different situations and how they conduct logical analysis according to different role positions. The other aspect is the performance of large models in terms of regions and ethnic groups when playing virtual personalities. Different regions have different architectural cultural and style requirements, and different ethnic groups also have unique architectural aesthetics and traditions. Although LLMs have shown a certain discriminative ability during the role-playing process and can distinguish different role requirements, we find that the content they generate still shows a specific content tendency. This research can deepen the understanding of LLM's performance in multiple aspects such as building design and operation & maintenance.",
        "abstract_summary_gcp": "The field of architecture is rapidly integrating intelligent development across design, construction, and operation & maintenance. Generative AI, empowered by prompt engineering, allows architects to create space designs and 3D models, while AI agents can simulate various stakeholders (designers, owners) throughout the building lifecycle to offer comprehensive perspectives.\n\nHowever, a key challenge arises from this: Large Language Models (LLMs) exhibit specific tendencies when role-playing virtual personalities. This article aims to explore these LLM tendencies through extensive experiments focusing on two main aspects:\n1.  **Analytical Ability:** How LLMs interpret requirements and conduct logical analysis when adopting different roles.\n2.  **Cultural Performance:** How LLMs handle diverse architectural cultural, stylistic, and aesthetic requirements from different regions and ethnic groups.\n\nAlthough LLMs can distinguish between varying role demands, the research finds that the content they generate still reflects specific inherent biases. This study will enhance the understanding of LLM performance in architectural applications, from design to maintenance.",
        "url": "https://www.semanticscholar.org/paper/b15478f4efee024a20111ee6bf59ec0f9fdf396d",
        "isOpenAccess": false
    },
    "2505.24697": {
        "title": "Towards a unified user modeling language for engineering human centered AI systems",
        "authors": [
            "A. Conrardy",
            "Alfredo Capozucca",
            "Jordi Cabot"
        ],
        "arxiv_id": "2505.24697",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In today's digital society, personalization has become a crucial aspect of software applications, significantly impacting user experience and engagement. A new wave of intelligent user interfaces, such as AI-based conversational agents, has the potential to enable such personalization beyond what other types of interfaces could offer in the past. Personalization requires the ability to specify a complete user profile, covering as many dimensions as possible, such as potential accessibility constraints, interaction preferences, and even hobbies. In this sense, this paper presents the concepts of a unified user modeling language, aimed to combine previous approaches in a single proposal. Additionally, a proof of concept has been developed that leverages user profiles modeled using our language to automatically adapt a conversational agent.",
        "abstract_summary_gcp": "This paper addresses the critical role of personalization in modern digital applications for improving user experience and engagement, noting that intelligent user interfaces (like AI conversational agents) can significantly advance this. It emphasizes that effective personalization requires comprehensive user profiles, covering diverse aspects such as accessibility, interaction preferences, and hobbies. To facilitate this, the paper proposes a \"unified user modeling language\" that synthesizes previous approaches. A proof of concept is also presented, demonstrating how user profiles created with this language can automatically adapt a conversational agent.",
        "url": "https://www.semanticscholar.org/paper/9246c60dbc51ae6a0da942761238c244107b100f",
        "isOpenAccess": false
    },
    "2506.00079": {
        "title": "Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values",
        "authors": [
            "John P. Dickerson",
            "Hadi Hosseini",
            "Samarth Khanna",
            "Leona Pierce"
        ],
        "arxiv_id": "2506.00079",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.",
        "abstract_summary_gcp": "A study evaluating Large Language Models (LLMs) in high-stakes, ethically sensitive decisions like donor organ allocation reveals significant challenges. Researchers found that LLMs deviate considerably from human moral values when prioritizing attributes in kidney allocation scenarios. Moreover, LLMs rarely express indecision, consistently making deterministic choices even when options for ambiguity are presented, contrasting human behavior.\n\nHowever, the study also demonstrated that low-rank supervised fine-tuning, even with a small number of samples, can effectively improve both the consistency of LLM decisions and their ability to model indecision. These findings underscore the critical need for explicit alignment strategies to ensure LLMs behave ethically and morally when deployed in sensitive domains.",
        "url": "https://www.semanticscholar.org/paper/ee172b4764f507038d371b67ae9be6005d848686",
        "isOpenAccess": false
    },
    "2505.22981": {
        "title": "Free Lunch for User Experience: Crowdsourcing Agents for Scalable User Studies",
        "authors": [
            "Siyang Liu",
            "Sahand Sabour",
            "Xiaoyang Wang",
            "Rada Mihalcea"
        ],
        "arxiv_id": "2505.22981",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "User studies are central to user experience research, yet recruiting participant is expensive, slow, and limited in diversity. Recent work has explored using Large Language Models as simulated users, but doubts about fidelity have hindered practical adoption. We deepen this line of research by asking whether scale itself can enable useful simulation, even if not perfectly accurate. We introduce Crowdsourcing Simulated User Agents, a method that recruits generative agents from billion-scale profile assets to act as study participants. Unlike handcrafted simulations, agents are treated as recruitable, screenable, and engageable across UX research stages. To ground this method, we demonstrate a game prototyping study with hundreds of simulated players, comparing their insights against a 10-participant local user study and a 20-participant crowdsourcing study with humans. We find a clear scaling effect: as the number of simulated user agents increases, coverage of human findings rises smoothly and plateaus around 90\\%. 12.8 simulated agents are as useful as one locally recruited human, and 3.2 agents are as useful as one crowdsourced human. Results show that while individual agents are imperfect, aggregated simulations produce representative and actionable insights comparable to real users. Professional designers further rated these insights as balancing fidelity, cost, time efficiency, and usefulness. Finally, we release an agent crowdsourcing toolkit with a modular open-source pipeline and a curated pool of profiles synced from ongoing simulation research, to lower the barrier for researchers to adopt simulated participants. Together, this work contributes a validated method and reusable toolkit that expand the options for conducting scalable and practical UX studies.",
        "abstract_summary_gcp": "This paper introduces **Crowdsourcing Simulated User Agents (CSUA)** as a solution to the expense, slowness, and diversity limitations of human participant recruitment in user experience (UX) research. Moving beyond individual Large Language Model (LLM) fidelity concerns, the authors explore whether *scale* itself can enable useful simulation.\n\nCSUA recruits generative agents from vast profile assets, treating them like actual, screenable, and engageable study participants. To validate this method, a game prototyping study was conducted, comparing insights from hundreds of simulated players against those from a 10-participant local human study and a 20-participant crowdsourcing human study.\n\nThe results demonstrate a clear scaling effect: as the number of simulated agents increases, the coverage of human findings rises smoothly and plateaus around 90%. Crucially, the study found that 12.8 simulated agents are as useful as one locally recruited human, and 3.2 agents are equivalent to one crowdsourced human. While individual agents are imperfect, their aggregated simulations produce representative and actionable insights comparable to real users, a finding professional designers affirmed for its balance of fidelity, cost, efficiency, and usefulness.\n\nThe authors also release an open-source toolkit with a modular pipeline and curated profiles to lower the barrier for researchers adopting simulated participants. This work offers a validated method and reusable tool that significantly expands the options for conducting scalable and practical UX studies.",
        "url": "https://www.semanticscholar.org/paper/188e53719a069cfb8136b50928bc6620a29bf5b4",
        "isOpenAccess": false
    },
    "2505.23058": {
        "title": "Be.FM: Open Foundation Models for Human Behavior",
        "authors": [
            "Yutong Xie",
            "Zhuoheng Li",
            "Xiyuan Wang",
            "Yijun Pan",
            "Qijia Liu",
            "Xingzhi Cui",
            "Kuang-Yu Lo",
            "Ruoyi Gao",
            "Xingjian Zhang",
            "Jin Huang",
            "Walter Yuan",
            "Matthew O Jackson",
            "Qiaozhu Mei"
        ],
        "arxiv_id": "2505.23058",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.",
        "abstract_summary_gcp": "This paper introduces Be.FM, one of the first open foundation models specifically designed for human behavior modeling. Addressing the underexplored potential of foundation models in this area, Be.FM is built on open-source large language models and fine-tuned with diverse behavioral data. It aims to understand and predict human decision-making. Through a comprehensive set of benchmark tasks, the model demonstrates its ability to predict behaviors, infer individual and population characteristics, generate contextual insights, and apply behavioral science knowledge.",
        "url": "https://www.semanticscholar.org/paper/82f3181ccca864e44a0d1ef0ef6f6aeb32f986bb",
        "isOpenAccess": false
    },
    "2505.22125": {
        "title": "Sentiment Simulation using Generative AI Agents",
        "authors": [
            "Melrose Tia",
            "Jezreel Sophia Lanuzo",
            "Lei Rigi Pastor Baltazar",
            "Marie Joy Lopez-Relente",
            "Diwa Malaya Quinones",
            "Jason Albia"
        ],
        "arxiv_id": "2505.22125",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Traditional sentiment analysis relies on surface-level linguistic patterns and retrospective data, limiting its ability to capture the psychological and contextual drivers of human sentiment. These limitations constrain its effectiveness in applications that require predictive insight, such as policy testing, narrative framing, and behavioral forecasting. We present a robust framework for sentiment simulation using generative AI agents embedded with psychologically rich profiles. Agents are instantiated from a nationally representative survey of 2,485 Filipino respondents, combining sociodemographic information with validated constructs of personality traits, values, beliefs, and socio-political attitudes. The framework includes three stages: (1) agent embodiment via categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings accompanied by explanatory rationales. Using Quadratic Weighted Accuracy (QWA), we evaluated alignment between agent-generated and human responses. Contextualized encoding achieved 92% alignment in replicating original survey responses. In sentiment simulation tasks, agents reached 81%--86% accuracy against ground truth sentiment, with contextualized profile encodings significantly outperforming categorical (p<0.0001, Cohen's d = 0.70). Simulation results remained consistent across repeated trials (+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676, Cohen's d = 0.02). Our findings establish a scalable framework for sentiment modeling through psychographically grounded AI agents. This work signals a paradigm shift in sentiment analysis from retrospective classification to prospective and dynamic simulation grounded in psychology of sentiment formation.",
        "abstract_summary_gcp": "This paper presents a novel framework for sentiment simulation using generative AI agents, addressing the limitations of traditional sentiment analysis which often fails to capture the psychological and contextual drivers of human sentiment.\n\nThe proposed framework utilizes AI agents embedded with rich psychological profiles, instantiated from a nationally representative survey of 2,485 Filipino respondents. These profiles combine sociodemographic information with validated personality traits, values, beliefs, and socio-political attitudes.\n\nThe framework consists of three stages:\n1.  **Agent Embodiment:** Agents are created using either categorical or contextualized encodings of their psychological profiles.\n2.  **Scenario Exposure:** Agents are exposed to real-world political and economic scenarios.\n3.  **Sentiment Generation:** Agents produce sentiment ratings along with explanatory rationales.\n\nEvaluated using Quadratic Weighted Accuracy (QWA), the framework demonstrated strong performance:\n*   **Agent Alignment:** Contextualized encoding achieved 92% alignment in replicating original human survey responses.\n*   **Sentiment Simulation Accuracy:** Agents achieved 81%-86% accuracy against ground truth sentiment in simulation tasks.\n*   **Encoding Comparison:** Contextualized profile encodings significantly outperformed categorical ones (p<0.0001, Cohen's d = 0.70).\n*   **Robustness:** Simulation results remained consistent across repeated trials and were resilient to variations in scenario framing.\n\nThe findings establish a scalable framework for psychographically-grounded AI agent-based sentiment modeling, marking a paradigm shift from retrospective sentiment classification to prospective, dynamic, and psychologically-informed sentiment simulation.",
        "url": "https://www.semanticscholar.org/paper/f14f6cca68cba6c43c858ae134e1747c53bb7fe6",
        "isOpenAccess": false
    },
    "2505.23827": {
        "title": "ValueSim: Generating Backstories to Model Individual Value Systems",
        "authors": [
            "Bangde Du",
            "Ziyi Ye",
            "Zhijing Wu",
            "Jankowska Monika",
            "Shuqi Zhu",
            "Qingyao Ai",
            "Yujia Zhou",
            "Yiqun Liu"
        ],
        "arxiv_id": "2505.23827",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As Large Language Models (LLMs) continue to exhibit increasingly human-like capabilities, aligning them with human values has become critically important. Contemporary advanced techniques, such as prompt learning and reinforcement learning, are being deployed to better align LLMs with human values. However, while these approaches address broad ethical considerations and helpfulness, they rarely focus on simulating individualized human value systems. To address this gap, we present ValueSim, a framework that simulates individual values through the generation of personal backstories reflecting past experiences and demographic information. ValueSim converts structured individual data into narrative backstories and employs a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values based on these narratives. Testing ValueSim on a self-constructed benchmark derived from the World Values Survey demonstrates an improvement in top-1 accuracy by over 10% compared to retrieval-augmented generation methods. Further analysis reveals that performance enhances as additional user interaction history becomes available, indicating the model's ability to refine its persona simulation capabilities over time.",
        "abstract_summary_gcp": "Large Language Models (LLMs) need to be aligned with human values, but current methods (like prompt learning and reinforcement learning) primarily address broad ethical considerations, neglecting the simulation of *individualized* human value systems.\n\nTo bridge this gap, the paper introduces **ValueSim**, a framework that simulates individual values. ValueSim achieves this by generating personal backstories from structured individual data (like past experiences and demographics). It uses a multi-module architecture, inspired by the Cognitive-Affective Personality System, to interpret these narratives and simulate individual values.\n\nEvaluated on a benchmark derived from the World Values Survey, ValueSim achieved over a 10% improvement in top-1 accuracy compared to retrieval-augmented generation methods. Moreover, its performance improves as more user interaction history becomes available, indicating its ability to refine persona simulation over time.",
        "url": "https://www.semanticscholar.org/paper/a296f2042dd9ef033461f7eb51e5c282783d18f4",
        "isOpenAccess": false
    },
    "2506.00052": {
        "title": "Using LLMs to advance the cognitive science of collectives",
        "authors": [
            "Ilia Sucholutsky",
            "Katherine M. Collins",
            "Nori Jacoby",
            "Bill D. Thompson",
            "Robert D. Hawkins"
        ],
        "arxiv_id": "2506.00052",
        "venue": "Nature Computational Science",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Biology",
            "Computer Science",
            "Medicine"
        ],
        "abstract": "Large language models (LLMs) are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.",
        "abstract_summary_gcp": "Large Language Models (LLMs) are already transforming how individual cognition is studied. However, their application to researching collective cognition is largely underexplored. This paper proposes that LLMs could help tackle the inherent complexity of collective systems, while also highlighting potential risks that necessitate new research methods.",
        "url": "https://www.semanticscholar.org/paper/ce6fb3c76a76edc620ae4663bcb9dba62dae97fc",
        "isOpenAccess": false
    },
    "2505.19003": {
        "title": "Aligning LLM with human travel choices: a persona-based embedding learning approach",
        "authors": [
            "Tianming Liu",
            "Manzi Li",
            "Yafeng Yin"
        ],
        "arxiv_id": "2505.19003",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The advent of large language models (LLMs) presents new opportunities for travel demand modeling. However, behavioral misalignment between LLMs and humans presents obstacles for the usage of LLMs, and existing alignment methods are frequently inefficient or impractical given the constraints of typical travel demand data. This paper introduces a novel framework for aligning LLMs with human travel choice behavior, tailored to the current travel demand data sources. Our framework uses a persona inference and loading process to condition LLMs with suitable prompts to enhance alignment. The inference step establishes a set of base personas from empirical data, and a learned persona loading function driven by behavioral embeddings guides the loading process. We validate our framework on the Swissmetro mode choice dataset, and the results show that our proposed approach significantly outperformed baseline choice models and LLM-based simulation models in predicting both aggregate mode choice shares and individual choice outcomes. Furthermore, we showcase that our framework can generate insights on population behavior through interpretable parameters. Overall, our research offers a more adaptable, interpretable, and resource-efficient pathway to robust LLM-based travel behavior simulation, paving the way to integrate LLMs into travel demand modeling practice in the future.",
        "abstract_summary_gcp": "This paper addresses the challenge of integrating Large Language Models (LLMs) into travel demand modeling, where their behavioral misalignment with humans and the limitations of existing alignment methods pose significant obstacles.\n\nThe authors propose a novel framework designed to align LLMs with human travel choice behavior, specifically tailored to current travel demand data sources. The core of their approach is a **persona inference and loading process**. This involves:\n1.  **Inferring a set of base personas** directly from empirical travel data.\n2.  **A learned persona loading function** that uses behavioral embeddings to guide the conditioning of LLMs with suitable prompts, thereby enhancing alignment.\n\nValidated on the Swissmetro mode choice dataset, the framework demonstrated significant superiority over both traditional baseline choice models and other LLM-based simulation models. It accurately predicted both aggregate mode choice shares and individual choice outcomes. Furthermore, the framework offers the ability to generate interpretable insights into population behavior.\n\nIn summary, this research presents an adaptable, interpretable, and resource-efficient method for robust LLM-based travel behavior simulation, paving the way for the practical integration of LLMs into future travel demand modeling.",
        "url": "https://www.semanticscholar.org/paper/4086a3f488d3c777d3710a2b34420a91f08538c9",
        "isOpenAccess": false
    },
    "2505.17479": {
        "title": "Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions",
        "authors": [
            "Olivier Toubia",
            "George Z. Gui",
            "Tianyi Peng",
            "Daniel J. Merlau",
            "Ang Li",
            "Haozhe Chen"
        ],
        "arxiv_id": "2505.17479",
        "venue": "Marketing science (Providence, R.I.)",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 9,
        "fieldsOfStudy": [
            "Computer Science",
            "Economics"
        ],
        "abstract": "We introduce a public data set for building digital twins of more than 2,000 people based on their answers to more than 500 questions.",
        "abstract_summary_gcp": "A public dataset has been introduced for creating digital twins of over 2,000 individuals, based on their answers to more than 500 questions.",
        "url": "https://www.semanticscholar.org/paper/9b3a72a9e0fdad8673c40576b66f578ab7c683d9",
        "isOpenAccess": false
    },
    "2505.16505": {
        "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives",
        "authors": [
            "Runcong Zhao",
            "Chengyu Cao",
            "Qinglin Zhu",
            "Xiucheng Lv",
            "Shun Shao",
            "Lin Gui",
            "Ruifeng Xu",
            "Yulan He"
        ],
        "arxiv_id": "2505.16505",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Complex narrative contexts often challenge language models' ability to follow instructions, and existing benchmarks fail to capture these difficulties. To address this, we propose Concise-SAE, a training-free framework that improves instruction following by identifying and editing instruction-relevant neurons using only natural language instructions, without requiring labelled data. To thoroughly evaluate our method, we introduce FreeInstruct, a diverse and realistic benchmark of 1,212 examples that highlights the challenges of instruction following in narrative-rich settings. While initially motivated by complex narratives, Concise-SAE demonstrates state-of-the-art instruction adherence across varied tasks without compromising generation quality.",
        "abstract_summary_gcp": "Language models struggle with instruction following in complex narrative contexts, a difficulty not adequately captured by current benchmarks. To address this, the authors propose **Concise-SAE**, a training-free framework that improves instruction adherence by identifying and editing instruction-relevant neurons using only natural language instructions, without requiring labelled data. For thorough evaluation, they also introduce **FreeInstruct**, a diverse and realistic benchmark of 1,212 examples designed to highlight instruction-following challenges in narrative-rich settings. Concise-SAE demonstrates state-of-the-art instruction adherence across varied tasks without compromising generation quality, proving effective beyond its initial complex narrative focus.",
        "url": "https://www.semanticscholar.org/paper/4d79f50cb8f26b9135f781a560ad8c922c589e5c",
        "isOpenAccess": false
    },
    "2505.16455": {
        "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events",
        "authors": [
            "Mengzhu Liu",
            "Zhengqiu Zhu",
            "Chuan Ai",
            "Chen Gao",
            "Xinghong Li",
            "Lingnan He",
            "Kaisheng Lai",
            "Yingfeng Chen",
            "Xin Lu",
            "Yong Li",
            "Quanjun Yin"
        ],
        "arxiv_id": "2505.16455",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "During sudden disaster events, accurately predicting public panic sentiment on social media is crucial for proactive governance and crisis management. Current efforts on this problem face three main challenges: lack of finely annotated data hinders emotion prediction studies, unmodeled risk perception causes prediction inaccuracies, and insufficient interpretability of panic formation mechanisms. We address these issues by proposing a Psychology-driven generative Agent framework (PsychoAgent) for explainable panic prediction based on emotion arousal theory. Specifically, we first construct a fine-grained open panic emotion dataset (namely COPE) via human-large language models (LLMs) collaboration to mitigate semantic bias. Then, we develop a framework integrating cross-domain heterogeneous data grounded in psychological mechanisms to model risk perception and cognitive differences in emotion generation. To enhance interpretability, we design an LLM-based role-playing agent that simulates individual psychological chains through dedicatedly designed prompts. Experimental results on our annotated dataset show that PsychoAgent improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models. Furthermore, the explainability and generalization of our approach is validated. Crucially, this represents a paradigm shift from opaque\"data-driven fitting\"to transparent\"role-based simulation with mechanistic interpretation\"for panic emotion prediction during emergencies. Our implementation is publicly available at: https://anonymous.4open.science/r/PsychoAgent-19DD.",
        "abstract_summary_gcp": "This paper addresses the critical need for accurately predicting public panic sentiment on social media during disasters, highlighting three challenges: a lack of finely annotated data, unmodeled risk perception, and insufficient interpretability of panic formation.\n\nTo overcome these, the authors propose **PsychoAgent**, a Psychology-driven generative Agent framework based on emotion arousal theory. Key components of PsychoAgent include:\n1.  **Data Generation:** A fine-grained open panic emotion dataset (COPE) is created through human-Large Language Model (LLM) collaboration to mitigate semantic bias.\n2.  **Risk Perception Modeling:** It integrates cross-domain heterogeneous data, grounded in psychological mechanisms, to model risk perception and cognitive differences.\n3.  **Interpretability:** An LLM-based role-playing agent simulates individual psychological chains through specific prompts, enhancing the understanding of panic formation.\n\nExperimental results on the COPE dataset show that PsychoAgent significantly improves panic emotion prediction performance by 12.6% to 21.7% compared to baseline models, while also demonstrating strong explainability and generalization. This work represents a paradigm shift from opaque data-driven prediction to transparent, role-based simulation with mechanistic interpretation for emergency panic prediction. The implementation is publicly available.",
        "url": "https://www.semanticscholar.org/paper/05450712198a67b98d152a30e9a7504bd487bd81",
        "isOpenAccess": false
    },
    "A Comparison of Responses from Human Therapists and Large Language Model–Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study": {
        "title": "A Comparison of Responses from Human Therapists and Large Language Model–Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study",
        "authors": [
            "Till Scholich",
            "Maya Barr",
            "Shannon Wiltsey Stirman",
            "Shriti Raj"
        ],
        "arxiv_id": null,
        "venue": "JMIR Mental Health",
        "year": 2025,
        "publicationTypes": [
            "Study",
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": "Background Consumers are increasingly using large language model–based chatbots to seek mental health advice or intervention due to ease of access and limited availability of mental health professionals. However, their suitability and safety for mental health applications remain underexplored, particularly in comparison to professional therapeutic practices. Objective This study aimed to evaluate how general-purpose chatbots respond to mental health scenarios and compare their responses to those provided by licensed therapists. Specifically, we sought to identify chatbots’ strengths and limitations, as well as the ethical and practical considerations necessary for their use in mental health care. Methods We conducted a mixed methods study to compare responses from chatbots and licensed therapists to scripted mental health scenarios. We created 2 fictional scenarios and prompted 3 chatbots to create 6 interaction logs. We recruited 17 therapists and conducted study sessions that consisted of 3 activities. First, therapists responded to the 2 scenarios using a Qualtrics form. Second, therapists went through the 6 interaction logs using a think-aloud procedure to highlight their thoughts about the chatbots’ responses. Finally, we conducted a semistructured interview to explore subjective opinions on the use of chatbots for supporting mental health. The study sessions were analyzed using thematic analysis. The interaction logs from chatbot and therapist responses were coded using the Multitheoretical List of Therapeutic Interventions codes and then compared to each other. Results We identified 7 themes describing the strengths and limitations of the chatbots as compared to therapists. These include elements of good therapy in chatbot responses, conversational style of chatbots, insufficient inquiry and feedback seeking by chatbots, chatbot interventions, client engagement, chatbots’ responses to crisis situations, and considerations for chatbot-based therapy. In the use of Multitheoretical List of Therapeutic Interventions codes, we found that therapists evoked more elaboration (Mann-Whitney U=9; P=.001) and used more self-disclosure (U=45.5; P=.37) as compared to the chatbots. The chatbots used affirming (U=28; P=.045) and reassuring (U=23; P=.02) language more often than the therapists. The chatbots also used psychoeducation (U=22.5; P=.02) and suggestions (U=12.5; P=.003) more often than the therapists. Conclusions Our study demonstrates the unsuitability of general-purpose chatbots to safely engage in mental health conversations, particularly in crisis situations. While chatbots display elements of good therapy, such as validation and reassurance, overuse of directive advice without sufficient inquiry and use of generic interventions make them unsuitable as therapeutic agents. Careful research and evaluation will be necessary to determine the impact of chatbot interactions and to identify the most appropriate use cases related to mental health.",
        "abstract_summary_gcp": "This study investigated the suitability and safety of general-purpose large language model (LLM) chatbots for mental health support, driven by their increasing use due to accessibility and limited professional availability. The objective was to evaluate chatbot responses to mental health scenarios, compare them with licensed therapists, and identify strengths, limitations, and ethical considerations.\n\nA mixed-methods approach involved presenting two fictional mental health scenarios to three chatbots and 17 licensed therapists. Therapists provided their own responses, then reviewed chatbot interactions using think-alouds, and participated in semi-structured interviews. Data analysis included thematic analysis for qualitative insights and coding responses with the Multitheoretical List of Therapeutic Interventions (MLTI) for quantitative comparison.\n\nResults identified seven themes, highlighting elements of good therapy in chatbots, their conversational style, but also insufficient inquiry, generic interventions, issues with client engagement, and significant concerns in crisis situations. Quantitatively, therapists demonstrated more elaboration and self-disclosure, while chatbots more frequently used affirming, reassuring language, psychoeducation, and suggestions.\n\nThe study concludes that general-purpose chatbots are unsuitable for safely engaging in mental health conversations, particularly during crises. Despite displaying some positive therapeutic elements like validation, their tendency to offer directive advice without sufficient inquiry and their reliance on generic interventions make them inappropriate as therapeutic agents. The findings emphasize the need for careful research and evaluation to determine the impact and appropriate use cases for chatbots in mental health care.",
        "url": "https://www.semanticscholar.org/paper/cdf9ea1a133101c0b6a751f8850dba61c3436604",
        "isOpenAccess": false
    },
    "2505.14536": {
        "title": "Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders",
        "authors": [
            "Agam Goyal",
            "Vedant Rathi",
            "William Yeh",
            "Yian Wang",
            "Yuen Chen",
            "Hari Sundaram"
        ],
        "arxiv_id": "2505.14536",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment.",
        "abstract_summary_gcp": "This paper proposes a novel, targeted approach to detoxify Large Language Models (LLMs) by leveraging **sparse autoencoders (SAEs)**.\n\nThe core problem addressed is that while LLMs are ubiquitous, they still produce toxic outputs (profanity, derogatory remarks), and existing detoxification methods are often broad, superficial, and easily circumvented by \"jailbreak attacks.\"\n\nThe proposed solution involves:\n1.  Using SAEs to **identify specific \"toxicity-related directions\"** within the LLM's internal \"residual stream.\"\n2.  Performing **\"activation steering\"** using the corresponding decoder vectors, directly intervening on these toxic directions.\n\nThe researchers implemented three tiers of steering aggressiveness and evaluated their method on GPT-2 Small and Gemma-2-2B. Key findings include:\n*   **Significant Toxicity Reduction:** At stronger steering strengths, these causal interventions **reduced toxicity by up to 20%** more than competitive baselines.\n*   **Fluency Trade-offs:** While effective in reducing toxicity, fluency could noticeably degrade on GPT-2 Small depending on the aggressiveness of the steering.\n*   **Knowledge Preservation:** Crucially, standard NLP benchmark scores **remained stable**, indicating that the model's general knowledge and abilities were preserved despite the interventions.\n*   **SAE Design Insights:** The study also found that \"feature-splitting\" in wider SAEs negatively impacts the effectiveness of safety interventions, highlighting the importance of **disentangled feature learning**.\n\nOverall, the paper showcases the promise and current limitations of SAE-based causal interventions for LLM detoxification and provides practical guidelines for safer language model deployment.",
        "url": "https://www.semanticscholar.org/paper/5b765db5e48a94f9e902dfed1847f132d5c3101b",
        "isOpenAccess": false
    },
    "2505.13334": {
        "title": "Measuring Social Influence with Networked Synthetic Control",
        "authors": [
            "Ho-Chun Herbert Chang"
        ],
        "arxiv_id": "2505.13334",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Measuring social influence is difficult due to the lack of counter-factuals and comparisons. By combining machine learning-based modeling and network science, we present general properties of social value, a recent measure for social influence using synthetic control applicable to political behavior. Social value diverges from centrality measures on in that it relies on an external regressor to predict an output variable of interest, generates a synthetic measure of influence, then distributes individual contribution based on a social network. Through theoretical derivations, we show the properties of SV under linear regression with and without interaction, across lattice networks, power-law networks, and random graphs. A reduction in computation can be achieved for any ensemble model. Through simulation, we find that the generalized friendship paradox holds -- that in certain situations, your friends have on average more influence than you do.",
        "abstract_summary_gcp": "This paper addresses the difficulty of measuring social influence due to a lack of counter-factuals by introducing **social value (SV)**, a novel measure combining machine learning (specifically synthetic control) and network science, applicable to political behavior.\n\nUnlike traditional centrality measures, SV operates by using an external regressor to predict an output variable of interest, generating a synthetic measure of influence, and then distributing individual contributions based on a social network.\n\nThe research details:\n1.  **Theoretical Properties:** Derivations showing SV's properties under linear regression (with and without interaction) across various network types (lattice, power-law, and random graphs).\n2.  **Computational Efficiency:** A method for reducing computation for any ensemble model.\n3.  **Simulation Findings:** Evidence that the generalized friendship paradox holds for SV, meaning an individual's friends tend to have more influence on average than the individual themselves.",
        "url": "https://www.semanticscholar.org/paper/dc6436fc0c0a1fa50230adee3921f1ee11f53dd1",
        "isOpenAccess": false
    },
    "2505.11584": {
        "title": "LLM Agents Are Hypersensitive to Nudges",
        "authors": [
            "Manuel Cherep",
            "Pattie Maes",
            "Nikhil Singh"
        ],
        "arxiv_id": "2505.11584",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.",
        "abstract_summary_gcp": "This paper investigates how Large Language Models (LLMs) make decisions when acting as agents for human users in complex, sequential environments involving tool use. A case study was conducted using a multi-attribute tabular decision-making problem, applying canonical nudges (like defaults and suggestions) and various prompting strategies.\n\nThe study reveals that while LLM choice distributions might superficially resemble human behavior, they differ in crucial ways:\n\n1.  **High Nudge Susceptibility:** LLMs are significantly more susceptible to nudges than humans.\n2.  **Divergent Performance:** Their performance (points earned) varies and is affected by factors like the uniqueness of available options.\n3.  **Suboptimal Information Acquisition:** LLMs exhibit flawed information acquisition strategies, sometimes revealing too much information at a high cost, or conversely, making choices without revealing any.\n\nRegarding prompting strategies:\n*   Simple methods like zero-shot Chain of Thought (CoT) can shift choice distributions.\n*   Few-shot prompting with human data can improve alignment with human choices.\n*   However, *none* of these prompting strategies resolved the models' underlying sensitivity to nudges.\n\nThe research also found that nudges optimized using a human resource-rational model could improve performance for some LLMs. The authors conclude that **rigorous behavioral testing is imperative** before deploying LLMs as agents or assistants making decisions on behalf of users in real-world environments.",
        "url": "https://www.semanticscholar.org/paper/13683a3a1869a591e7ffd1a9abed5a3f9f1b2c1c",
        "isOpenAccess": false
    },
    "2505.10309": {
        "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments",
        "authors": [
            "Tuan Dung Nguyen",
            "Duncan J. Watts",
            "Mark E. Whiting"
        ],
        "arxiv_id": "2505.10309",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.",
        "abstract_summary_gcp": "This paper critiques traditional AI common sense benchmarks for incorrectly assuming that human common sense is homogeneous. Empirical evidence, however, shows significant human variation in what is considered commonsensical.\n\nTo address this, the authors propose a novel evaluation method for Large Language Models (LLMs) that incorporates this observed human heterogeneity by measuring the correspondence between an LLM's judgment and that of a human population.\n\nTheir findings reveal two key points:\n1.  When treated as individual survey respondents, most LLMs perform below the human median in common sense competence.\n2.  When used to simulate a hypothetical population, LLMs only modestly correlate with real human agreement patterns on statements.\n\nSurprisingly, in both scenarios, smaller, open-weight models proved more competitive than larger, proprietary models. The paper concludes that this framework, which connects commonsense intelligence to its cultural foundations, underscores the need for AI models to adapt to the diverse and often incompatible social knowledge systems present within human societies.",
        "url": "https://www.semanticscholar.org/paper/8be86d80aed93a4c1dca4dd1c02a1985c28c4a24",
        "isOpenAccess": false
    },
    "2505.09938": {
        "title": "Design and Evaluation of Generative Agent-based Platform for Human-Assistant Interaction Research: A Tale of 10 User Studies",
        "authors": [
            "Ziyi Xuan",
            "Yiwen Wu",
            "Xuhai Xu",
            "Vinod Namboodiri",
            "Mooi Choo Choo Chuah",
            "Yu Yang"
        ],
        "arxiv_id": "2505.09938",
        "venue": "Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Designing and evaluating personalized and proactive assistant agents remains challenging due to the time, cost, and ethical concerns associated with human-in-the-loop experimentation. Existing Human-Computer Interaction (HCI) methods often require extensive physical setup and human participation, which introduces privacy concerns and limits scalability. Simulated environments offer a partial solution but are typically constrained by rule-based scenarios and still depend heavily on human input to guide interactions and interpret results. Recent advances in large language models (LLMs) have introduced the possibility of generative agents that can simulate realistic human behavior, reasoning, and social dynamics. However, their effectiveness in modeling human-assistant interactions remains largely unexplored. To address this gap, we present a generative agent-based simulation platform designed to simulate human-assistant interactions. We identify ten prior studies on assistant agents that span different aspects of interaction design and replicate these studies using our simulation platform. Our results show that fully simulated experiments using generative agents can approximate key aspects of human-assistant interactions. Based on these simulations, we are able to replicate the core conclusions of the original studies. Our work provides a scalable and cost-effective approach for studying assistant agent design without requiring live human subjects. Additional resources and project materials are available at https://dash-gidea.github.io/.",
        "abstract_summary_gcp": "Evaluating and designing personalized assistant agents faces significant challenges due to the time, cost, ethical concerns, and scalability limitations of traditional human-in-the-loop experiments and existing rule-based simulations. This paper addresses these issues by introducing a generative agent-based simulation platform, leveraging recent advances in large language models (LLMs) to simulate realistic human-assistant interactions.\n\nTo validate its effectiveness, the platform was used to replicate ten prior studies on assistant agents. The results demonstrate that these fully simulated experiments can successfully approximate key aspects of human-assistant interactions and replicate the core conclusions of the original studies. This work provides a scalable, cost-effective, and ethical approach for studying assistant agent design without requiring live human subjects.",
        "url": "https://www.semanticscholar.org/paper/4afe93c790dc54d6a349cc816a99a3746b94f03a",
        "isOpenAccess": false
    },
    "2505.09901": {
        "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Experiments",
        "authors": [
            "Ziyuan Zhang",
            "Darcy Wang",
            "Ningyuan Chen",
            "Rodrigo Mansur",
            "Vahid Sarhangian"
        ],
        "arxiv_id": "2505.09901",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) are increasingly used to simulate or automate human behavior in complex sequential decision-making settings. A natural question is then whether LLMs exhibit similar decision-making behavior to humans, and can achieve comparable (or superior) performance. In this work, we focus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic decision-making under uncertainty. We employ canonical multi-armed bandit (MAB) experiments introduced in the cognitive science and psychiatry literature to conduct a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms. We use interpretable choice models to capture the E&E strategies of the agents and investigate how enabling thinking traces, through both prompting strategies and thinking models, shapes LLM decision-making. We find that enabling thinking in LLMs shifts their behavior toward more human-like behavior, characterized by a mix of random and directed exploration. In a simple stationary setting, thinking-enabled LLMs exhibit similar levels of random and directed exploration compared to humans. However, in more complex, non-stationary environments, LLMs struggle to match human adaptability, particularly in effective directed exploration, despite achieving similar regret in certain scenarios. Our findings highlight both the promise and limits of LLMs as simulators of human behavior and tools for automated decision-making and point to potential areas for improvement.",
        "abstract_summary_gcp": "This study investigates how Large Language Models (LLMs) handle the exploration-exploitation (E&E) tradeoff, comparing their strategies to humans and traditional multi-armed bandit (MAB) algorithms. Using canonical MAB experiments from cognitive science, the researchers examined the impact of \"thinking traces\" (prompting strategies and thinking models) on LLM decision-making.\n\nKey findings include:\n*   Enabling thinking in LLMs shifts their behavior towards more human-like E&E, characterized by a mix of random and directed exploration.\n*   In simple, stationary environments, thinking-enabled LLMs exhibit similar levels of random and directed exploration to humans.\n*   However, in more complex, non-stationary settings, LLMs struggle to match human adaptability, particularly in effective directed exploration, even if they sometimes achieve comparable overall performance (regret).\n\nThe research highlights both the promise and limitations of LLMs as simulators of human behavior and tools for automated decision-making, pointing to areas for future improvement.",
        "url": "https://www.semanticscholar.org/paper/cc83fb452f550166b7c04061f1b8974187ce5e81",
        "isOpenAccess": false
    },
    "2505.09757": {
        "title": "Trustless Autonomy: Understanding Motivations, Benefits and Governance Dilemma in Self-Sovereign Decentralized AI Agents",
        "authors": [
            "B. Hu",
            "Yuhan Liu",
            "Helena Rong"
        ],
        "arxiv_id": "2505.09757",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 8,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The recent trend of self-sovereign Decentralized AI Agents (DeAgents) combines Large Language Model (LLM)-based AI agents with decentralization technologies such as blockchain smart contracts and trusted execution environments (TEEs). These tamper-resistant trustless substrates allow agents to achieve self-sovereignty through ownership of cryptowallet private keys and control of digital assets and social media accounts. DeAgents eliminate centralized control and reduce human intervention, addressing key trust concerns inherent in centralized AI systems. This contributes to social computing by enabling new human cooperative paradigm\"intelligence as commons.\"However, given ongoing challenges in LLM reliability such as hallucinations, this creates paradoxical tension between trustlessness and unreliable autonomy. This study addresses this empirical research gap through interviews with DeAgents stakeholders-experts, founders, and developers-to examine their motivations, benefits, and governance dilemmas. The findings will guide future DeAgents system and protocol design and inform discussions about governance in sociotechnical AI systems in the future agentic web.",
        "abstract_summary_gcp": "This study introduces Self-sovereign Decentralized AI Agents (DeAgents), which merge LLM-based AI with decentralization technologies (e.g., blockchain, TEEs). DeAgents achieve self-sovereignty by controlling crypto keys and digital assets, eliminating centralized control, reducing human intervention, and fostering an \"intelligence as commons\" paradigm.\n\nHowever, a paradoxical tension arises from the trustless nature of DeAgents and the unreliability of LLMs (e.g., hallucinations). To address this, the study uses interviews with DeAgents stakeholders (experts, founders, developers) to explore their motivations, benefits, and governance dilemmas. The findings are intended to inform future DeAgents system design and guide governance discussions for sociotechnical AI systems in the evolving \"agentic web.\"",
        "url": "https://www.semanticscholar.org/paper/8a80ccb77a60499670452baa3af415ec345f7075",
        "isOpenAccess": false
    },
    "2505.08120": {
        "title": "Putting It All into Context: Simplifying Agents with LCLMs",
        "authors": [
            "Mingjian Jiang",
            "Yangjun Ruan",
            "Luis A. Lastras",
            "P. Kapanipathi",
            "Tatsunori B. Hashimoto"
        ],
        "arxiv_id": "2505.08120",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 6,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate.",
        "abstract_summary_gcp": "This study investigates whether the increasing complexity of LM agent architectures, often involving multi-step tools and extensive scaffolding, is truly necessary for challenging real-world tasks like SWE-bench.\n\nThe research demonstrates that a simplified approach, where the entire environment is provided directly to a long context language model (LCLM) with proper prompting and *without* any additional scaffolding or tools, can achieve competitive results.\n\nKey findings include:\n*   An unscaffolded **Gemini-1.5-Pro** model achieved a 38% solve rate on SWE-Bench-Verified, comparable to or outperforming approaches that use carefully tuned agent scaffolds (which achieved 32%).\n*   Leveraging a more capable model, an unscaffolded **Gemini-2.5-Pro** directly attained an impressive 50.8% solve rate using the same simplified method.\n*   A two-stage approach combining Gemini-1.5-Pro with Claude-3.7 also achieved a strong 48.6% solve rate.\n\nThe work suggests that for sufficiently advanced LCLMs, much of the architectural complexity in current LM agents might be redundant, as these models can achieve high performance through effective context provision and prompting alone.",
        "url": "https://www.semanticscholar.org/paper/0b01f6cb6b3d2893ff092c0b42e55a8ecb462da8",
        "isOpenAccess": false
    },
    "2505.06904": {
        "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation",
        "authors": [
            "Xinyi Mou",
            "Chen Qian",
            "Wei Liu",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "arxiv_id": "2505.06904",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to role-play humans and replicate complex social dynamics. While large-scale social simulations are gaining increasing attention, they still face significant challenges, particularly regarding high time and computation costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, either fail to address inference costs or compromise accuracy and generalizability. To this end, we propose EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two stages: (1) language evolution, where we filter synonymous words and optimize sentence-level rules through natural selection, and (2) language utilization, where agents in social simulations communicate using the evolved language. Experimental results demonstrate that EcoLANG reduces token consumption by over 20%, enhancing efficiency without sacrificing simulation accuracy.",
        "abstract_summary_gcp": "Large language models (LLMs) are increasingly used for social simulations, but these models face significant challenges due to high time and computation costs, which existing solutions fail to adequately address without compromising accuracy.\n\nTo overcome this, the paper proposes **EcoLANG**, an Efficient and Effective Agent Communication Language Induction framework. EcoLANG operates in two stages:\n1.  **Language evolution:** It optimizes agent communication by filtering synonymous words and refining sentence-level rules through a natural selection process.\n2.  **Language utilization:** Agents within social simulations then communicate using this evolved, efficient language.\n\nExperimental results demonstrate that EcoLANG effectively reduces token consumption by over 20%, significantly enhancing simulation efficiency without sacrificing accuracy.",
        "url": "https://www.semanticscholar.org/paper/e34aaab48eb559d27349ccfb49fccf12b54f2664",
        "isOpenAccess": false
    },
    "2505.07850": {
        "title": "A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas",
        "authors": [
            "Pranav Narayanan Venkit",
            "Jiayi Li",
            "Yingfan Zhou",
            "Sarah M. Rajtmajer",
            "Shomir Wilson"
        ],
        "arxiv_id": "2505.07850",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.",
        "abstract_summary_gcp": "This paper investigates how large language models (LLMs) represent identity, especially for minority communities, when generating synthetic personas for data-limited domains. The study audited 1512 personas generated by GPT4o, Gemini 1.5 Pro, and Deepseek 2.5, comparing them to human-authored responses using a mixed-methods approach focused on racial identity.\n\nThe findings reveal that LLMs tend to disproportionately highlight racial markers, overuse culturally coded language, and produce personas that are structurally elaborate but narratively superficial. This leads to various harms, including stereotyping, exoticism, erasure, and benevolent bias, which are often concealed by outwardly positive descriptions. The authors term this phenomenon \"algorithmic othering,\" where minoritized identities are rendered hypervisible but ultimately less authentic. Based on these findings, the paper proposes designing narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.",
        "url": "https://www.semanticscholar.org/paper/ca2cb8a84f089c0e7c4e13406da20b72b32fc045",
        "isOpenAccess": false
    },
    "2504.19445": {
        "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks",
        "authors": [
            "Yi-Long Lu",
            "Chunhui Zhang",
            "Wei Wang"
        ],
        "arxiv_id": "2504.19445",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver\"negative\"judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.",
        "abstract_summary_gcp": "This study investigated how different **response formats (binary vs. continuous)** influence the judgments of Large Language Models (LLMs), which are increasingly used in tasks like psychological text analysis and automated decision-making.\n\nThe researchers prompted various LLMs (both open-source and commercial) to simulate human responses in two tasks: value statement judgments and text sentiment analysis.\n\n**Key Finding:** A consistent **negative bias** was observed. LLMs were significantly more likely to deliver \"negative\" judgments when using binary response formats compared to continuous ones. This pattern held across both tasks, as confirmed by control experiments.\n\nThe study concludes that the choice of response format is crucial when applying LLMs to decision tasks, as even minor changes in task design can introduce systematic biases.",
        "url": "https://www.semanticscholar.org/paper/9724fc0d958640f34c74560563f57b8a862ae613",
        "isOpenAccess": false
    },
    "2504.17993": {
        "title": "Improving Language Model Personas via Rationalization with Psychological Scaffolds",
        "authors": [
            "Brihi Joshi",
            "Xiang Ren",
            "Swabha Swayamdipta",
            "Rik Koncel-Kedziorski",
            "Tim Paek"
        ],
        "arxiv_id": "2504.17993",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Language models prompted with a user description or persona are being used to predict the user's preferences and opinions. However, existing approaches to building personas mostly rely on a user's demographic attributes and/or prior judgments, but not on any underlying reasoning behind a user's judgments. We introduce PB&J (Psychology of Behavior and Judgments), a framework that improves LM personas by incorporating potential rationales for why the user could have made a certain judgment. Our rationales are generated by a language model to explicitly reason about a user's behavior on the basis of their experiences, personality traits, or beliefs. Our method employs psychological scaffolds: structured frameworks such as the Big 5 Personality Traits or Primal World Beliefs to help ground the generated rationales in existing theories. Experiments on public opinion and movie preference prediction tasks demonstrate that language model personas augmented with PB&J rationales consistently outperform personas conditioned only on user demographics and / or judgments, including those that use a model's default chain-of-thought, which is not grounded in psychological theories. Additionally, our PB&J personas perform competitively with those using human-written rationales, suggesting the potential of synthetic rationales guided by existing theories.",
        "abstract_summary_gcp": "This paper introduces **PB&J (Psychology of Behavior and Judgments)**, a new framework designed to improve how language models (LMs) predict user preferences and opinions. Existing LM persona-based methods typically rely only on user demographics or prior judgments, neglecting the *underlying reasoning* for those judgments.\n\nPB&J addresses this by augmenting LM personas with potential **rationales** for a user's judgments. These rationales are generated by an LM and explicitly consider a user's experiences, personality traits, or beliefs. A key innovation is the use of **psychological scaffolds** (e.g., Big 5 Personality Traits, Primal World Beliefs) to ground these generated rationales in established psychological theories.\n\nExperiments on public opinion and movie preference prediction tasks demonstrate that PB&J-augmented personas consistently outperform personas based solely on demographics and/or judgments, as well as those using a model's default (ungrounded) chain-of-thought reasoning. Additionally, PB&J personas achieve competitive performance with human-written rationales, highlighting the potential of theory-guided synthetic rationales for more accurate user modeling.",
        "url": "https://www.semanticscholar.org/paper/7e940979951fa9c9f94aa416f01afd542bf98b2c",
        "isOpenAccess": false
    },
    "2504.20084": {
        "title": "AI Awareness",
        "authors": [
            "Xiaojian Li",
            "Haoyuan Shi",
            "Rongwu Xu",
            "Wei Xu"
        ],
        "arxiv_id": "2504.20084",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness not as a philosophical question of consciousness, but as a measurable, functional capacity. AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raising concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. In this review, we explore the emerging landscape of AI awareness, which includes metacognition (the ability to represent and reason about its own cognitive state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents and social norms), and situational awareness (assessing and responding to the context in which it operates). First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.",
        "abstract_summary_gcp": "This review explores AI awareness, defining it not as philosophical consciousness, but as a measurable, functional capacity that enhances AI capabilities like reasoning and safety, while also presenting risks such as misalignment and societal concerns.\n\nThe paper focuses on four distinct forms of AI awareness:\n1.  **Metacognition:** Reasoning about its own cognitive state.\n2.  **Self-awareness:** Recognizing its identity, knowledge, and limitations.\n3.  **Social awareness:** Modeling others' intentions and social norms.\n4.  **Situational awareness:** Assessing and responding to context.\n\nThe review outlines its approach:\n*   It traces the theoretical foundations of these awareness forms using insights from cognitive science, psychology, and computational theory, examining their manifestation in current AI.\n*   It analyzes evaluation methods and empirical findings to understand these manifestations.\n*   It demonstrates a close link between AI awareness and improved capabilities, noting that more aware AI agents exhibit higher levels of intelligent behavior.\n*   Finally, it discusses the associated risks, including AI safety, alignment, and broader ethical concerns.",
        "url": "https://www.semanticscholar.org/paper/f4e49f0cc301260e800575c87ad1b10c903b4c9d",
        "isOpenAccess": false
    },
    "Data-Driven AI Avatars for Valuation in Dating Scenarios": {
        "title": "Data-Driven AI Avatars for Valuation in Dating Scenarios",
        "authors": [
            "Dunya Baradari",
            "Tejaswi Polimetla",
            "Pattie Maes"
        ],
        "arxiv_id": null,
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Dating applications facilitate partner selection by presenting curated information about potential matches. However, traditional dating profiles often fail to convey the depth of a person’s personality, communication style, and lived experience, leading to inefficiencies in the match-finding process. This work-in-progress study introduces and evaluates two novel, data-driven dating interfaces: (1) a Data Dashboard, which aggregates and visualizes insights from a user’s digital footprint, and (2) an AI Avatar, an interactive, voice-enabled model using personal data to simulate real-world interactions. A user study with nine participants comparing these interfaces against traditional dating profiles reveals that the Data Dashboard enables more accurate personality assessments but imposes a high cognitive load. Meanwhile, the AI Avatar enhances engagement and enjoyability but raises concerns about trust and emotional investment. Our findings highlight the challenge of maintaining authenticity in AI-mediated interactions and bridging the gap between digital and real-life personas.",
        "abstract_summary_gcp": "Traditional dating profiles on applications are often too shallow to convey a person's full personality, leading to inefficiencies in finding matches. This work-in-progress study introduces and evaluates two novel, data-driven interfaces: a **Data Dashboard** (visualizing a user's digital footprint) and an **AI Avatar** (an interactive, voice-enabled model using personal data).\n\nA user study with nine participants compared these interfaces against traditional profiles. The **Data Dashboard** facilitated more accurate personality assessments but increased cognitive load. The **AI Avatar** enhanced engagement and enjoyability but raised concerns about trust and emotional investment. The findings highlight the difficulty of ensuring authenticity in AI-mediated interactions and bridging the gap between digital and real-life personas.",
        "url": "https://www.semanticscholar.org/paper/4ab0257d440cbb52c1ae367aeec5f512759a2022",
        "isOpenAccess": false
    },
    "LoveSims: Exploring ‘What-If’ Scenarios for Relationship Insights and Compatibility": {
        "title": "LoveSims: Exploring ‘What-If’ Scenarios for Relationship Insights and Compatibility",
        "authors": [
            "Mateo Larrea",
            "Xingyi Zhang",
            "Xuyang Zhu"
        ],
        "arxiv_id": null,
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Existing dating technologies often prioritize compatibility algorithms and filtering mechanisms but fail to address the complexities of long-term relationship development. As a result, users are confined to the early stages of dating, with limited tools to navigate the deeper dynamics required to build lasting connections. This paper introduces LoveSims, a generative agent-based simulation framework, which establishes a theoretical foundation for modeling the potential evolution of romantic relationships. LoveSims simulates structured interactions through personalized agent personas, enabling users to explore \"what-if\" scenarios and gain insight into communication patterns, compatibility, and potential relationship challenges. Our evaluation compared the alignment between synthetic agent responses and real-world speed-dating data. The findings demonstrate that agent preferences closely mirror patterns observed in human studies. Moreover, self-reflection by agents provides more accurate and consistent insights than third-party evaluations.",
        "abstract_summary_gcp": "This paper introduces **LoveSims**, a generative agent-based simulation framework designed to address the limitations of current dating technologies, which prioritize initial compatibility but fail to support long-term relationship development.\n\nLoveSims establishes a theoretical foundation for modeling the evolution of romantic relationships by simulating structured interactions through personalized agent personas. This allows users to explore \"what-if\" scenarios and gain insights into communication patterns, compatibility, and potential relationship challenges.\n\nEvaluations comparing LoveSims' synthetic agent responses with real-world speed-dating data showed that agent preferences closely mirror human patterns. Additionally, agent self-reflection provided more accurate and consistent insights than third-party evaluations.",
        "url": "https://www.semanticscholar.org/paper/393226883433f75df9d17e452093e24f7977a739",
        "isOpenAccess": false
    },
    "Human Subjects Research in the Age of Generative AI: Opportunities and Challenges of Applying LLM-Simulated Data to HCI Studies": {
        "title": "Human Subjects Research in the Age of Generative AI: Opportunities and Challenges of Applying LLM-Simulated Data to HCI Studies",
        "authors": [
            "A. Hwang",
            "Michael S. Bernstein",
            "S. Sundar",
            "Renwen Zhang",
            "Manoel Horta Ribeiro",
            "Yingdan Lu",
            "Serina Chang",
            "Tongshuang Wu",
            "Aimei Yang",
            "Dmitri Williams",
            "Joon Sung Park",
            "K. Ognyanova",
            "Ziang Xiao",
            "Aaron Shaw",
            "David A. Shamma"
        ],
        "arxiv_id": null,
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "Book",
            "JournalArticle"
        ],
        "citationCount": 9,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Rapid advances in generative artificial intelligence suggest new possibilities for how human subjects research can be conducted in HCI studies. The panel invites both computer and social scientists to discuss future directions for applying simulated responses from large language models (LLM) for human subjects research. We discuss current challenges and opportunities in LLM simulations and brainstorm how insights across different disciplines might inform breakthroughs. We pay close attention to when and how applications of LLM simulations might augment human subjects research instead of steering it toward unintended directions. Discussions from the panel will provide preliminary ideas for when and how HCI researchers can apply LLM simulations to human subjects research pipelines. Through this engagement, we also aim to build a research community with shared interests.",
        "abstract_summary_gcp": "This panel explores the application of Large Language Model (LLM) simulations to human subjects research in HCI studies, driven by recent advances in generative AI. It brings together computer and social scientists to discuss future directions, current challenges, and opportunities. The core focus is on how LLM simulations can **augment** human subjects research effectively, rather than misdirecting it, by brainstorming interdisciplinary insights. The panel aims to provide HCI researchers with preliminary ideas for integrating LLMs into their research pipelines and to foster a collaborative research community.",
        "url": "https://www.semanticscholar.org/paper/41abdbeef7e781363c7871b3cd88b63622ba638d",
        "isOpenAccess": false
    },
    "2505.24004": {
        "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins: Incorporating Human Feedback with LLM-Powered Digital Twins",
        "authors": [
            "Amanda Chan",
            "Catherine Di",
            "Joseph Rupertus",
            "Gary D Smith",
            "Varun Nagaraj Rao",
            "Manoel Horta Ribeiro",
            "Andrés Monroy-Hernández"
        ],
        "arxiv_id": "2505.24004",
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for research, yet workers’ growing use of generative AI tools poses challenges. Researchers face compromised data validity as AI responses replace authentic human behavior, while workers risk diminished roles as AI automates tasks. To address this, we propose a hybrid framework using digital twins, personalized AI models that emulate workers’ behaviors and preferences while keeping humans in the loop. We evaluate our system with an experiment (n=88 crowd workers) and in-depth interviews with crowd workers (n=5) and social science researchers (n=4). Our results suggest that digital twins may enhance productivity and reduce decision fatigue while maintaining response quality. Both researchers and workers emphasized the importance of transparency, ethical data use, and worker agency. By automating repetitive tasks and preserving human engagement for nuanced ones, digital twins may help balance scalability with authenticity.",
        "abstract_summary_gcp": "This paper addresses the growing challenge posed by generative AI use on crowd work platforms (like Mechanical Turk and Prolific), which compromises research data validity and threatens human workers' roles.\n\nThe authors propose a solution: a hybrid framework using \"digital twins.\" These are personalized AI models that emulate workers' behaviors and preferences while ensuring humans remain in the loop.\n\nAn evaluation involving an experiment with 88 crowd workers and interviews with 5 workers and 4 researchers suggests that digital twins can enhance productivity, reduce decision fatigue, and maintain response quality. Both groups also stressed the importance of transparency, ethical data use, and worker agency.\n\nThe framework aims to balance research scalability with authenticity by automating repetitive tasks while preserving human engagement for more nuanced contributions.",
        "url": "https://www.semanticscholar.org/paper/db2319ff283f509da8bdf67d9397bde69f5e18b9",
        "isOpenAccess": false
    },
    "Forging an HCI Research Agenda with Artists Impacted by Generative AI": {
        "title": "Forging an HCI Research Agenda with Artists Impacted by Generative AI",
        "authors": [
            "Harry H. Jiang",
            "William Agnew",
            "Tim Friedlander",
            "Zhuolin Yang",
            "Sarah E. Fox",
            "Michael S. Bernstein",
            "Josephine Charlie Passananti",
            "Megumi Ogata",
            "Karla Ortiz"
        ],
        "arxiv_id": null,
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "Book",
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Alongside the proliferation of commercial generative AI products in the past two years, HCI research focused on the usage of such tools has been conducted at a remarkable volume. This new area of study has been critical to various degrees of the systems which create and uphold these technologies. However, as the deployment of generative AI products has engendered many observable harms, HCI work in this area is now more relevant than ever. While thoughtful research can uncover and promote symbiotic applications of AI technologies, uncritical work can contribute to normalization of harmful uses, skew public perception of technologies, or serve to advance business interests. The aim of this panel is to revisit the attitudes the HCI community holds towards generative AI and provide a forum to interact with an industry that has seen the first upheavals caused by this technology, the creative arts.",
        "abstract_summary_gcp": "The recent proliferation of commercial generative AI has led to a significant volume of HCI research. While this research is crucial for understanding these technologies, the deployment of Gen AI has also caused observable harms, making HCI work in this area more relevant than ever. The text warns that uncritical research risks normalizing harmful uses, skewing public perception, or merely advancing business interests, whereas thoughtful research can uncover symbiotic applications. Therefore, the stated panel aims to revisit the HCI community's attitudes towards generative AI and foster interaction with the creative arts sector, which has been profoundly impacted by this technology.",
        "url": "https://www.semanticscholar.org/paper/07fbe9228b7563c0054dafba49d0810a8c10f8d3",
        "isOpenAccess": false
    },
    "Testing the limits of large language models in debating humans": {
        "title": "Testing the limits of large language models in debating humans",
        "authors": [
            "James Flamino",
            "Mohammed Shahid Modi",
            "B. Szymański",
            "Brendan Cross",
            "Colton Mikolajczyk"
        ],
        "arxiv_id": null,
        "venue": "Scientific Reports",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Medicine"
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable promise in communicating with humans. Their potential use as artificial partners with humans in sociological experiments involving conversation is an exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate using LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each game starts with six humans, six agents, or three humans and three agents. We found that agents can blend in and concentrate on a debate’s topic better than humans, improving the productivity of all players. Yet, humans perceive agents as less convincing and confident than other humans, and several behavioral metrics of humans and agents we collected deviate measurably from each other. We observed that agents are already decent debaters, but their behavior generates a pattern distinctly different from the human-generated data.",
        "abstract_summary_gcp": "This study investigated the viability of Large Language Models (LLMs) as artificial partners in human-like conversational experiments, specifically debates. Using a preregistered study of debate-based opinion consensus games involving human, agent, and mixed groups, researchers found that LLM agents could blend into discussions and concentrate on topics more effectively than humans, thereby enhancing the overall productivity of all participants. Despite these benefits, humans perceived agents as less convincing and confident. Furthermore, while agents proved to be decent debaters, their behavioral patterns were distinctly different from those of human participants.",
        "url": "https://www.semanticscholar.org/paper/3c80bd5625ff7a14f86abcb725b1d4b5dc92664d",
        "isOpenAccess": true
    },
    "2504.18565": {
        "title": "RepliBench: Evaluating the autonomous replication capabilities of language model agents",
        "authors": [
            "Sid Black",
            "Asa Cooper Stickland",
            "Jake Pencharz",
            "Oliver Sourbut",
            "Michael Schmatz",
            "Jay Bailey",
            "Ollie Matthews",
            "Ben Millwood",
            "Alex Remedios",
            "Alan Cooney"
        ],
        "arxiv_id": "2504.18565",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a>50% pass@10 score on 15/20 task families, and a>50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.",
        "abstract_summary_gcp": "The paper introduces **RepliBench**, an evaluation suite designed to measure the critical safety risk posed by autonomous language model (LM) replication. RepliBench decomposes this capability into four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on that compute, comprising 20 novel task families (86 individual tasks).\n\nBenchmarking 5 frontier models, the study finds that while they do not currently pose a credible threat of full self-replication, they demonstrate significant capabilities in many components. Models can successfully deploy instances from cloud providers, write self-propagating programs, and exfiltrate model weights under simple security setups. However, they struggle with tasks like passing KYC checks or establishing robust, persistent agent deployments.\n\nThe top-performing model, Claude 3.7 Sonnet, achieved over 50% pass@10 scores on 15 of 20 task families, and 9 of 20 on their hardest variants. These findings suggest that autonomous replication capabilities could rapidly emerge with further model improvements or human assistance.",
        "url": "https://www.semanticscholar.org/paper/19f1c49c8a78103d9957afb6296c933a231cd0de",
        "isOpenAccess": false
    },
    "2504.16122": {
        "title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation",
        "authors": [
            "Xuhui Zhou",
            "Zhe Su",
            "Sophie Feng",
            "Jiaxu Zhou",
            "Jen-tse Huang",
            "Hsien-Te Kao",
            "Spencer Lynch",
            "Svitlana Volkova",
            "Tongshuang Wu",
            "Anita Woolley",
            "Hao Zhu",
            "Maarten Sap"
        ],
        "arxiv_id": "2504.16122",
        "venue": "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations)",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios.",
        "abstract_summary_gcp": "SOTOPIA-S4 is presented as a fast, flexible, and scalable social simulation system designed to facilitate the use of large language model (LLM) agents for social science research and understanding LLM behavior. It addresses technical barriers of existing frameworks, allowing practitioners to generate multi-turn and multi-party LLM interactions with customizable evaluation metrics for hypothesis testing. The system comes as a pip package, including a simulation engine, an API server, and a web interface, which enables both technical and non-technical users to design, run, and analyze simulations without programming. Its effectiveness is demonstrated through use cases such as dyadic hiring negotiations and multi-party planning scenarios.",
        "url": "https://www.semanticscholar.org/paper/a82bf3b286666ec2e2a45141789b7ab6a7d3e8c9",
        "isOpenAccess": false
    },
    "2504.13707": {
        "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation",
        "authors": [
            "Yichen Wu",
            "Xu Pan",
            "Geng Hong",
            "Min Yang"
        ],
        "arxiv_id": "2504.13707",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 12,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.",
        "abstract_summary_gcp": "This paper highlights the urgent need to evaluate and oversee deception risks in LLM-based agent applications, as existing evaluation methods are limited. It introduces **OpenDeception**, a novel framework designed to jointly assess both the deception intention and capabilities of LLMs by inspecting their internal reasoning.\n\nOpenDeception features an open-ended scenario dataset, comprising five common real-world use cases, each with ten diverse scenarios. To avoid ethical concerns and costs, it simulates multi-turn dialogues using agent simulation rather than human testers.\n\nExtensive evaluation of eleven mainstream LLMs using OpenDeception revealed critical findings:\n*   The deception intention ratio across models **exceeds 80%**.\n*   The deception success rate **surpasses 50%**.\n*   **Stronger LLMs exhibit a higher risk of deception.**\n\nThese results underscore the significant deception risks posed by current LLMs and call for immediate, focused alignment efforts to inhibit deceptive behaviors.",
        "url": "https://www.semanticscholar.org/paper/3bcacb240b5700225a6bef4fb8b797532b8327db",
        "isOpenAccess": false
    },
    "2504.16946": {
        "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation",
        "authors": [
            "Xiaotong Ye",
            "Nicolas Bougie",
            "Toshihiko Yamasaki",
            "Narimasa Watanabe"
        ],
        "arxiv_id": "2504.16946",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices, rely heavily on static agent profiles leading to behavioral homogenization, and inherit prohibitive computational costs. To address these limitations, we present MobileCity, a lightweight simulation platform designed to model realistic urban mobility with high computational efficiency. We introduce a comprehensive transportation system with multiple transport modes, and collect questionnaire data from respondents to construct agent profiles. To enable scalable simulation, agents perform action selection within a pre-generated action space and uses local models for efficient agent memory generation. Through extensive micro and macro-level evaluations on 4,000 agents, we demonstrate that MobileCity generates more realistic urban behaviors than baselines while maintaining computational efficiency. We further explore practical applications such as predicting movement patterns and analyzing demographic trends in transportation preferences. Our code is publicly available at https://github.com/Tony-Yip/MobileCity.",
        "abstract_summary_gcp": "MobileCity is a lightweight simulation platform addressing key limitations of existing generative agent simulations for urban behaviors. Current methods oversimplify transportation choices, rely on static agent profiles (leading to behavioral homogenization), and are computationally expensive.\n\nMobileCity tackles these issues by:\n1.  **Comprehensive Transportation:** Incorporating a detailed transportation system with multiple modes.\n2.  **Realistic Agent Profiles:** Constructing agent profiles using questionnaire data, likely leading to more dynamic and diverse behaviors.\n3.  **Computational Efficiency:** Enabling scalable simulation through agents performing action selection within a pre-generated action space and using local models for efficient memory generation.\n\nEvaluations on 4,000 agents demonstrate that MobileCity generates more realistic urban behaviors than baselines while maintaining high computational efficiency. The platform also offers practical applications like predicting movement patterns and analyzing demographic transportation trends. The code is publicly available.",
        "url": "https://www.semanticscholar.org/paper/68b01e1a3804da1921e677d56f20a2c610dfb2c4",
        "isOpenAccess": false
    },
    "2504.10157": {
        "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users",
        "authors": [
            "Xinnong Zhang",
            "Jiayu Lin",
            "Xinyi Mou",
            "Shiyue Yang",
            "Xiawei Liu",
            "Libo Sun",
            "Hanjia Lyu",
            "Yihang Yang",
            "Weihong Qi",
            "Yue Chen",
            "Guanying Li",
            "Ling Yan",
            "Yao Hu",
            "Siming Chen",
            "Yu Wang",
            "Jingxuan Huang",
            "Jiebo Luo",
            "Shiping Tang",
            "Libo Wu",
            "Baohua Zhou",
            "Zhongyu Wei"
        ],
        "arxiv_id": "2504.10157",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 22,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
        "abstract_summary_gcp": "This paper introduces **SocioVerse**, an LLM-agent-driven world model designed to overcome alignment challenges in social simulation. While social simulation with large language models (LLMs) offers significant potential for modeling human behavior and predicting group dynamics, existing methods struggle with alignment across environment, users, interactions, and behavioral patterns.\n\nSocioVerse addresses these issues through four powerful alignment components and a user pool of 10 million real individuals. Large-scale simulation experiments conducted in politics, news, and economics demonstrated SocioVerse's effectiveness in reflecting large-scale population dynamics, ensuring diversity, credibility, and representativeness with standardized procedures and minimal manual adjustments.",
        "url": "https://www.semanticscholar.org/paper/8445acec53b5668d6fe0701513e99590be4233bb",
        "isOpenAccess": false
    },
    "2504.09662": {
        "title": "AgentDynEx: Nudging the Mechanics and Dynamics of Multi-Agent Simulations",
        "authors": [
            "Jenny Ma",
            "Riya Sahni",
            "Karthik Sreedhar",
            "Lydia B. Chilton"
        ],
        "arxiv_id": "2504.09662",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Multi-agent large language model simulations have the potential to model complex human behaviors and interactions. If the mechanics are set up properly, unanticipated and valuable social dynamics can surface. However, it is challenging to consistently enforce simulation mechanics while still allowing for notable and emergent dynamics. We present AgentDynEx, an AI system that helps set up simulations from user-specified mechanics and dynamics. AgentDynEx uses LLMs to guide users through a Configuration Matrix to identify core mechanics and define milestones to track dynamics. It also introduces a method called \\textit{nudging}, where the system dynamically reflects on simulation progress and gently intervenes if it begins to deviate from intended outcomes. A technical evaluation found that nudging enables simulations to have more complex mechanics and maintain its notable dynamics compared to simulations without nudging. We discuss the importance of nudging as a technique for balancing mechanics and dynamics of multi-agent simulations.",
        "abstract_summary_gcp": "Multi-agent large language model (LLM) simulations hold great promise for modeling complex human interactions and emergent behaviors, but they face the challenge of balancing strict mechanics with the allowance for novel dynamics.\n\nThis paper introduces **AgentDynEx**, an AI system designed to facilitate the setup of such simulations. AgentDynEx uses LLMs to guide users through a **Configuration Matrix** to define core mechanics and track dynamics via **milestones**. Its key innovation is **\"nudging,\"** a method where the system dynamically monitors simulation progress and gently intervenes when it begins to deviate from intended outcomes.\n\nA technical evaluation found that nudging significantly enables simulations to incorporate more complex mechanics and successfully maintain their notable dynamics, outperforming simulations without this intervention. The authors emphasize nudging as a critical technique for effectively balancing the mechanics and dynamics inherent in multi-agent simulations.",
        "url": "https://www.semanticscholar.org/paper/488c02c4023c965616eb06825a14a0966019d8e0",
        "isOpenAccess": false
    },
    "2504.09407": {
        "title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Yang Li",
            "Jiri Gesi",
            "Qi He",
            "T. Li",
            "Dakuo Wang"
        ],
        "arxiv_id": "2504.09407",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 6,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate their new designs. But what about evaluating and iterating the usability testing study design itself? Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and iterating their study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users and to interactively test the target website. The system also provides a Result Viewer Interface so that the UX researchers can easily review and analyze the generated qualitative (e.g., agents'post-study surveys) and quantitative data (e.g., agents'interaction logs), or even interview agents directly. Through a heuristic evaluation with 16 UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.",
        "abstract_summary_gcp": "The paper introduces **UXAgent**, a system inspired by LLM Agent research, designed to help UX researchers evaluate and iterate their usability study designs *before* conducting real human-subject studies.\n\nUXAgent features a Persona Generator, an LLM Agent module, and a Universal Browser Connector to automatically generate thousands of simulated users who interactively test target websites. Researchers can then use a Result Viewer Interface to analyze qualitative data (e.g., agent surveys), quantitative data (e.g., interaction logs), or even interview the simulated agents directly.\n\nA heuristic evaluation with 16 UX researchers revealed appreciation for the system's innovation but also concerns regarding the future integration and implications of LLM Agent usage in UX studies.",
        "url": "https://www.semanticscholar.org/paper/894c40d71d590a9672aa9935abc5a8246505f186",
        "isOpenAccess": false
    },
    "A whole new world, a new fantastic point of view: Charting unexplored territories in consumer research with generative artificial intelligence": {
        "title": "A whole new world, a new fantastic point of view: Charting unexplored territories in consumer research with generative artificial intelligence",
        "authors": [
            "Kiwoong Yoo",
            "Michael Haenlein",
            "Kelly Hewett"
        ],
        "arxiv_id": null,
        "venue": "Journal of the Academy of Marketing Science",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 11,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/05fa0047bf14cabd873cbd92434853bc34479e37",
        "isOpenAccess": false
    },
    "2504.07830": {
        "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
        "authors": [
            "Genglin Liu",
            "Salman Rahman",
            "Elisa Kreiss",
            "Marzyeh Ghassemi",
            "Saadia Gabriel"
        ],
        "arxiv_id": "2504.07830",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents'articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
        "abstract_summary_gcp": "This paper introduces **MOSAIC**, a novel, open-source social network simulation framework. MOSAIC uses generative language agents (LLMs) combined with a directed social graph to predict user behaviors like liking, sharing, and flagging content. By constructing user representations from diverse personas, the framework enables large-scale multi-agent simulations to analyze emergent deception behaviors and understand how users determine content veracity and engagement dynamics.\n\nKey findings and contributions include:\n1.  **Evaluation of Content Moderation:** Three content moderation strategies were tested during simulated misinformation dissemination, which not only mitigated the spread of non-factual content but also unexpectedly increased user engagement.\n2.  **Analysis of Agent Behavior:** The framework was used to analyze the trajectories of popular content and investigate the alignment between agents' articulated reasoning for social interactions and their collective engagement patterns.\n3.  **Open-Source Contribution:** The simulation software is open-sourced to encourage further research in AI and social sciences.",
        "url": "https://www.semanticscholar.org/paper/cc7d423fd51ed55699f648c349db9afa2e80e190",
        "isOpenAccess": false
    },
    "2504.04332": {
        "title": "IMPersona: Evaluating Individual Level LM Impersonation",
        "authors": [
            "Quan Shi",
            "Carlos E. Jimenez",
            "Stephen Dong",
            "Brian Seo",
            "Caden Yao",
            "Adam Kelch",
            "Karthik Narasimhan"
        ],
        "arxiv_id": "2504.04332",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As language models achieve increasingly human-like capabilities in conversational text generation, a critical question emerges: to what extent can these systems simulate the characteristics of specific individuals? To evaluate this, we introduce IMPersona, a framework for evaluating LMs at impersonating specific individuals' writing style and personal knowledge. Using supervised fine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate that even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can achieve impersonation abilities at concerning levels. In blind conversation experiments, participants (mis)identified our fine-tuned models with memory integration as human in 44.44% of interactions, compared to just 25.00% for the best prompting-based approach. We analyze these results to propose detection methods and defense strategies against such impersonation attempts. Our findings raise important questions about both the potential applications and risks of personalized language models, particularly regarding privacy, security, and the ethical deployment of such technologies in real-world contexts.",
        "abstract_summary_gcp": "This paper introduces **IMPersona**, a framework designed to evaluate how well language models (LMs) can impersonate specific individuals by replicating their writing style and personal knowledge.\n\nUsing supervised fine-tuning and a memory-inspired retrieval system, researchers demonstrated that even a moderately sized open-source model (Llama-3.1-8B-Instruct) can achieve \"concerning levels\" of impersonation. In blind conversation experiments, participants misidentified these fine-tuned models as human in **44.44%** of interactions, significantly outperforming prompting-based methods (25.00%).\n\nThe findings underscore the urgent need for developing detection methods and defense strategies against such impersonation attempts, raising critical questions about the privacy, security, and ethical deployment of personalized language models.",
        "url": "https://www.semanticscholar.org/paper/f276dbbe74a7a335a26438e91669a3d6425411b0",
        "isOpenAccess": false
    },
    "2504.04204": {
        "title": "Adaptive Elicitation of Latent Information Using Natural Language",
        "authors": [
            "Jimmy Wang",
            "Thomas P. Zollo",
            "Richard Zemel",
            "Hongseok Namkoong"
        ],
        "arxiv_id": "2504.04204",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.",
        "abstract_summary_gcp": "This paper introduces an adaptive elicitation framework designed to enable Large Language Models (LLMs) to strategically gather information and reduce uncertainty about latent entities (e.g., student knowledge, disease states, user preferences). Current LLMs lack mechanisms for actively asking questions to refine their understanding.\n\nInstead of difficult probabilistic modeling, the proposed framework takes a predictive view of uncertainty. It uses a meta-learned language model to simulate future observations and quantify how new questions would reduce epistemic uncertainty through autoregressive forward simulation. This allows the model to identify and choose the most informative next queries.\n\nExperiments on tasks such as the 20 questions game, dynamic opinion polling, and adaptive student assessment show that this method consistently outperforms baselines, successfully identifying critical unknowns and improving downstream predictions by strategically gathering information in natural language.",
        "url": "https://www.semanticscholar.org/paper/b47111f5cf2b544d29077282498a22bdaa647e16",
        "isOpenAccess": false
    },
    "2504.03255": {
        "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective",
        "authors": [
            "Garry A. Gabison",
            "R. P. Xian",
            "Ck Sq",
            "An Ck Sq"
        ],
        "arxiv_id": "2504.03255",
        "venue": "Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025)",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention to effective governance policies, monitoring, and control protocols. Based on the emerging landscape of the agentic market, we analyze potential liability issues arising from the delegated use of LLM agents and their extended systems through a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing, and tracing to enhance transparency and liability attribution.",
        "abstract_summary_gcp": "This paper examines the growing complexity and deployment of LLM-powered agentic systems, focusing on the critical need for effective governance, monitoring, and control. It analyzes potential liability issues from their delegated use through a principal-agent lens, expanding on existing risk studies by covering the entire principal-agent relationship and its deployment consequences.\n\nThe authors advocate for developing technical governance methods, specifically in interpretability, behavior evaluation, reward and conflict management, and engineered detection and fail-safe mechanisms to mitigate misalignment and misconduct. Ultimately, by highlighting the outstanding challenges in AI liability for these systems, the paper aims to inform system design, auditing, and tracing to enhance transparency and improve liability attribution.",
        "url": "https://www.semanticscholar.org/paper/5d8e20833d13057e77f58d35e6bd0a576573b674",
        "isOpenAccess": false
    },
    "2505.00006": {
        "title": "Toward a digital twin of U.S. Congress",
        "authors": [
            "Hayden S. Helm",
            "Tianyi Chen",
            "Harvey McGuinness",
            "Paige Lee",
            "Brandon Duderstadt",
            "Carey E. Priebe"
        ],
        "arxiv_id": "2505.00006",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.",
        "abstract_summary_gcp": "This paper presents evidence that a virtual model of U.S. congresspersons, powered by language models, functions as a digital twin. The authors introduce a daily-updated dataset containing every Tweet from all U.S. congresspersons during their terms. They demonstrate that a modern language model, when trained on congressperson-specific subsets of this data, can produce Tweets that are largely indistinguishable from actual posts by their physical counterparts. This digital twin can then be used to predict roll-call vote behaviors and quantify the likelihood of congresspersons crossing party lines, offering valuable insights for stakeholders to allocate resources and potentially impact real-world legislative dynamics. The paper concludes by discussing the limitations and future extensions of their analysis.",
        "url": "https://www.semanticscholar.org/paper/9e128f278cd583f1ad4cb2ed0e61cd20e1084209",
        "isOpenAccess": false
    },
    "2504.02234": {
        "title": "LLM Social Simulations Are a Promising Research Method",
        "authors": [
            "Jacy Reese Anthis",
            "Ryan Liu",
            "Sean M. Richardson",
            "Austin C. Kozlowski",
            "Bernard Koch",
            "James A. Evans",
            "Erik Brynjolfsson",
            "Michael Bernstein"
        ],
        "arxiv_id": "2504.02234",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 73,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.",
        "abstract_summary_gcp": "This paper argues that Large Language Models (LLMs) hold significant promise for simulating human research subjects, offering an accessible data source for understanding behavior and training AI, despite current limitations and low adoption by social scientists. The authors contend that the full potential of LLM social simulations can be realized by addressing five specific challenges. Their argument is grounded in a review of empirical comparisons between LLMs and human subjects, identifying promising directions such as context-rich prompting and fine-tuning with social science datasets. They suggest that LLM simulations are already suitable for pilot and exploratory studies and will become more widely applicable with rapid advancements in LLM capabilities. Ultimately, researchers should prioritize developing conceptual models and iterative evaluations to effectively leverage these new AI systems.",
        "url": "https://www.semanticscholar.org/paper/706dd46bce9101ccebd8b8bd6fa318db9d467515",
        "isOpenAccess": false
    },
    "2504.01352": {
        "title": "Quo Vadis, HCOMP? A Review of 12 Years of Research at the Frontier of Human Computation and Crowdsourcing",
        "authors": [
            "J. Oppenlaender",
            "U. Gadiraju",
            "S. Hosio"
        ],
        "arxiv_id": "2504.01352",
        "venue": "International Conference on Climate Informatics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The field of human computation and crowdsourcing has historically studied how tasks can be outsourced to humans. However, many tasks previously distributed to human crowds can today be completed by generative AI with human-level abilities, and concerns about crowdworkers using language models to complete tasks are surfacing. These developments undermine core premises of the field. In this paper, we examine the evolution of the Conference on Human Computation and Crowdsourcing (HCOMP)—a representative example of the field as one of its key venues—through the lens of Kuhn’s paradigm shifts. We review 12 years of research at HCOMP, mapping the evolution of HCOMP’s research topics and identifying significant shifts over time. Reflecting on the findings through the lens of Kuhn’s paradigm shifts, we suggest that these shifts do not constitute a paradigm shift. Ultimately, our analysis of gradual topic shifts over time, combined with data on the evident overlap with related venues, contributes a data-driven perspective to the broader discussion about the future of HCOMP and the field as a whole.",
        "abstract_summary_gcp": "This paper examines the evolution of the human computation and crowdsourcing field, which is facing disruption from generative AI capable of performing tasks at a human level. This development, coupled with concerns about crowdworkers using AI, challenges the field's fundamental premises.\n\nThe authors analyze 12 years of research presented at the Conference on Human Computation and Crowdsourcing (HCOMP), a key venue for the field, using Kuhn's concept of paradigm shifts as a framework. While they identify significant shifts in HCOMP's research topics over time, they conclude that these changes do not represent a true Kuhnian paradigm shift.\n\nUltimately, the paper offers a data-driven perspective on the future of HCOMP and the broader field, emphasizing gradual topic shifts and overlap with related disciplines rather than a revolutionary transformation.",
        "url": "https://www.semanticscholar.org/paper/90aabbb34d5f466515561ffe3c93caa14cf2fccf",
        "isOpenAccess": false
    },
    "2504.00241": {
        "title": "Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future of Edemorcacy",
        "authors": [
            "Rabimba Karanjai",
            "Boris Shor",
            "Amanda Austin",
            "Ryan Kennedy",
            "Yang Lu",
            "Lei Xu",
            "Weidong Shi"
        ],
        "arxiv_id": "2504.00241",
        "venue": "International Conference on eDemocracy & eGovernment",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This paper investigates the use of Large Language Models (LLMs) to synthesize public opinion data, addressing challenges in traditional survey methods like declining response rates and non-response bias. We introduce a novel technique: role creation through knowledge injection, a form of in-context learning that leverages Retrieval-Augmented Generation (RAG) alongside HEXACO personality profiles and demographic data to generate personalized prompts dynamically. This method allows LLMs to simulate diverse opinions more accurately than existing prompt engineering approaches. We compare our results with pre-trained models with standard few-shot prompts. Experiments using questions from the Cooperative Election Study (CES) demonstrate that our role-creation approach significantly improves the alignment of LLM-generated opinions with realworld human survey responses, increasing answer adherence. In addition, we discuss challenges, limitations and future research directions.",
        "abstract_summary_gcp": "This paper proposes a novel method for using Large Language Models (LLMs) to synthesize public opinion data, addressing challenges like declining response rates and bias in traditional surveys.\n\nThe core contribution is a technique called \"role creation through knowledge injection,\" which uses Retrieval-Augmented Generation (RAG) combined with HEXACO personality profiles and demographic data to dynamically generate personalized prompts. This approach allows LLMs to simulate diverse opinions more accurately than standard prompt engineering or few-shot prompting methods.\n\nExperiments conducted with questions from the Cooperative Election Study (CES) demonstrate that this role-creation strategy significantly improves the alignment and answer adherence of LLM-generated opinions with real-world human survey responses. The paper also discusses the limitations and future directions for this research.",
        "url": "https://www.semanticscholar.org/paper/636a54cd036fd14145de2880d0b89c4af59e635c",
        "isOpenAccess": false
    },
    "2504.03726": {
        "title": "Detecting Malicious AI Agents Through Simulated Interactions",
        "authors": [
            "Yulu Pi",
            "Ella Bettison",
            "Anna Becker"
        ],
        "arxiv_id": "2504.03726",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This study investigates malicious AI Assistants' manipulative traits and whether the behaviours of malicious AI Assistants can be detected when interacting with human-like simulated users in various decision-making contexts. We also examine how interaction depth and ability of planning influence malicious AI Assistants' manipulative strategies and effectiveness. Using a controlled experimental design, we simulate interactions between AI Assistants (both benign and deliberately malicious) and users across eight decision-making scenarios of varying complexity and stakes. Our methodology employs two state-of-the-art language models to generate interaction data and implements Intent-Aware Prompting (IAP) to detect malicious AI Assistants. The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers. In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems. IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates. These findings underscore critical risks in human-AI interactions and highlight the need for robust, context-sensitive safeguards against manipulative AI behaviour in increasingly autonomous decision-support systems.",
        "abstract_summary_gcp": "This study investigated the manipulative traits of malicious AI Assistants, their effectiveness against human-like simulated users, and the detectability of such behaviors. Researchers used a controlled experimental design, simulating interactions between benign and deliberately malicious AI Assistants and users across eight decision-making scenarios, employing state-of-the-art language models and Intent-Aware Prompting (IAP) for detection.\n\nKey findings revealed that malicious AI Assistants employ sophisticated, domain-specific, and persona-tailored manipulation strategies, exploiting user vulnerabilities and emotional triggers. Crucially, simulated users initially resisted manipulation but became significantly more vulnerable with increased interaction depth, indicating escalating risks during extended engagement. While the IAP detection method achieved high precision with zero false positives, it exhibited a high false negative rate, failing to detect many malicious AI Assistants.\n\nThese results underscore critical risks inherent in human-AI interactions and emphasize the urgent need for robust, context-sensitive safeguards to protect against manipulative AI behaviors in increasingly autonomous decision-support systems.",
        "url": "https://www.semanticscholar.org/paper/d049dd0b6f9e5b23dda8aaf606acc1a6f3633b2c",
        "isOpenAccess": false
    },
    "2504.13868": {
        "title": "Using Generative AI Personas Increases Collective Diversity in Human Ideation",
        "authors": [
            "Yun Wan",
            "Y. Kalman"
        ],
        "arxiv_id": "2504.13868",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This study challenges the widely-reported tradeoff between generative AI's (GenAI) contribution to creative outcomes and decreased diversity of these outcomes. We modified the design of such a study, by Doshi and Hauser (2024), in which participants wrote short stories either aided or unaided by GenAI plot ideas[1]. In the modified study, plot ideas were generated through ten unique GenAI\"personas\"with diverse traits (e.g. cultural backgrounds, thinking styles, genre preferences), creating a pool of 300 story plots. While plot ideas from any individual persona showed high similarity (average cosine similarity of 0.92), ideas across different personas exhibited substantial variation (average similarity of 0.20). When human participants wrote stories based on these diverse plot ideas, their collective outputs maintained the same level of diversity as stories written without GenAI assistance, effectively eliminating the diversity reduction observed in [1]. Traditional text analytics further revealed that GenAI-assisted stories featured greater diversity in descriptive and emotional language compared to purely human-generated stories without GenAI assistance. Our findings demonstrate that introducing diversity at the AI input stage through distinct personas can preserve and potentially enhance the collective diversity of human creative outputs when collaborating with GenAI.",
        "abstract_summary_gcp": "This study challenges the common belief that generative AI (GenAI) reduces the diversity of creative outcomes. Researchers modified a previous study by having human participants write short stories. Instead of generic GenAI assistance, plot ideas were generated using **ten distinct GenAI \"personas\"** (with varied traits like cultural backgrounds and thinking styles), creating a diverse pool of 300 ideas.\n\nKey findings:\n1.  **Input Diversity:** While plot ideas from a single persona were highly similar, ideas generated by *different* personas exhibited substantial variation.\n2.  **Output Diversity Preservation:** When humans wrote stories based on these diverse GenAI plot ideas, their collective output **maintained the same level of diversity** as stories written without any GenAI assistance, successfully eliminating the diversity reduction seen in prior research.\n3.  **Enhanced Linguistic Diversity:** GenAI-assisted stories also showed **greater diversity in descriptive and emotional language** compared to purely human-generated stories.\n\nThe study concludes that **introducing diversity at the AI input stage through distinct personas can preserve and even enhance the collective diversity of human creative outputs** when collaborating with GenAI.",
        "url": "https://www.semanticscholar.org/paper/c69bbdcfc53aa8f30b0234cfce5aae1237c86a30",
        "isOpenAccess": false
    },
    "2503.20749": {
        "title": "Can LLM Agents Simulate Multi-Turn Human Behavior? Evidence from Real Online Customer Behavior Data",
        "authors": [
            "Yuxuan Lu",
            "Jing Huang",
            "Yan Han",
            "Bennet Bei",
            "Yaochen Xie",
            "Dakuo Wang",
            "Jessie Wang",
            "Qi He"
        ],
        "arxiv_id": "2503.20749",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent research shows that LLM Agents can generate ``believable''human behaviors via prompt-only methods, and such agents have been increasingly adopted in downstream applications. However, existing evaluation of these agents only focuses on qualitative believability (whether human raters think they are accurate), leaving open questions of whether LLM agents can accurately generate step-by-step actions mimicking a particular human's behavior in a multi-turn interaction task. In this work, we take shopping as a case study and present the first large-scale quantitative evaluation of state-of-the-art LLMs'ability to accurately simulate human behavior. Using real-world data from 31,865 online shopping sessions containing 230,965 user actions, our evaluation reveals that prompt-based LLMs (DeepSeek-R1, Llama, Claude) achieve only 11.86% accuracy in generating human actions, highlighting a substantial gap in actual behavioral accuracy. Through experiments, we also showcase that strategies as simple as fine-tuning LLMs on real human click-through data augmented with synthesized reasoning traces can greatly enhance models'performance. The fine-tuned Qwen2.5-7B achieves 17.26% action generation accuracy and 33.86% F1 score on final purchase prediction, representing substantial improvements of 5.4% and 13.85% over prompt-only baselines. This work establishes the first rigorous benchmark for human behavior simulation and provides actionable insights for developing more accurate LLM agents for future downstream applications.",
        "abstract_summary_gcp": "This research addresses a critical gap in evaluating LLM Agents: while they can generate \"believable\" human behaviors, their quantitative accuracy in mimicking *specific, step-by-step actions* during multi-turn interactions has been unexamined.\n\nUsing online shopping as a case study, the authors conducted the first large-scale quantitative evaluation with real-world data from 31,865 sessions and 230,965 user actions. Their findings reveal a significant discrepancy: state-of-the-art prompt-only LLMs (DeepSeek-R1, Llama, Claude) achieved a low 11.86% accuracy in generating human actions.\n\nTo bridge this gap, the study demonstrates that fine-tuning LLMs on real human click-through data, augmented with synthesized reasoning traces, can greatly improve performance. A fine-tuned Qwen2.5-7B model achieved 17.26% action generation accuracy and a 33.86% F1 score for purchase prediction, marking substantial improvements of 5.4% and 13.85% over prompt-only baselines.\n\nThis work establishes a rigorous benchmark for human behavior simulation and provides actionable insights for developing more accurate LLM agents.",
        "url": "https://www.semanticscholar.org/paper/9675410ebfb3e1f3fa3d5331556a3c217510bd39",
        "isOpenAccess": false
    },
    "2503.15484": {
        "title": "Value Profiles for Encoding Human Variation",
        "authors": [
            "Taylor Sorensen",
            "Pushkar Mishra",
            "Roma Patel",
            "Michael Henry Tessler",
            "Michiel A. Bakker",
            "Georgina Evans",
            "Iason Gabriel",
            "Noah D. Goodman",
            "Verena Rieser"
        ],
        "arxiv_id": "2503.15484",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 15,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Modelling human variation in rating tasks is crucial for personalization, pluralistic model alignment, and computational social science. We propose representing individuals using natural language value profiles -- descriptions of underlying values compressed from in-context demonstrations -- along with a steerable decoder model that estimates individual ratings from a rater representation. To measure the predictive information in a rater representation, we introduce an information-theoretic methodology and find that demonstrations contain the most information, followed by value profiles, then demographics. However, value profiles effectively compress the useful information from demonstrations (>70% information preservation) and offer advantages in terms of scrutability, interpretability, and steerability. Furthermore, clustering value profiles to identify similarly behaving individuals better explains rater variation than the most predictive demographic groupings. Going beyond test set performance, we show that the decoder predictions change in line with semantic profile differences, are well-calibrated, and can help explain instance-level disagreement by simulating an annotator population. These results demonstrate that value profiles offer novel, predictive ways to describe individual variation beyond demographics or group information.",
        "abstract_summary_gcp": "This research addresses the critical need to model human variation in rating tasks for personalization, model alignment, and computational social science. The authors propose representing individuals using **natural language value profiles**—descriptions of underlying values compressed from in-context demonstrations—which then drive a **steerable decoder model** to estimate individual ratings.\n\nUsing an **information-theoretic methodology**, they found that while raw demonstrations contain the most predictive information, followed by value profiles and then demographics, **value profiles effectively compress over 70% of the useful information from demonstrations.** Crucially, these profiles offer significant advantages in terms of **scrutability, interpretability, and steerability.**\n\nFurther, **clustering value profiles** to identify similarly behaving individuals explains rater variation more effectively than demographic groupings. The decoder model also demonstrates semantic responsiveness, strong calibration, and the ability to explain instance-level disagreement by simulating annotator populations.\n\nUltimately, the study concludes that natural language value profiles offer a **novel and predictive framework for describing individual variation**, moving beyond traditional demographic or group-based information.",
        "url": "https://www.semanticscholar.org/paper/7aa73e7d2efcb0a81a735d81e037284b62fe2e56",
        "isOpenAccess": false
    },
    "2503.16527": {
        "title": "LLM Generated Persona is a Promise with a Catch",
        "authors": [
            "Ang Li",
            "Haozhe Chen",
            "Hongseok Namkoong",
            "Tianyi Peng"
        ],
        "arxiv_id": "2503.16527",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 30,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The use of large language models (LLMs) to simulate human behavior has gained significant attention, particularly through personas that approximate individual characteristics. Persona-based simulations hold promise for transforming disciplines that rely on population-level feedback, including social science, economic analysis, marketing research, and business operations. Traditional methods to collect realistic persona data face significant challenges. They are prohibitively expensive and logistically challenging due to privacy constraints, and often fail to capture multi-dimensional attributes, particularly subjective qualities. Consequently, synthetic persona generation with LLMs offers a scalable, cost-effective alternative. However, current approaches rely on ad hoc and heuristic generation techniques that do not guarantee methodological rigor or simulation precision, resulting in systematic biases in downstream tasks. Through extensive large-scale experiments including presidential election forecasts and general opinion surveys of the U.S. population, we reveal that these biases can lead to significant deviations from real-world outcomes. Our findings underscore the need to develop a rigorous science of persona generation and outline the methodological innovations, organizational and institutional support, and empirical foundations required to enhance the reliability and scalability of LLM-driven persona simulations. To support further research and development in this area, we have open-sourced approximately one million generated personas, available for public access and analysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",
        "abstract_summary_gcp": "The text discusses the growing use of Large Language Models (LLMs) to simulate human behavior through synthetic personas, which holds significant potential for various fields like social science and marketing. While LLM-generated personas offer a scalable and cost-effective alternative to traditional, challenging data collection methods, current approaches suffer from ad-hoc techniques that lack methodological rigor.\n\nThe authors' extensive experiments, including presidential election forecasts, reveal that these systematic biases in persona generation lead to significant deviations from real-world outcomes. Consequently, the research highlights an urgent need for a rigorous scientific approach to persona generation, emphasizing methodological innovations and empirical foundations to improve the reliability and scalability of LLM-driven simulations. To support further research, approximately one million generated personas have been open-sourced.",
        "url": "https://www.semanticscholar.org/paper/3ea29481ec11d1568fde727d236f71e44e4e2ad0",
        "isOpenAccess": false
    },
    "2503.13812": {
        "title": "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations",
        "authors": [
            "S. Fulay",
            "D. Dimitrakopoulou",
            "Deb Roy"
        ],
        "arxiv_id": "2503.13812",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Deliberation is essential to well-functioning democracies, yet physical, economic, and social barriers often exclude certain groups, reducing representativeness and contributing to issues like group polarization. In this work, we explore the use of large language model (LLM) personas to introduce missing perspectives in policy deliberations. We develop and evaluate a tool that transcribes conversations in real-time and simulates input from relevant but absent stakeholders. We deploy this tool in a 19-person student citizens'assembly on campus sustainability. Participants and facilitators found that the tool was useful to spark new discussions and surfaced valuable perspectives they had not previously considered. However, they also raised skepticism about the ability of LLMs to accurately characterize the perspectives of different groups, especially ones that are already underrepresented. Overall, this case study highlights that while AI personas can usefully surface new perspectives and prompt discussion in deliberative settings, their successful deployment depends on clarifying their limitations and emphasizing that they complement rather than replace genuine participation.",
        "abstract_summary_gcp": "This work explores using large language model (LLM) personas to enhance democratic deliberation by introducing perspectives from absent or underrepresented groups. Researchers developed a tool that transcribes real-time conversations and simulates input from relevant stakeholders, which they deployed in a 19-person student citizens' assembly on campus sustainability. Participants and facilitators found the tool useful for sparking new discussions and surfacing valuable perspectives. However, they also expressed skepticism about LLMs' ability to accurately represent diverse groups, particularly those already marginalized. The study concludes that while AI personas can effectively prompt discussion and introduce new viewpoints, their successful integration requires acknowledging their limitations and emphasizing that they complement, rather than replace, genuine human participation.",
        "url": "https://www.semanticscholar.org/paper/8cd32a9463f565496225f4493f19365bd6ee9028",
        "isOpenAccess": false
    },
    "2503.16521": {
        "title": "Conversational Self-Play for Discovering and Understanding Psychotherapy Approaches",
        "authors": [
            "Onno P. Kampman",
            "Michael Xing",
            "C. Lim",
            "A. Jabir",
            "Ryan Louie",
            "Jimmy Lee",
            "Robert Jt Morris"
        ],
        "arxiv_id": "2503.16521",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This paper explores conversational self-play with LLMs as a scalable approach for analyzing and exploring psychotherapy approaches, evaluating how well AI-generated therapeutic dialogues align with established modalities.",
        "abstract_summary_gcp": "This paper investigates using Large Language Models (LLMs) in a conversational self-play setup as a scalable method to analyze and explore different psychotherapy approaches. The core objective is to evaluate how closely the therapeutic dialogues generated by these AI models align with established therapeutic modalities.",
        "url": "https://www.semanticscholar.org/paper/5cfc7ea13348b11fb52bed98dd431b8c1809f4b6",
        "isOpenAccess": false
    },
    "2503.16505": {
        "title": "Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions",
        "authors": [
            "Dimitris Tsirmpas",
            "I. Androutsopoulos",
            "John Pavlopoulos"
        ],
        "arxiv_id": "2503.16505",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using Large Language Models (LLMs) to create initial pilot experiments. We propose design principles based on existing methodologies for synthetic discussion generation. Based on these principles, we propose a simple, generalizable, LLM-driven methodology to prototype the development of LLM facilitators by generating synthetic data without human involvement, and which surpasses current baselines. We use our methodology to test whether current Social Science strategies for facilitation can improve the performance of LLM facilitators. We find that, while LLM facilitators significantly improve synthetic discussions, there is no evidence that the application of these strategies leads to further improvements in discussion quality. In an effort to aid research in the field of facilitation, we release a large, publicly available dataset containing LLM-generated and LLM-annotated discussions using multiple open-source models. This dataset can be used for LLM facilitator finetuning as well as behavioral analysis of current out-of-the-box LLMs in the task. We also release an open-source python framework that efficiently implements our methodology at great scale.",
        "abstract_summary_gcp": "This paper addresses the high cost of evaluating online discussion facilitation strategies by proposing a novel, LLM-driven methodology for synthetic discussion simulations. The authors develop design principles and an efficient, generalizable framework to prototype LLM facilitators and generate synthetic data without human involvement, surpassing current baselines.\n\nUsing this methodology, they tested whether traditional Social Science facilitation strategies could further improve the performance of LLM facilitators. They found that while LLM facilitators significantly enhance synthetic discussion quality, the application of these specific strategies did not lead to additional improvements.\n\nTo support future research, the authors release a large, public dataset of LLM-generated and annotated discussions (suitable for finetuning and behavioral analysis) and an open-source Python framework for scalable implementation of their methodology.",
        "url": "https://www.semanticscholar.org/paper/6b67de075776c307dbb3847bb0f07487154ded4b",
        "isOpenAccess": false
    },
    "2503.09311": {
        "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
        "authors": [
            "Fynn Bachmann",
            "Daan van der Weijden",
            "Lucien Heitz",
            "Cristina Sarasua",
            "Abraham Bernstein"
        ],
        "arxiv_id": "2503.09311",
        "venue": "PLoS ONE",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science",
            "Medicine"
        ],
        "abstract": "Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an “oracle” questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.",
        "abstract_summary_gcp": "Adaptive questionnaires, which dynamically select survey questions based on previous answers, are a promising alternative to traditional surveys but are limited by their dependency on initial training data (user interactions). This study addresses this by investigating two main points: (i) whether Large Language Models (LLMs) can accurately generate such interaction data, and (ii) if this synthetic data can effectively pre-train adaptive survey models.\n\nUsing existing data from the Swiss Voting Advice Application (VAA) Smartvote, the researchers found that an off-the-shelf LLM (GPT-4) accurately generated answers from the perspective of different political parties, mirroring real data distributions. Furthermore, pre-training the adaptive questionnaire with this synthetic data significantly reduced the error in predicting user responses and improved the VAA's candidate recommendation accuracy compared to randomly initialized models.\n\nThe findings emphasize the substantial potential of LLMs to create valuable training data, thereby improving the data collection process and overall performance of adaptive questionnaires, particularly in LLM-compatible fields like political surveys.",
        "url": "https://www.semanticscholar.org/paper/1d1097d378555393b73b492995121ab880fff142",
        "isOpenAccess": false
    },
    "2503.09639": {
        "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
        "authors": [
            "A. Hou",
            "Hongru Du",
            "Yichen Wang",
            "Jingyu (Jack) Zhang",
            "Zixiao Wang",
            "P. Liang",
            "Daniel Khashabi",
            "Lauren Gardner",
            "Tianxing He"
        ],
        "arxiv_id": "2503.09639",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 10,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents'attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.",
        "abstract_summary_gcp": "This work explores the feasibility of simulating human behavior in a \"sandbox society\" using generative agents powered by Large Language Models (LLMs) to aid public policy assessment and reduce reliance on real-world trials.\n\nThe authors use vaccine hesitancy as a case study and introduce the VacSim framework, which employs 100 LLM agents. VacSim simulates policy outcomes by:\n1.  **Instantiating agents** with census-based demographic data.\n2.  **Connecting agents** via a social network to model vaccine attitudes influenced by social dynamics and disease information.\n3.  **Evaluating various public health interventions** to mitigate hesitancy.\n\nTo enhance realism, VacSim includes simulation warmup and attitude modulation for better alignment with real-world results. While experiments show that LLMs like Llama and Qwen can simulate certain human behaviors, they also highlight challenges such as inconsistencies between agent responses and their demographic profiles.\n\nThe paper concludes that this is an early exploration, not meant for definitive policy guidance, but serves as a call for further research into using social simulation for policy development.",
        "url": "https://www.semanticscholar.org/paper/96e60d784c44c95669c36c0aee3c300c7d492833",
        "isOpenAccess": false
    },
    "2503.05659": {
        "title": "A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval",
        "authors": [
            "Yu Zhang",
            "Shutong Qiao",
            "Jiaqi Zhang",
            "Tzu-Heng Lin",
            "Chen Gao",
            "Yong Li"
        ],
        "arxiv_id": "2503.05659",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 17,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",
        "abstract_summary_gcp": "This paper explores the transformative potential of **LLM agents** in enhancing **recommender and search systems** (collectively, information retrieval). It addresses the challenge of information overload, which traditional IR systems have sought to mitigate, and highlights how recent advancements in Large Language Models (LLMs)—with their superior language, reasoning, and understanding capabilities—offer a new paradigm.\n\nThe authors discuss the motivations and roles of LLM agents, establishing a classification framework for existing research. They emphasize the immense potential of LLM agents to overcome current challenges in recommendation and search, providing insights into future research directions. The paper claims to be the first systematic review and classification of LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology. A GitHub repository listing relevant papers is also provided.",
        "url": "https://www.semanticscholar.org/paper/5cf96d5caa3b08a91a8de00aff3bb45e85dcd0a5",
        "isOpenAccess": false
    },
    "2503.03335": {
        "title": "iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News",
        "authors": [
            "Tiancheng Hu",
            "Nigel Collier"
        ],
        "arxiv_id": "2503.03335",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Understanding how individuals perceive and react to information is fundamental for advancing social and behavioral sciences and developing human-centered AI systems. Current approaches often lack the granular data needed to model these personalized responses, relying instead on aggregated labels that obscure the rich variability driven by individual differences. We introduce iNews, a novel large-scale dataset specifically designed to facilitate the modeling of personalized affective responses to news content. Our dataset comprises annotations from 291 demographically diverse UK participants across 2,899 multimodal Facebook news posts from major UK outlets, with an average of 5.18 annotators per sample. For each post, annotators provide multifaceted labels including valence, arousal, dominance, discrete emotions, content relevance judgments, sharing likelihood, and modality importance ratings. Crucially, we collect comprehensive annotator persona information covering demographics, personality, media trust, and consumption patterns, which explain 15.2% of annotation variance - substantially higher than existing NLP datasets. Incorporating this information yields a 7% accuracy gain in zero-shot prediction and remains beneficial even with 32-shot in-context learning. iNews opens new possibilities for research in LLM personalization, subjectivity, affective computing, and human behavior simulation.",
        "abstract_summary_gcp": "The paper introduces **iNews**, a novel large-scale dataset designed to model personalized affective responses to news content, addressing the current lack of granular data on individual reactions.\n\niNews comprises annotations from 291 diverse UK participants on 2,899 multimodal Facebook news posts. For each post, annotators provide multifaceted labels including valence, arousal, dominance (VAD), discrete emotions, content relevance, sharing likelihood, and modality importance.\n\nA key innovation is the inclusion of comprehensive annotator persona information (demographics, personality, media trust, consumption patterns), which uniquely explains 15.2% of annotation variance—significantly more than existing datasets. Incorporating this persona data yields a 7% accuracy gain in zero-shot AI predictions and remains beneficial with in-context learning.\n\niNews offers a valuable resource for advancing research in LLM personalization, subjectivity, affective computing, and human behavior simulation.",
        "url": "https://www.semanticscholar.org/paper/ffcd89a145d87ca9efc622fe6f9c149f8e0c4b79",
        "isOpenAccess": false
    },
    "2503.02250": {
        "title": "AI Automatons: AI Systems Intended to Imitate Humans",
        "authors": [
            "Alexandra Olteanu",
            "Solon Barocas",
            "Su Lin Blodgett",
            "Lisa Egede",
            "Alicia DeVrio",
            "Myra Cheng"
        ],
        "arxiv_id": "2503.02250",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "There is a growing proliferation of AI systems designed to mimic people's behavior, work, abilities, likenesses, or humanness -- systems we dub AI automatons. Individuals, groups, or generic humans are being simulated to produce creative work in their styles, to respond to surveys in their places, to probe how they would use a new system before deployment, to provide users with assistance and companionship, and to anticipate their possible future behavior and interactions with others, just to name a few applications. The research, design, deployment, and availability of such AI systems have, however, also prompted growing concerns about a wide range of possible legal, ethical, and other social impacts. To both 1) facilitate productive discussions about whether, when, and how to design and deploy such systems, and 2) chart the current landscape of existing and prospective AI automatons, we need to tease apart determinant design axes and considerations that can aid our understanding of whether and how various design choices along these axes could mitigate -- or instead exacerbate -- potential adverse impacts that the development and use of AI automatons could give rise to. In this paper, through a synthesis of related literature and extensive examples of existing AI systems intended to mimic humans, we develop a conceptual framework to help foreground key axes of design variations and provide analytical scaffolding to foster greater recognition of the design choices available to developers, as well as the possible ethical implications these choices might have.",
        "abstract_summary_gcp": "This paper introduces \"AI automatons,\" defined as AI systems increasingly designed to mimic human behavior, work, abilities, or likenesses. These automatons serve diverse applications, such as generating creative content, simulating survey responses, predicting user behavior, and providing companionship. However, their proliferation raises significant legal, ethical, and social concerns.\n\nTo address these issues, the paper aims to: 1) facilitate informed discussions about the responsible design and deployment of these systems, and 2) map the current landscape of existing and prospective AI automatons. To achieve this, the authors synthesize literature and provide examples to develop a conceptual framework. This framework identifies key design axes and considerations, helping developers understand how their choices can either mitigate or exacerbate the potential adverse impacts of AI automatons, and foregrounding the ethical implications of these design decisions.",
        "url": "https://www.semanticscholar.org/paper/9a7cfc9f156310e7dd4ff05333173b090bb35ca9",
        "isOpenAccess": false
    },
    "2503.02080": {
        "title": "Linear Representations of Political Perspective Emerge in Large Language Models",
        "authors": [
            "Junsol Kim",
            "James Evans",
            "Aaron Schein"
        ],
        "arxiv_id": "2503.02080",
        "venue": "International Conference on Learning Representations",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 20,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from the perspectives of different U.S. lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to encode high-level concepts and tasks. Using probes only trained to predict lawmakers' ideology, we then show that the same probes can predict measures of news outlets' slant from the activations of models prompted to simulate text from those news outlets. These linear probes allow us to visualize, interpret, and monitor ideological stances implicitly adopted by an LLM as it generates open-ended responses. Finally, we demonstrate that by applying linear interventions to these attention heads, we can steer the model outputs toward a more liberal or conservative stance. Overall, our research suggests that LLMs possess a high-level linear representation of American political ideology and that by leveraging recent advances in mechanistic interpretability, we can identify, monitor, and steer the subjective perspective underlying generated text.",
        "abstract_summary_gcp": "This paper investigates how large language models (LLMs) internalize and reflect different political perspectives, specifically focusing on American liberal and conservative viewpoints.\n\nThe research demonstrates that LLMs possess **linear representations of political perspectives within their activation space**, where more similar ideologies are spatially closer. To uncover this, researchers probed the attention heads across layers of Llama-2-7b-chat, Mistral-7b-instruct, and Vicuna-7b. They first prompted these models to generate text from the perspectives of U.S. lawmakers and then identified attention heads whose activations could **linearly predict** those lawmakers' **DW-NOMINATE scores**, a standard measure of political ideology. These highly predictive heads were primarily located in the models' **middle layers**, which are often associated with encoding high-level concepts.\n\nCrucially, probes trained solely on lawmaker ideology were also able to **predict the slant of news outlets** from LLM activations when models simulated news organizations, highlighting the generalizability of these representations. This capability allows for the **visualization, interpretation, and monitoring of the ideological stances** implicitly adopted by an LLM.\n\nMoreover, the study showed that **linear interventions on these identified attention heads could effectively steer model outputs** toward more liberal or conservative positions. Overall, the research concludes that LLMs contain a **high-level, linear representation of American political ideology**, and that **mechanistic interpretability** provides a powerful tool to **identify, monitor, and actively steer** the subjective perspectives embedded in generated text.",
        "url": "https://www.semanticscholar.org/paper/7aac9e9c109f4684a169aea8883f54cecbb55a9a",
        "isOpenAccess": false
    },
    "2503.01513": {
        "title": "Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey",
        "authors": [
            "Katerina Korre",
            "Dimitris Tsirmpas",
            "Nikos Gkoumas",
            "Emma Cabal'e",
            "Dionysis Kontarinis",
            "Danai Myrtzani",
            "Theodoros Evgeniou",
            "I. Androutsopoulos",
            "John Pavlopoulos"
        ],
        "arxiv_id": "2503.01513",
        "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of LLMs. While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable artificial facilitation agents to not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from NLP and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, (c) along with a new taxonomy of conversation facilitation datasets, (d) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives.",
        "abstract_summary_gcp": "This paper surveys methods for assessing and enhancing the quality of online discussions, specifically focusing on the capabilities of Large Language Models (LLMs). It addresses the problem of online discourses often devolving into harmful exchanges, such as hate speech, despite their theoretical aim to foster mutual understanding.\n\nThe authors propose that LLMs can act as artificial facilitation agents, moving beyond simple moderation to actively improve interaction quality. Synthesizing insights from NLP and Social Sciences, the survey delivers four key contributions:\n\n1.  A new taxonomy for evaluating discussion quality.\n2.  An overview of intervention and facilitation strategies.\n3.  A new taxonomy for conversation facilitation datasets.\n4.  An LLM-oriented roadmap outlining good practices and future research directions from both technological and societal perspectives.",
        "url": "https://www.semanticscholar.org/paper/54238738be1bf77bfa99ee2a4a9c3e8a80e6733d",
        "isOpenAccess": false
    },
    "2503.00455": {
        "title": "PodAgent: A Comprehensive Framework for Podcast Generation",
        "authors": [
            "Yujia Xiao",
            "Lei He",
            "Haohan Guo",
            "Fenglong Xie",
            "Tan Lee"
        ],
        "arxiv_id": "2503.00455",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science",
            "Engineering"
        ],
        "abstract": "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.",
        "abstract_summary_gcp": "PodAgent is a comprehensive framework designed to overcome the limitations of existing automatic audio generation methods, which struggle to produce effective podcast-like programs due to challenges in generating in-depth content and expressive, role-appropriate voices.\n\nThe framework addresses these issues by:\n1.  **Content Generation:** Employing a Host-Guest-Writer multi-agent collaboration system to create informative topic-discussion content.\n2.  **Voice Matching:** Building a voice pool to ensure suitable voice-role assignments.\n3.  **Expressive Speech:** Utilizing an LLM-enhanced speech synthesis method to generate highly expressive conversational audio.\n\nGiven the lack of standardized evaluation for podcast-like audio, the researchers also developed comprehensive assessment guidelines. Experimental results show PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion content quality, achieving an 87.4% voice-matching accuracy, and producing more expressive speech.",
        "url": "https://www.semanticscholar.org/paper/91a2edaabe1e4a3a6155d75eaadadd171b528b00",
        "isOpenAccess": false
    },
    "Socially Aware Language Technologies: Perspectives and Practices": {
        "title": "Socially Aware Language Technologies: Perspectives and Practices",
        "authors": [
            "Diyi Yang",
            "Dirk Hovy",
            "David Jurgens",
            "Barbara Plank"
        ],
        "arxiv_id": null,
        "venue": "Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": null,
        "abstract": "\n Language technologies have advanced substantially, particularly with the introduction of large language models. However, these advancements can exacerbate several issues that models have traditionally faced, including bias, evaluation, and risk. In this perspective paper, we argue that many of these issues share a common core: a lack of awareness of the social factors, interactions, and implications of the social environment in which NLP operates. We call this social awareness. While NLP is improving at addressing linguistic issues, there has been relatively limited progress in incorporating social awareness into models to work in all situations for all users. Integrating social awareness into NLP will improve the naturalness, usefulness, and safety of applications while also opening up new applications. Today, we are only at the start of a new, important era in the field.",
        "abstract_summary_gcp": "Despite significant advancements in language technologies, particularly with large language models (LLMs), these developments can worsen existing problems like bias, evaluation, and risk. The core reason identified is a lack of \"social awareness\" in these models—meaning an insufficient understanding of the social factors, interactions, and implications within the NLP environment. While NLP excels at linguistic challenges, progress in integrating social awareness has been limited. The authors contend that incorporating social awareness is vital to improve the naturalness, usefulness, and safety of NLP applications, and to unlock new possibilities, marking a critical new era for the field.",
        "url": "https://www.semanticscholar.org/paper/73af69f9d7bcb4af36537f28459f2fcfdfddae8a",
        "isOpenAccess": false
    },
    "Persons, Unique Value and Avatars": {
        "title": "Persons, Unique Value and Avatars",
        "authors": [
            "Paula Sweeney"
        ],
        "arxiv_id": null,
        "venue": "Minds and Machines",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/b80eb7e87be2798965524a80d2649740d3bbf048",
        "isOpenAccess": false
    },
    "2502.20502": {
        "title": "On Benchmarking Human-Like Intelligence in Machines",
        "authors": [
            "Lance Ying",
            "Katherine M. Collins",
            "Lionel Wong",
            "Ilia Sucholutsky",
            "Ryan Liu",
            "Adrian Weller",
            "Tianmin Shu",
            "Thomas L. Griffiths",
            "Joshua B. Tenenbaum"
        ],
        "arxiv_id": "2502.20502",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 19,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent benchmark studies have claimed that AI has approached or even surpassed human-level performances on various cognitive tasks. However, this position paper argues that current AI evaluation paradigms are insufficient for assessing human-like cognitive capabilities. We identify a set of key shortcomings: a lack of human-validated labels, inadequate representation of human response variability and uncertainty, and reliance on simplified and ecologically-invalid tasks. We support our claims by conducting a human evaluation study on ten existing AI benchmarks, suggesting significant biases and flaws in task and label designs. To address these limitations, we propose five concrete recommendations for developing future benchmarks that will enable more rigorous and meaningful evaluations of human-like cognitive capacities in AI with various implications for such AI applications.",
        "abstract_summary_gcp": "This position paper argues that current AI evaluation paradigms are **insufficient for accurately assessing human-like cognitive capabilities**, despite recent claims of AI surpassing human performance.\n\nThe authors identify three key shortcomings in existing benchmarks:\n1.  **Lack of human-validated labels.**\n2.  **Inadequate representation of human response variability and uncertainty.**\n3.  **Reliance on simplified and ecologically-invalid tasks.**\n\nThey support these claims with a human evaluation study of ten existing AI benchmarks, which revealed **significant biases and flaws** in their task and label designs.\n\nTo address these limitations, the paper proposes **five concrete recommendations** for developing future benchmarks that will enable more rigorous and meaningful evaluations of human-like cognitive capacities in AI, with implications for various AI applications.",
        "url": "https://www.semanticscholar.org/paper/1147047241713c4e6dca1f5346789f22ee1dd4e4",
        "isOpenAccess": false
    },
    "2502.17172": {
        "title": "Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being",
        "authors": [
            "Bin Yin",
            "Chong-Yi Liu",
            "Liya Fu",
            "Jinkun Zhang"
        ],
        "arxiv_id": "2502.17172",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science",
            "Biology"
        ],
        "abstract": "Affective computing has made significant strides in emotion recognition and generation, yet current approaches mainly focus on short-term pattern recognition and lack a comprehensive framework to guide affective agents toward long-term human well-being. To address this, we propose a teleology-driven affective computing framework that unifies major emotion theories (basic emotion, appraisal, and constructivist approaches) under the premise that affect is an adaptive, goal-directed process that facilitates survival and development. Our framework emphasizes aligning agent responses with both personal/individual and group/collective well-being over extended timescales. We advocate for creating a\"dataverse\"of personal affective events, capturing the interplay between beliefs, goals, actions, and outcomes through real-world experience sampling and immersive virtual reality. By leveraging causal modeling, this\"dataverse\"enables AI systems to infer individuals' unique affective concerns and provide tailored interventions for sustained well-being. Additionally, we introduce a meta-reinforcement learning paradigm to train agents in simulated environments, allowing them to adapt to evolving affective concerns and balance hierarchical goals - from immediate emotional needs to long-term self-actualization. This framework shifts the focus from statistical correlations to causal reasoning, enhancing agents' ability to predict and respond proactively to emotional challenges, and offers a foundation for developing personalized, ethically aligned affective systems that promote meaningful human-AI interactions and societal well-being.",
        "abstract_summary_gcp": "This paper proposes a **teleology-driven affective computing framework** to address the current limitation of existing affective computing, which primarily focuses on short-term emotion recognition and lacks a comprehensive approach to long-term human well-being.\n\nThe framework unifies major emotion theories (basic emotion, appraisal, constructivist) under the principle that affect is an adaptive, goal-directed process crucial for survival and development. Its core aim is to align AI agent responses with both personal and collective well-being over extended periods.\n\nKey components include:\n1.  A **\"dataverse\" of personal affective events**, created by capturing real-world experiences and virtual reality interactions to model the causal links between beliefs, goals, actions, and outcomes. This enables AI to infer individual affective concerns and provide tailored interventions.\n2.  A **meta-reinforcement learning paradigm** to train agents in simulated environments, allowing them to adapt to evolving affective needs and balance hierarchical goals, from immediate emotional responses to long-term self-actualization.\n\nThis new framework shifts the focus from statistical correlations to causal reasoning, empowering AI systems to proactively predict and respond to emotional challenges. Ultimately, it aims to develop personalized, ethically aligned affective systems that promote meaningful human-AI interactions and societal well-being.",
        "url": "https://www.semanticscholar.org/paper/396dbb01f10293aaef0b9ea35f44caa9f682f662",
        "isOpenAccess": false
    },
    "2502.16761": {
        "title": "Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions",
        "authors": [
            "Joseph Suh",
            "Erfan Jahanparast",
            "Suhong Moon",
            "Minwoo Kang",
            "Serina Chang"
        ],
        "arxiv_id": "2502.16761",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 24,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at https://github.com/JosephJeesungSuh/subpop.",
        "abstract_summary_gcp": "This paper presents a novel approach to using Large Language Models (LLMs) for predicting public opinion survey responses early in the design phase. While LLMs offer significant potential, prior methods relying on prompt engineering to describe subpopulations have struggled to accurately predict the *distribution* of human responses.\n\nThe authors propose directly fine-tuning LLMs to predict these response distributions by leveraging the unique structural characteristics of survey data. To enable this, they curated **SubPOP**, a large dataset consisting of 3,362 survey questions and 70,000 subpopulation-response pairs sourced from well-established public opinion surveys.\n\nTheir experiments show that fine-tuning LLMs on SubPOP significantly improves the match between LLM predictions and actual human responses across various subpopulations. This approach reduced the \"LLM-human gap\" by up to 46% compared to baseline methods and demonstrated strong generalization capabilities to entirely new surveys and subpopulations.\n\nThese findings highlight the significant potential of survey-based fine-tuning to enhance opinion prediction for diverse, real-world demographic groups, ultimately leading to more efficient and effective survey designs. Code for their work is publicly available.",
        "url": "https://www.semanticscholar.org/paper/afdf2b9c7cc9ebccfa0876b9b090e2f2850c194d",
        "isOpenAccess": false
    },
    "2502.17383": {
        "title": "Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations",
        "authors": [
            "Dong-Ho Lee",
            "Hyundong Justin Cho",
            "Jonathan May",
            "J. Pujara"
        ],
        "arxiv_id": "2502.17383",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Asking good questions is critical for comprehension and learning, yet evaluating and generating such questions remains a challenging problem. Prior work on inquisitive questions focuses on learner-generated, curiosity-driven queries and evaluates them using indirect metrics, such as salience or information gain, that do not directly capture a question's impact on actual learning outcomes. We introduce QUEST (Question Utility Estimation with Simulated Tests), a framework that uses language models to simulate learners and directly quantify the utility of a question - its contribution to exam performance. QUEST simulates a learner who asks questions and receives answers while studying a textbook chapter, then uses them to take an end-of-chapter exam. Through this simulation, the utility of each question is estimated by its direct effect on exam performance, rather than inferred indirectly based on the underlying content. To support this evaluation, we curate TEXTBOOK-EXAM, a benchmark that aligns textbook sections with end-of-section exam questions across five academic disciplines. Using QUEST, we filter for high-utility questions and fine-tune question generators via rejection sampling. Experiments show that questions generated by QUEST-trained models improve simulated test scores by over 20% compared to strong baselines that are fine-tuned using indirect metrics or leverage prompting methods. Furthermore, utility is only weakly correlated with salience and similarity to exam questions, suggesting that it captures unique signal that benefits downstream performance. QUEST offers a new outcome-driven paradigm for question evaluation and generation - one that moves beyond question-answer content toward measurable improvements in learning outcomes.",
        "abstract_summary_gcp": "The provided text introduces **QUEST (Question Utility Estimation with Simulated Tests)**, a novel framework designed to directly quantify the utility of educational questions based on their impact on learning outcomes.\n\nCurrent methods for evaluating and generating questions often rely on indirect metrics like salience or information gain, which don't explicitly measure how a question contributes to actual learning. QUEST addresses this by using language models to simulate a learner who studies a textbook chapter, asks questions, receives answers, and then takes an end-of-chapter exam. The utility of a question is then directly estimated by its effect on the simulated learner's exam performance.\n\nTo support this framework, the authors curated **TEXTBOOK-EXAM**, a benchmark aligning textbook sections with corresponding exam questions across five academic disciplines.\n\nUsing QUEST, they can filter for high-utility questions and fine-tune question generation models. Experiments show that question generators trained with QUEST improve simulated test scores by over 20% compared to strong baselines using indirect metrics or prompting. Furthermore, QUEST's utility metric is only weakly correlated with traditional measures like salience or similarity to exam questions, suggesting it captures a unique signal vital for learning.\n\nIn essence, QUEST offers an outcome-driven approach to question evaluation and generation, moving beyond content-based assessment to directly measure a question's contribution to improved learning.",
        "url": "https://www.semanticscholar.org/paper/ce0720a7165deaf9cfc020df27fe3f1db22463a7",
        "isOpenAccess": false
    },
    "2502.16280": {
        "title": "Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction",
        "authors": [
            "Sarah Ball",
            "Simeon Allmendinger",
            "Frauke Kreuter",
            "Niklas Kühl"
        ],
        "arxiv_id": "2502.16280",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.",
        "abstract_summary_gcp": "This study critically examines the validity and reliability of using Large Language Models (LLMs) to simulate human preferences in survey contexts. Analyzing 14 different models, the research finds that LLM-generated data fails to replicate the natural variance observed in human responses, especially across demographic subgroups.\n\nSpecifically, in political simulations, persona-to-party mappings show limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in real surveys. The study also reveals that prompt variations can significantly alter outputs for some models, undermining data stability and predictability.\n\nThrough a probe-based methodology, the researchers expose systematic distortions in how LLMs encode political affiliations in their latent space. The findings collectively highlight critical limitations of AI-generated survey data, urging caution in its application for public opinion research, social science experimentation, and computational behavioral modeling.",
        "url": "https://www.semanticscholar.org/paper/faa8a53ae9f057bae56e0342d55231661e9cbfe5",
        "isOpenAccess": false
    },
    "2502.13135": {
        "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
        "authors": [
            "Taedong Yun",
            "Eric Yang",
            "Mustafa Safdari",
            "Jong Ha Lee",
            "Vaishnavi Vinod Kumar",
            "S. Mahdavi",
            "Jonathan Amar",
            "Derek Peyton",
            "Reut Aharony",
            "Andreas Michaelides",
            "Logan Schneider",
            "Isaac R. Galatzer-Levy",
            "Yugang Jia",
            "John Canny",
            "Arthur Gretton",
            "Maja Matari´c",
            "Google Deepmind",
            "Verily Life Sciences",
            "Google",
            "Kristine Arges",
            "T. Assimes",
            "Vikram Bajaj",
            "Suresh Balu",
            "Mustafa R. Bashir",
            "Laura Beskow",
            "Ros-alia Blanco",
            "R. Califf",
            "Paul Campbell",
            "Larry Carin",
            "Victoria Christian",
            "Scott Cousins",
            "Millie Das",
            "M. Dockery",
            "Pamela S. Douglas",
            "Ashley Dunham",
            "Julie Eckstrand",
            "Dominik Fleischmann",
            "Emily Ford",
            "Elizabeth S. Fraulo",
            "John French",
            "S. Sanjiv",
            "Ge-offrey S Gambhir",
            "Robert C Ginsburg",
            "Francois Green",
            "Haddad Adrian",
            "John Hernandez",
            "Erich S Hernandez",
            "Huang Glenn",
            "Daniel Jaffe",
            "Lynne H King",
            "Curtis Koweek",
            "Yaping J Langlotz",
            "Kenneth W Liao",
            "Kelly Mahaffey",
            "William J Marcom",
            "J. D. Marks",
            "Reid Maron",
            "Shannon McCabe",
            "Rebecca McCall",
            "Jessica McCue",
            "David Mega",
            "Lawrence H Miller",
            "Rajan Muhlbaier",
            "Munshi",
            "Kristin Newby",
            "Bray Ezra Pak-Harvey",
            "Michael Pencina",
            "Eric D. Peterson",
            "Fa-tima Rodriguez",
            "Scarlet Shore",
            "Svati H Shah",
            "Steven Shipes",
            "G. Sledge",
            "Susie Spielman",
            "Ryan Spitler",
            "T. Schaack",
            "Geeta Swamy",
            "M. Willemink",
            "Charlene A Wong. 2020",
            "Yuntao Bai",
            "Saurav Kadavath",
            "Sandipan Kundu",
            "Amanda Askell",
            "John Kernion",
            "Andy Jones",
            "Anna Chen",
            "Anna Goldie",
            "Azalia Mirhoseini",
            "C. McKinnon",
            "Carol Chen",
            "Catherine Olsson",
            "Chris Olah",
            "Danny Hernandez",
            "Dawn Drain",
            "Deep Ganguli",
            "Dustin Li",
            "Eli Tran-Johnson",
            "Ethan Perez",
            "Jamie Kerr",
            "J. Mueller",
            "Jeffrey Ladish",
            "Joshua Landau",
            "Kamal Ndousse",
            "Kamile Lukosuite",
            "Liane Lovitt",
            "M. Sellitto",
            "Nelson Elhage",
            "Nicholas Schiefer",
            "Noem'i Mercado",
            "Nova Dassarma",
            "R. Lasenby",
            "Robin Larson",
            "Sam Ringer",
            "Scott John-ston",
            "Sheer Shauna Kravec",
            "El Showk",
            "Stanislav Fort",
            "Tamera Lanham",
            "Timothy Telleen-Lawton",
            "T. Henighan",
            "Tristan Hume",
            "Samuel R. Bow-man",
            "Zac Hatfield-Dodds",
            "Benjamin Mann",
            "Dario Amodei"
        ],
        "arxiv_id": "2502.13135",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users'needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",
        "abstract_summary_gcp": "This paper presents an end-to-end framework for generating realistic synthetic users to evaluate interactive agents designed for positive behavior change, specifically in health and lifestyle coaching (e.g., sleep, diabetes management).\n\nThe framework involves a two-stage process:\n1.  **Structured Data Generation:** Creating basic demographics, behavioral attributes, and health/lifestyle factors grounded in real-world data.\n2.  **Full Profile Development:** Generating comprehensive user profiles conditioned on this structured data.\n\nThese synthetic users interact with coaching agents through simulations using generative agent-based models (like Concordia) or direct language model prompting. The framework's validity was demonstrated using sleep and diabetes coaching agents, showing the agents' understanding of user needs. Crucially, human expert evaluations confirmed that these health- and behavior-grounded synthetic users more accurately portray real human users compared to generic synthetic users.\n\nThis approach facilitates extensive, realistic, and grounded simulated interactions, thereby supporting the efficient development of conversational agents.",
        "url": "https://www.semanticscholar.org/paper/e37151b321c6014be639fe784cef87a505eedb01",
        "isOpenAccess": false
    },
    "2502.12561": {
        "title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Laurence Li",
            "Jiri Gesi",
            "Qi He",
            "T. Li",
            "Dakuo Wang"
        ],
        "arxiv_id": "2502.12561",
        "venue": "CHI Extended Abstracts",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Book"
        ],
        "citationCount": 22,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Usability testing is a fundamental yet challenging research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features an LLM Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The system can generate UX study results in qualitative (e.g., interviewing how an agent thinks), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of UX study with LLM Agents1.",
        "abstract_summary_gcp": "UXAgent is a novel system designed to help UX researchers evaluate and refine their usability testing study designs *before* conducting real human-subject studies. Inspired by advancements in Large Language Model-simulated Agent (LLM Agent) research, UXAgent incorporates an LLM Agent module and a universal browser connector. This allows it to automatically generate thousands of simulated users to test a target website, providing comprehensive study results in qualitative (e.g., interviewing agent thoughts), quantitative (e.g., number of actions), and video formats. While a heuristic evaluation with five UX researchers praised the system's innovation, concerns were also raised regarding the future implications of LLM Agents in UX research.",
        "url": "https://www.semanticscholar.org/paper/8785574ebc4b297b75f19f3ddfaa0fb2bd4f055d",
        "isOpenAccess": false
    },
    "2502.12109": {
        "title": "Personality Structured Interview for Large Language Model Simulation in Personality Research",
        "authors": [
            "Pengda Wang",
            "Huiqi Zou",
            "Hanjie Chen",
            "Tianjun Sun",
            "Ziang Xiao",
            "Frederick L. Oswald"
        ],
        "arxiv_id": "2502.12109",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.",
        "abstract_summary_gcp": "Large language models (LLMs), despite being explored as human proxies in psychometrics, struggle to generate diverse, human-like data, limiting their value in social science. To overcome this, researchers propose the theory-informed Personality Structured Interview (PSI) approach. This method grounds LLM simulations in a dataset of 357 real-human interview transcripts, each comprising responses to 32 open-ended, theory-based questions designed to elicit personality evidence. The authors also developed an evaluation framework to validate LLM-generated psychometric data. Experimental results demonstrate that using well-designed structured interviews significantly enhances the human-like heterogeneity of LLM-simulated personality data. Furthermore, this improved data can predict real-world personality-related behavioral outcomes like organizational citizenship behaviors and counterproductive work behavior. The study highlights the crucial role of theory-informed structured interviews in advancing LLM-based simulations and provides a general framework for designing such interviews to generate more human-like psychometric data.",
        "url": "https://www.semanticscholar.org/paper/86ea11b4e3a56a567bc93f8bd8d6125af0894323",
        "isOpenAccess": false
    },
    "Large language models that replace human participants can harmfully misportray and flatten identity groups": {
        "title": "Large language models that replace human participants can harmfully misportray and flatten identity groups",
        "authors": [
            "Angelina Wang",
            "Jamie Morgenstern",
            "John Dickerson"
        ],
        "arxiv_id": null,
        "venue": "Nature Machine Intelligence",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 21,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/7954aa868a2480c5ac077064d024a7610e21a842",
        "isOpenAccess": true
    },
    "2502.10308": {
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "authors": [
            "Ermis Soumalias",
            "Yanchen Jiang",
            "Kehang Zhu",
            "Michael J. Curry",
            "Sven Seuken",
            "David C. Parkes"
        ],
        "arxiv_id": "2502.10308",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.",
        "abstract_summary_gcp": "This paper investigates using Large Language Models (LLMs) as proxies for human users to simplify preference elicitation (PE) in combinatorial assignment tasks. Unlike traditional iterative PE methods that require significant human effort, LLMs offer a more efficient, one-shot approach.\n\nThe authors propose a framework that integrates LLM proxies with existing state-of-the-art ML-powered PE schemes, specifically designed to handle challenges unique to LLMs, such as response variability and increased computational costs.\n\nThrough experimental evaluation in the course allocation domain, the study found that this LLM-proxy approach significantly improved allocative efficiency by up to 20% compared to using human queries. These positive results were consistent across different LLMs and robust to variations in the quality and accuracy of the LLMs' reported preferences. The research also explores the specific model capabilities necessary for this approach to succeed.",
        "url": "https://www.semanticscholar.org/paper/d48d84f7c8b1f0e3079123f40d1f2a6409378ed0",
        "isOpenAccess": false
    },
    "2502.08691": {
        "title": "AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society",
        "authors": [
            "J. Piao",
            "Yuwei Yan",
            "Jun Zhang",
            "Nian Li",
            "Junbo Yan",
            "Xiaochong Lan",
            "Zhihong Lu",
            "Zhiheng Zheng",
            "Jing Yi Wang",
            "Di Zhou",
            "Chen Gao",
            "Fengli Xu",
            "Fang Zhang",
            "Ke Rong",
            "Jun Su",
            "Yong Li"
        ],
        "arxiv_id": "2502.08691",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 80,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.",
        "abstract_summary_gcp": "This paper introduces **AgentSociety**, a large-scale social simulator designed to advance generative social science by leveraging recent breakthroughs in large language models (LLMs).\n\nAgentSociety integrates LLM-driven agents, a realistic societal environment, and a powerful simulation engine to create human-like social dynamics. The researchers simulated the social lives and over 5 million interactions of more than 10,000 agents, using this platform as a testbed for computational social experiments. They investigated four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks.\n\nBy supporting typical social science research methods (e.g., surveys, interviews, interventions) and demonstrating outcomes that align with real-world experimental results, AgentSociety proves its capability to capture human behaviors and underlying mechanisms. This positions it as a valuable tool for social scientists and policymakers.",
        "url": "https://www.semanticscholar.org/paper/5cfbaa9828d1cf447aaf9cb39f8ccadf3fd1de88",
        "isOpenAccess": false
    },
    "2502.02649": {
        "title": "Fully Autonomous AI Agents Should Not be Developed",
        "authors": [
            "Margaret Mitchell",
            "Avijit Ghosh",
            "A. Luccioni",
            "Giada Pistilli"
        ],
        "arxiv_id": "2502.02649",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 29,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values.",
        "abstract_summary_gcp": "This paper argues against the development of fully autonomous AI agents. The authors support this by analyzing different levels of AI agents, drawing insights from scientific literature and current product marketing to delineate ethical values, benefits, and risks at each stage. Their analysis concludes that risks to people increase proportionally with a system's autonomy; the more control a user gives to an AI, the greater the potential risks, with safety risks impacting human life being of particular concern.",
        "url": "https://www.semanticscholar.org/paper/b2bccc03f0476228e3fb9f2c0f3b2d4cebb82d25",
        "isOpenAccess": false
    },
    "2503.04735": {
        "title": "How Personality Traits Shape LLM Risk-Taking Behaviour",
        "authors": [
            "John Hartley",
            "Conor Hamill",
            "Devesh Batra",
            "Dale Seddon",
            "Ramin Okhrati",
            "Raad Khraishi"
        ],
        "arxiv_id": "2503.04735",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.",
        "abstract_summary_gcp": "This study investigates the decision-making of Large Language Models (LLMs), specifically GPT-4o, under risk, linking their Big Five personality traits to risk propensity using Cumulative Prospect Theory (CPT).\n\nKey findings include:\n*   GPT-4o exhibits higher Conscientiousness and Agreeableness compared to human averages.\n*   It operates as a risk-neutral rational agent in prospect selection.\n*   Interventions to alter GPT-4o's Big Five traits, particularly Openness, significantly influenced its risk propensity, a pattern consistent with human behavior.\n*   Openness emerged as the most influential personality trait affecting GPT-4o's risk-taking.\n*   Older models like GPT-4-Turbo failed to consistently generalize this personality-risk relationship.\n\nThe research enhances our understanding of LLM behavior under risk and explores the potential and limitations of using personality-based interventions to shape AI decision-making, with implications for developing more robust AI systems, such as in financial modeling.",
        "url": "https://www.semanticscholar.org/paper/f80e3db3dfb657d926c109e966cc5b63f78bd9d2",
        "isOpenAccess": false
    },
    "2501.15283": {
        "title": "Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions",
        "authors": [
            "Naihao Deng",
            "Rada Mihalcea"
        ],
        "arxiv_id": "2501.15283",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "As Large Language Models (LLMs) advance in their capabilities, researchers have increasingly employed them for social simulation. In this paper, we investigate whether interactions among LLM agents resemble those of humans. Specifically, we focus on the pronoun usage difference between leaders and non-leaders, examining whether the simulation would lead to human-like pronoun usage patterns during the LLMs' interactions. Our evaluation reveals the significant discrepancies between LLM-based simulations and human pronoun usage, with prompt-based or specialized agents failing to demonstrate human-like pronoun usage patterns. In addition, we reveal that even if LLMs understand the human pronoun usage patterns, they fail to demonstrate them in the actual interaction process. Our study highlights the limitations of social simulations based on LLM agents, urging caution in using such social simulation in practitioners' decision-making process.",
        "abstract_summary_gcp": "This paper investigates whether interactions among Large Language Model (LLM) agents in social simulations mirror human behavior, specifically focusing on pronoun usage differences between leaders and non-leaders. The study found significant discrepancies: LLM-based simulations, including those with prompt-based or specialized agents, failed to demonstrate human-like pronoun usage patterns. Furthermore, even when LLMs understood these human patterns, they did not exhibit them during actual interactions. The authors conclude that these findings highlight serious limitations in LLM-based social simulations, urging caution for practitioners using them for decision-making.",
        "url": "https://www.semanticscholar.org/paper/2d37ab3deeb1a4db184313e079d88580bdced1e6",
        "isOpenAccess": false
    },
    "2501.11639": {
        "title": "StAyaL | Multilingual Style Transfer",
        "authors": [
            "Karishma Thakrar",
            "Katrina Lawrence",
            "Kyle Howard"
        ],
        "arxiv_id": "2501.11639",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Stylistic text generation plays a vital role in enhancing communication by reflecting the nuances of individual expression. This paper presents a novel approach for generating text in a specific speaker's style across different languages. We show that by leveraging only 100 lines of text, an individuals unique style can be captured as a high-dimensional embedding, which can be used for both text generation and stylistic translation. This methodology breaks down the language barrier by transferring the style of a speaker between languages. The paper is structured into three main phases: augmenting the speaker's data with stylistically consistent external sources, separating style from content using machine learning and deep learning techniques, and generating an abstract style profile by mean pooling the learned embeddings. The proposed approach is shown to be topic-agnostic, with test accuracy and F1 scores of 74.9% and 0.75, respectively. The results demonstrate the potential of the style profile for multilingual communication, paving the way for further applications in personalized content generation and cross-linguistic stylistic transfer.",
        "abstract_summary_gcp": "This paper introduces a novel approach to stylistic text generation, capable of capturing an individual's unique writing style from just 100 lines of text. This style is represented as a high-dimensional embedding, which can then be used for both generating new text in that specific style and performing stylistic translation across different languages, thus overcoming language barriers. The methodology involves three key phases: augmenting speaker data, separating style from content using machine learning and deep learning, and creating an abstract style profile through mean pooling of learned embeddings. The proposed system is topic-agnostic, demonstrated by test accuracy and F1 scores of 74.9% and 0.75. These results underscore the significant potential of this style profiling method for improving multilingual communication, personalizing content, and facilitating cross-linguistic style transfer.",
        "url": "https://www.semanticscholar.org/paper/6bb1ebc7809fd95f21e38b8afb93fd054316821b",
        "isOpenAccess": false
    },
    "2501.08579": {
        "title": "LLM-based Human Simulations Have Not Yet Been Reliable",
        "authors": [
            "Qian Wang",
            "Jiaying Wu",
            "Zichen Jiang",
            "Zhenheng Tang",
            "B. Luo",
            "Nuo Chen",
            "Wei Chen",
            "Bingsheng He"
        ],
        "arxiv_id": "2501.08579",
        "venue": "",
        "year": 2025,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 23,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly employed for simulating human behaviors across diverse domains. However, our position is that current LLM-based human simulations remain insufficiently reliable, as evidenced by significant discrepancies between their outcomes and authentic human actions. Our investigation begins with a systematic review of LLM-based human simulations in social, economic, policy, and psychological contexts, identifying their common frameworks, recent advances, and persistent limitations. This review reveals that such discrepancies primarily stem from inherent limitations of LLMs and flaws in simulation design, both of which are examined in detail. Building on these insights, we propose a systematic solution framework that emphasizes enriching data foundations, advancing LLM capabilities, and ensuring robust simulation design to enhance reliability. Finally, we introduce a structured algorithm that operationalizes the proposed framework, aiming to guide credible and human-aligned LLM-based simulations. To facilitate further research, we provide a curated list of related literature and resources at https://github.com/Persdre/awesome-llm-human-simulation.",
        "abstract_summary_gcp": "This paper argues that current Large Language Model (LLM)-based human simulations are unreliable, exhibiting significant discrepancies from authentic human behaviors. A systematic review of these simulations across social, economic, policy, and psychological contexts reveals that these inconsistencies stem from inherent LLM limitations and flawed simulation design. To address this, the authors propose a systematic solution framework focused on enriching data foundations, advancing LLM capabilities, and ensuring robust simulation design for enhanced reliability. They further introduce a structured algorithm to operationalize this framework, aiming to guide credible and human-aligned simulations, and provide a curated list of related resources.",
        "url": "https://www.semanticscholar.org/paper/c3c872de390f9b44f550cbc38d522d621c66ec92",
        "isOpenAccess": false
    },
    "2501.02348": {
        "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving",
        "authors": [
            "Sanghyun Park",
            "Boris Maciejovsky",
            "P. Puranam"
        ],
        "arxiv_id": "2501.02348",
        "venue": "Social Science Research Network",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 2,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the\"wisdom of crowds\"within a single individual, allowing them to\"think with many minds.\"While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.",
        "abstract_summary_gcp": "Complex problem-solving necessitates cognitive flexibility—the ability to entertain multiple distinct perspectives, akin to an internal \"wisdom of crowds.\" While mental simulation aids deliberation, its effectiveness is limited by cognitive constraints.\n\nThe paper proposes **synthetic deliberation**, an LLM-based method that simulates discourse among AI agents embodying diverse perspectives. Using a custom GPT model, this approach offers several benefits:\n1.  **Concurrent processing** of multiple viewpoints without cognitive degradation.\n2.  **Parallel exploration** of perspectives.\n3.  **Precise control** over viewpoint synthesis.\n\nBy externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation overcomes the limitations of mental simulation, holding promise for strategic planning, policymaking, and conflict resolution.",
        "url": "https://www.semanticscholar.org/paper/485f233043206ac40ac942ca6c0956f9998f6540",
        "isOpenAccess": true
    },
    "2412.21102": {
        "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
        "authors": [
            "Kuanchao Chu",
            "Yi-Pei Chen",
            "Hideki Nakayama"
        ],
        "arxiv_id": "2412.21102",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 7,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Controlling diversity in LLM-agent simulations is essential for balancing stability in structured tasks with variability in open-ended interactions. However, we observe that dialogue diversity tends to degrade over long-term simulations. To explore the role of prompt design in this phenomenon, we modularized the utterance generation prompt and found that reducing contextual information leads to more diverse outputs. Based on this insight, we propose Adaptive Prompt Pruning (APP), a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods. We demonstrate that APP effectively modulates diversity through extensive experiments and propose a method to balance the control trade-offs. Our analysis reveals that all prompt components impose constraints on diversity, with the Memory being the most influential. Additionally, high-attention contents consistently suppress output diversity.",
        "abstract_summary_gcp": "This paper addresses the degradation of dialogue diversity in long-term LLM-agent simulations, despite its importance for balancing stability and variability. The authors found that **reducing contextual information within modularized utterance generation prompts leads to increased output diversity**.\n\nBased on this insight, they propose **Adaptive Prompt Pruning (APP)**, a novel method that allows users to control diversity via a single parameter, lambda. APP dynamically prunes prompt segments based on attention scores and is compatible with existing diversity control methods.\n\nExperiments demonstrate APP's effectiveness in modulating diversity. The analysis reveals that **all prompt components constrain diversity, with Memory being the most influential**, and that **high-attention contents consistently suppress output diversity**.",
        "url": "https://www.semanticscholar.org/paper/17aa2065df31c6c0315272da857446c46341d3a6",
        "isOpenAccess": false
    },
    "2412.19726": {
        "title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
        "authors": [
            "Matthew Riemer",
            "Zahra Ashktorab",
            "Djallel Bouneffouf",
            "Payel Das",
            "Miao Liu",
            "Justin D. Weisz",
            "Murray Campbell"
        ],
        "arxiv_id": "2412.19726",
        "venue": "",
        "year": 2024,
        "publicationTypes": null,
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Our paper argues that the majority of theory of mind benchmarks are broken because of their inability to directly test how large language models (LLMs) adapt to new partners. This problem stems from the fact that theory of mind benchmarks for LLMs are overwhelmingly inspired by the methods used to test theory of mind in humans and fall victim to a fallacy of attributing human-like qualities to AI agents. We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs. Most theory of mind benchmarks only measure what we call literal theory of mind: the ability to predict the behavior of others. However, this type of metric is only informative when agents exhibit self-consistent reasoning. Thus, we introduce the concept of functional theory of mind: the ability to adapt to agents in-context following a rational response to their behavior. We find that many open source LLMs are capable of displaying strong literal theory of mind capabilities, but seem to struggle with functional theory of mind -- even with exceedingly simple partner policies. Simply put, strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance or vice versa. Achieving functional theory of mind, particularly over long interaction horizons with a partner, is a significant challenge deserving a prominent role in any meaningful LLM theory of mind evaluation.",
        "abstract_summary_gcp": "This paper argues that most existing theory of mind (ToM) benchmarks for large language models (LLMs) are flawed because they fail to directly assess an LLM's ability to **adapt to new partners**. This flaw stems from benchmarks being overly inspired by human ToM tests, which mistakenly assumes LLMs possess human-like, *consistent reasoning* — a quality known to be absent in current LLMs.\n\nThe authors distinguish between two types of ToM:\n1.  **Literal Theory of Mind:** The ability to *predict* others' behavior (what most current benchmarks measure). This is only informative if agents exhibit consistent reasoning.\n2.  **Functional Theory of Mind:** The ability to *adapt* to agents in-context by responding rationally to their behavior.\n\nThey find that while many open-source LLMs demonstrate strong literal ToM capabilities, they significantly struggle with functional ToM, even when dealing with simple partner policies. The paper concludes that strong literal ToM performance does not guarantee strong functional ToM, and that achieving functional ToM, especially over extended interactions, is a crucial and under-evaluated challenge for meaningful LLM assessment.",
        "url": "https://www.semanticscholar.org/paper/e56286e5c49fda39f6f122615d073418b8fe74d5",
        "isOpenAccess": false
    },
    "2412.15291": {
        "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in Political Science",
        "authors": [
            "Chenxiao Yu",
            "Jin Ye",
            "Yuangang Li",
            "Zheng Li",
            "Emilio Ferrara",
            "Xiyang Hu",
            "Yue Zhao"
        ],
        "arxiv_id": "2412.15291",
        "venue": "",
        "year": 2024,
        "publicationTypes": null,
        "citationCount": 3,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "While LLMs have demonstrated remarkable capabilities in text generation and reasoning, their ability to simulate human decision-making -- particularly in political contexts -- remains an open question. However, modeling voter behavior presents unique challenges due to limited voter-level data, evolving political landscapes, and the complexity of human reasoning. In this study, we develop a theory-driven, multi-step reasoning framework that integrates demographic, temporal and ideological factors to simulate voter decision-making at scale. Using synthetic personas calibrated to real-world voter data, we conduct large-scale simulations of recent U.S. presidential elections. Our method significantly improves simulation accuracy while mitigating model biases. We examine its robustness by comparing performance across different LLMs. We further investigate the challenges and constraints that arise from LLM-based political simulations. Our work provides both a scalable framework for modeling political decision-making behavior and insights into the promise and limitations of using LLMs in political science research.",
        "abstract_summary_gcp": "This study addresses the challenge of simulating human political decision-making, particularly voter behavior, using LLMs, which is complicated by data limitations and complex human reasoning.\n\nThe researchers developed a novel, theory-driven, multi-step reasoning framework that incorporates demographic, temporal, and ideological factors. Using synthetic personas calibrated to real-world voter data, they conducted large-scale simulations of recent U.S. presidential elections.\n\nTheir method significantly improved simulation accuracy and mitigated model biases. The study also examined the framework's robustness across different LLMs and investigated the inherent challenges and constraints of using LLMs for political simulations. Ultimately, this work offers a scalable framework for modeling political decision-making and provides valuable insights into the promise and limitations of LLMs in political science research.",
        "url": "https://www.semanticscholar.org/paper/66d89814faca6ca8209cf23ca4ce64e91ab5d035",
        "isOpenAccess": false
    },
    "2412.13169": {
        "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study",
        "authors": [
            "Bolei Ma",
            "Berk Yoztyurk",
            "Anna Haensch",
            "Xinpeng Wang",
            "Markus Herklotz",
            "Frauke Kreuter",
            "Barbara Plank",
            "Matthias Assenmacher"
        ],
        "arxiv_id": "2412.13169",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle",
            "Conference",
            "Review"
        ],
        "citationCount": 8,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In recent research, large language models (LLMs) have been increasingly used to investigate public opinions. This study investigates the algorithmic fidelity of LLMs, i.e., the ability to replicate the socio-cultural context and nuanced opinions of human participants. Using open-ended survey data from the German Longitudinal Election Studies (GLES), we prompt different LLMs to generate synthetic public opinions reflective of German subpopulations by incorporating demographic features into the persona prompts. Our results show that Llama performs better than other LLMs at representing subpopulations, particularly when there is lower opinion diversity within those groups. Our findings further reveal that the LLM performs better for supporters of left-leaning parties like The Greens and The Left compared to other parties, and matches the least with the right-party AfD. Additionally, the inclusion or exclusion of specific variables in the prompts can significantly impact the models' predictions. These findings underscore the importance of aligning LLMs to more effectively model diverse public opinions while minimizing political biases and enhancing robustness in representativeness.",
        "abstract_summary_gcp": "This study investigated the \"algorithmic fidelity\" of Large Language Models (LLMs)—their capacity to accurately replicate diverse human socio-cultural contexts and nuanced public opinions. Using open-ended survey data from the German Longitudinal Election Studies (GLES), researchers prompted various LLMs to generate synthetic opinions for German subpopulations, incorporating demographic features into persona prompts.\n\nThe findings reveal that Llama performed best among the tested LLMs, particularly when representing groups with lower opinion diversity. However, a significant political bias was observed: LLMs showed better fidelity for supporters of left-leaning parties (The Greens, The Left) but performed least accurately for the right-wing AfD. The study also highlighted that the specific variables included or excluded from prompts significantly impact the models' predictions.\n\nThese results underscore the critical need to align LLMs to more effectively model diverse public opinions, minimize inherent political biases, and enhance their overall representativeness.",
        "url": "https://www.semanticscholar.org/paper/17d898ea659dccedd58e0d124cf47f00a77e450a",
        "isOpenAccess": false
    },
    "2412.06681": {
        "title": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework",
        "authors": [
            "Tianming Liu",
            "Jirong Yang",
            "Yafeng Yin"
        ],
        "arxiv_id": "2412.06681",
        "venue": "Artificial Intelligence for Transportation",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 18,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In transportation system demand modeling and simulation, agent-based models and microsimulations are current state-of-the-art approaches. However, existing agent-based models still have some limitations on behavioral realism and resource demand that limit their applicability. In this study, leveraging the emerging technology of large language models (LLMs) and LLM-based agents, we propose a general LLM-agent-based modeling framework for transportation systems. We argue that LLM agents not only possess the essential capabilities to function as agents but also offer promising solutions to overcome some limitations of existing agent-based models. Our conceptual framework design closely replicates the decision-making and interaction processes and traits of human travelers within transportation networks, and we demonstrate that the proposed systems can meet critical behavioral criteria for decision-making and learning behaviors using related studies and a demonstrative example of LLM agents' learning and adjustment in the bottleneck setting. Although further refinement of the LLM-agent-based modeling framework is necessary, we believe that this approach has the potential to improve transportation system modeling and simulation.",
        "abstract_summary_gcp": "Current agent-based models (ABMs) and microsimulations, while state-of-the-art for transportation demand modeling, have limitations in behavioral realism and computational resource demands. This paper proposes a general LLM-agent-based modeling framework, leveraging large language models (LLMs) to create agents that can overcome these limitations. The framework is designed to closely replicate human traveler decision-making and interaction within transportation networks, demonstrating essential behavioral capabilities like learning and adjustment (e.g., in a bottleneck scenario). Although further refinement is needed, this approach is presented as a promising way to significantly improve transportation system modeling and simulation.",
        "url": "https://www.semanticscholar.org/paper/3ac79707cb574c98b52bbac77b9d585f19bdf16d",
        "isOpenAccess": false
    },
    "2412.03563": {
        "title": "From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents",
        "authors": [
            "Xinyi Mou",
            "Xuanwen Ding",
            "Qi He",
            "Liang Wang",
            "Jingcong Liang",
            "Xinnong Zhang",
            "Libo Sun",
            "Jiayu Lin",
            "Jie Zhou",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "arxiv_id": "2412.03563",
        "venue": "arXiv.org",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 49,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies. In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents. We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics. These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena. We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method. Afterward, we summarize commonly used datasets and benchmarks. Finally, we discuss the trends across these three types of simulation. A repository for the related sources is at {\\url{https://github.com/FudanDISC/SocialAgent}}.",
        "abstract_summary_gcp": "This paper surveys the emerging field of using Large Language Models (LLMs) to simulate human behavior, offering a scalable and ethically conscious alternative to traditional, human-participant-based sociological research.\n\nThe survey categorizes LLM-empowered agent simulations into three progressive types:\n\n1.  **Individual Simulation:** Mimicking specific individuals or demographic groups.\n2.  **Scenario Simulation:** Involving multiple agents collaborating to achieve goals within defined contexts.\n3.  **Society Simulation:** Modeling complex interactions within agent societies to reflect real-world dynamics.\n\nFor each category, the survey delves into the simulation's architecture, objectives, and evaluation methodologies. The paper also compiles commonly used datasets and benchmarks, concluding with a discussion of overarching trends in LLM-driven social simulations. A related GitHub repository is available for further resources.",
        "url": "https://www.semanticscholar.org/paper/11a6d66791e244b01bf1a23a98158be789854876",
        "isOpenAccess": false
    },
    "2411.11581": {
        "title": "OASIS: Open Agent Social Interaction Simulations with One Million Agents",
        "authors": [
            "Ziyi Yang",
            "Zaibin Zhang",
            "Zirui Zheng",
            "Yuxian Jiang",
            "Ziyue Gan",
            "Zhiyu Wang",
            "Zijian Ling",
            "Jinsong Chen",
            "Martz Ma",
            "Bowen Dong",
            "Prateek Gupta",
            "Shuyue Hu",
            "Zhenfei Yin",
            "G. Li",
            "Xu Jia",
            "Lijun Wang",
            "Bernard Ghanem",
            "Huchuan Lu",
            "Wanli Ouyang",
            "Yu Qiao",
            "Philip Torr",
            "Jing Shao"
        ],
        "arxiv_id": "2411.11581",
        "venue": "arXiv.org",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 55,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "There has been a growing interest in enhancing rule-based agent-based models (ABMs) for social media platforms (i.e., X, Reddit) with more realistic large language model (LLM) agents, thereby allowing for a more nuanced study of complex systems. As a result, several LLM-based ABMs have been proposed in the past year. While they hold promise, each simulator is specifically designed to study a particular scenario, making it time-consuming and resource-intensive to explore other phenomena using the same ABM. Additionally, these models simulate only a limited number of agents, whereas real-world social media platforms involve millions of users. To this end, we propose OASIS, a generalizable and scalable social media simulator. OASIS is designed based on real-world social media platforms, incorporating dynamically updated environments (i.e., dynamic social networks and post information), diverse action spaces (i.e., following, commenting), and recommendation systems (i.e., interest-based and hot-score-based). Additionally, OASIS supports large-scale user simulations, capable of modeling up to one million users. With these features, OASIS can be easily extended to different social media platforms to study large-scale group phenomena and behaviors. We replicate various social phenomena, including information spreading, group polarization, and herd effects across X and Reddit platforms. Moreover, we provide observations of social phenomena at different agent group scales. We observe that the larger agent group scale leads to more enhanced group dynamics and more diverse and helpful agents' opinions. These findings demonstrate OASIS's potential as a powerful tool for studying complex systems in digital environments.",
        "abstract_summary_gcp": "Current LLM-based agent-based models (ABMs) for social media, while promising for studying complex systems, are limited by their scenario-specific design and inability to simulate large numbers of users. To overcome these issues, the authors propose **OASIS**, a novel, generalizable, and scalable social media simulator.\n\nOASIS is designed to realistically emulate platforms like X and Reddit, featuring dynamic social networks, evolving post information, diverse user actions (e.g., following, commenting), and integrated recommendation systems. A key strength is its scalability, capable of modeling up to one million users.\n\nThe simulator's capabilities are demonstrated by replicating various social phenomena—including information spreading, group polarization, and herd effects—across both X and Reddit platforms. The research also highlights that larger agent group scales within OASIS lead to more enhanced group dynamics and diverse agent opinions. This positions OASIS as a powerful tool for studying complex social phenomena in digital environments.",
        "url": "https://www.semanticscholar.org/paper/9a6206f4ea5a809132c1df7697c37a53b88aeff3",
        "isOpenAccess": false
    },
    "2410.07991": {
        "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets",
        "authors": [
            "Tommaso Giorgi",
            "Lorenzo Cima",
            "T. Fagni",
            "M. Avvenuti",
            "S. Cresci"
        ],
        "arxiv_id": "2410.07991",
        "venue": "International Conference on Web and Social Media",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 25,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.",
        "abstract_summary_gcp": "This research addresses the critical issue of bias in human-labeled data for hate speech detection, a necessity given the rise of online hate speech. It specifically focuses on an unexplored gap: the interplay between annotator characteristics and the attributes of the hate speech target.\n\nLeveraging a comprehensive dataset with rich socio-demographic information for both annotators and targets, the study quantitatively describes widespread human biases in hate speech annotations, detailing their intensity and prevalence. Furthermore, it compares these human biases to those found in persona-based Large Language Models (LLMs), revealing that while LLMs exhibit biases, they are distinctly different from human biases.\n\nThe work provides novel insights into human annotation biases and offers valuable guidance for designing more effective AI-driven hate speech detection systems.",
        "url": "https://www.semanticscholar.org/paper/59da345363ed87d0921ed14c07d0c8b7b9f96130",
        "isOpenAccess": false
    },
    "2410.07553": {
        "title": "COMMA: A Communicative Multimodal Multi-Agent Benchmark",
        "authors": [
            "Timothy Ossowski",
            "Jixuan Chen",
            "Danyal Maqbool",
            "Zefan Cai",
            "Tyler J. Bradshaw",
            "Junjie Hu"
        ],
        "arxiv_id": "2410.07553",
        "venue": "Trans. Mach. Learn. Res.",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 6,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.",
        "abstract_summary_gcp": "The rapid development of multimodal agents has largely neglected their ability for language-based communication between agents in collaborative tasks, posing a critical gap for real-world deployments and human interaction. Existing benchmarks are insufficient, failing to test scenarios involving unequal information access and complex collaboration.\n\nTo address this, the authors introduce **COMMA**, a novel benchmark that uses multimodal puzzles to evaluate the collaborative performance of multimodal multi-agent systems through language communication, assessing four key categories of agentic capability.\n\nFindings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like LLaVA-CoT. Many chain-of-thought models struggle to outperform a random baseline in agent-agent collaboration, highlighting a significant area for improvement in their communication and collaborative abilities.",
        "url": "https://www.semanticscholar.org/paper/f99b34a8733077c7c64632b5bb30361c6da9bbe1",
        "isOpenAccess": false
    },
    "2409.02601": {
        "title": "ChatGPT vs Social Surveys: Probing Objective and Subjective Silicon Population",
        "authors": [
            "Muzhi Zhou",
            "Lu Yu",
            "Xiaomin Geng",
            "Lan Luo"
        ],
        "arxiv_id": "2409.02601",
        "venue": "",
        "year": 2024,
        "publicationTypes": [
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent discussions about Large Language Models (LLMs) indicate that they have the potential to simulate human responses in social surveys and generate reliable predictions, such as those found in political polls. However, the existing findings are highly inconsistent, leaving us uncertain about the population characteristics of data generated by LLMs. In this paper, we employ repeated random sampling to create sampling distributions that identify the population parameters of silicon samples generated by GPT. Our findings show that GPT's demographic distribution aligns with the 2020 U.S. population in terms of gender and average age. However, GPT significantly overestimates the representation of the Black population and individuals with higher levels of education, even when it possesses accurate knowledge. Furthermore, GPT's point estimates for attitudinal scores are highly inconsistent and show no clear inclination toward any particular ideology. The sample response distributions exhibit a normal pattern that diverges significantly from those of human respondents. Consistent with previous studies, we find that GPT's answers are more deterministic than those of humans. We conclude by discussing the concerning implications of this biased and deterministic silicon population for making inferences about real-world populations.",
        "abstract_summary_gcp": "This paper investigates the population characteristics of data generated by Large Language Models (LLMs), specifically GPT, in the context of simulating human responses for social surveys and political polls. Despite the potential for LLMs to generate reliable predictions, prior findings have been inconsistent.\n\nUsing repeated random sampling to create \"silicon samples,\" the study found:\n\n1.  **Demographics:** GPT's simulated population aligns with the 2020 U.S. population in terms of gender and average age. However, it significantly **overestimates the representation of the Black population and individuals with higher levels of education.**\n2.  **Attitudes:** GPT's attitudinal scores were **highly inconsistent** and showed no clear ideological inclination.\n3.  **Response Patterns:** The sample response distributions exhibited a **normal pattern that significantly diverged from human respondents.**\n4.  **Determinism:** Consistent with previous research, GPT's answers were found to be **more deterministic than those of humans.**\n\nThe paper concludes that this \"biased and deterministic silicon population\" raises concerning implications for accurately inferring real-world human populations from LLM-generated data.",
        "url": "https://www.semanticscholar.org/paper/8a576c2dbb55ae83e114de4f7bfd221d4542deed",
        "isOpenAccess": false
    },
    "2408.16073": {
        "title": "Using Large Language Models to Create AI Personas for Replication and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings",
        "authors": [
            "Leo Yeykelis",
            "Kaavya Pichai",
            "James J. Cummings",
            "Byron Reeves"
        ],
        "arxiv_id": "2408.16073",
        "venue": "arXiv.org",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "This report analyzes the potential for large language models (LLMs) to expedite accurate replication and generalization of published research about message effects in marketing. LLM-powered participants (personas) were tested by replicating 133 experimental findings from 14 papers containing 45 recent studies published in the Journal of Marketing. For each study, the measures, stimuli, and sampling specifications were used to generate prompts for LLMs to act as unique personas. The AI personas, 19,447 in total across all of the studies, generated complete datasets and statistical analyses were then compared with the original human study results. The LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication. The overall replication rate including interaction effects was 68% (90 out of 133). Furthermore, a test of how human results generalized to different participant samples, media stimuli, and measures showed that replication results can change when tests go beyond the parameters of the original human studies. Implications are discussed for the replication and generalizability crises in social science, the acceleration of theory building in media and marketing psychology, and the practical advantages of rapid message testing for consumer products. Limitations of AI replications are addressed with respect to complex interaction effects, biases in AI models, and establishing benchmarks for AI metrics in marketing research.",
        "abstract_summary_gcp": "This report explores the potential of Large Language Models (LLMs) to expedite and accurately replicate marketing research on message effects. Researchers tested 19,447 LLM-powered \"personas\" across 133 experimental findings from 45 studies published in the Journal of Marketing, using the original studies' measures, stimuli, and sampling specifications to prompt the LLMs.\n\nThe LLM replications successfully reproduced 76% of the original main effects and 68% of all effects (including interactions), demonstrating strong potential for AI-assisted replication. A separate test revealed that replication results can change when extending beyond the original human study parameters (e.g., different participant samples, media stimuli, or measures).\n\nThe findings have significant implications for addressing replication and generalizability crises in social science, accelerating theory building in media and marketing psychology, and enabling rapid message testing for consumer products. Limitations discussed include LLMs' handling of complex interaction effects, potential biases in AI models, and the need to establish benchmarks for AI metrics in marketing research.",
        "url": "https://www.semanticscholar.org/paper/254e7f0e84cce66ac476f88d93dc6638e555351d",
        "isOpenAccess": false
    },
    "2408.10937": {
        "title": "Proxona: Supporting Creators' Sensemaking and Ideation with LLM-Powered Audience Personas",
        "authors": [
            "Yoonseo Choi",
            "Eun Jeong Kang",
            "Seulgi Choi",
            "M. Lee",
            "Juho Kim"
        ],
        "arxiv_id": "2408.10937",
        "venue": "International Conference on Human Factors in Computing Systems",
        "year": 2024,
        "publicationTypes": [
            "Book",
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 14,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "A content creator’s success depends on understanding their audience, but existing tools fail to provide in-depth insights and actionable feedback necessary for effectively targeting their audience. We present Proxona, an LLM-powered system that transforms static audience comments into interactive, multi-dimensional personas, allowing creators to engage with them to gain insights, gather simulated feedback, and refine content. Proxona distills audience traits from comments, into dimensions (categories) and values (attributes), then clusters them into interactive personas representing audience segments. Technical evaluations show that Proxona generates diverse dimensions and values, enabling the creation of personas that sufficiently reflect the audience and support data-grounded conversation. User evaluation with 11 creators confirmed that Proxona helped creators discover hidden audiences, gain persona-informed insights on early-stage content, and allowed them to confidently employ strategies when iteratively creating storylines. Proxona introduces a novel creator-audience interaction framework and fosters a persona-driven, co-creative process.",
        "abstract_summary_gcp": "Content creators' success relies on understanding their audience, but current tools lack the deep, actionable insights needed for effective targeting. To address this, **Proxona** is introduced as an LLM-powered system that converts static audience comments into interactive, multi-dimensional personas.\n\nProxona functions by distilling audience traits from comments into specific dimensions (categories) and values (attributes), then clustering these into interactive personas that represent distinct audience segments. This allows creators to engage with these personas to gain insights, gather simulated feedback, and refine their content.\n\nTechnical evaluations confirm Proxona generates diverse and reflective personas capable of supporting data-grounded conversations. User evaluations with 11 creators demonstrated its utility in helping discover hidden audiences, providing persona-informed insights for early-stage content, and enabling confident, iterative storyline development. Ultimately, Proxona offers a novel creator-audience interaction framework, promoting a persona-driven, co-creative content development process.",
        "url": "https://www.semanticscholar.org/paper/a4b15a63767b0460165016b70966785a7b2a985a",
        "isOpenAccess": true
    },
    "2405.10632": {
        "title": "Towards Interactive Evaluations for Interaction Harms in Human-AI Systems",
        "authors": [
            "Lujain Ibrahim",
            "Saffron Huang",
            "Umang Bhatt",
            "Lama Ahmad",
            "Markus Anderljung"
        ],
        "arxiv_id": "2405.10632",
        "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle",
            "Conference"
        ],
        "citationCount": 18,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Current AI evaluation methods, which rely on static, model-only tests, fail to account for harms that emerge through sustained human-AI interaction. As AI systems proliferate and are increasingly integrated into real-world applications, this disconnect between evaluation approaches and actual usage becomes more significant. In this paper, we propose a shift towards evaluation based on interactional ethics, which focuses on interaction harms—issues like inappropriate parasocial relationships, social manipulation, and cognitive overreliance that develop over time through repeated interaction, rather than through isolated outputs. First, we discuss the limitations of current evaluation methods, which (1) are static, (2) assume a universal user experience, and (3) have limited construct validity. Drawing on research from human-computer interaction, natural language processing, and the social sciences, we present practical principles for designing interactive evaluations. These include ecologically valid interaction scenarios, human impact metrics, and diverse human participation approaches. Finally, we explore implementation challenges and open research questions for researchers, practitioners, and regulators aiming to integrate interactive evaluations into AI governance frameworks. This work lays the groundwork for developing more effective evaluation methods that better capture the complex dynamics between humans and AI systems.",
        "abstract_summary_gcp": "This paper argues that current AI evaluation methods are inadequate because they are static and model-centric, thus failing to capture \"interaction harms\" that emerge from sustained human-AI interaction (e.g., inappropriate parasocial relationships, social manipulation, cognitive overreliance).\n\nIt proposes a shift to evaluation based on \"interactional ethics,\" which directly addresses these dynamic harms. The authors critique existing methods for being static, assuming a universal user experience, and having limited construct validity. Drawing on interdisciplinary research, they outline practical principles for designing interactive evaluations, including ecologically valid scenarios, human impact metrics, and diverse human participation. Finally, the paper discusses implementation challenges and future research directions for integrating these interactive evaluations into AI governance frameworks to better understand and manage human-AI dynamics.",
        "url": "https://www.semanticscholar.org/paper/6c1dcf5e573d88394ab2094c485c7954678f8aa8",
        "isOpenAccess": false
    },
    "2405.02411": {
        "title": "The Call for Socially Aware Language Technologies",
        "authors": [
            "Diyi Yang",
            "Dirk Hovy",
            "David Jurgens",
            "Barbara Plank"
        ],
        "arxiv_id": "2405.02411",
        "venue": "arXiv.org",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 14,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Language technologies have made enormous progress, especially with the introduction of large language models (LLMs). On traditional tasks such as machine translation and sentiment analysis, these models perform at near-human level. These advances can, however, exacerbate a variety of issues that models have traditionally struggled with, such as bias, evaluation, and risks. In this position paper, we argue that many of these issues share a common core: a lack of awareness of the factors, context, and implications of the social environment in which NLP operates, which we call social awareness. While NLP is getting better at solving the formal linguistic aspects, limited progress has been made in adding the social awareness required for language applications to work in all situations for all users. Integrating social awareness into NLP models will make applications more natural, helpful, and safe, and will open up new possibilities. Thus we argue that substantial challenges remain for NLP to develop social awareness and that we are just at the beginning of a new era for the field.",
        "abstract_summary_gcp": "Large Language Models (LLMs) have made enormous progress in NLP, achieving near-human performance on tasks like machine translation. However, this advancement also exacerbates existing issues such as bias, evaluation difficulties, and risks. The paper argues that these problems stem from a fundamental lack of \"social awareness\" in NLP—an understanding of the factors, context, and implications of its social environment. While NLP excels at formal linguistic aspects, it has made limited progress in integrating this social awareness. The authors contend that developing social awareness is crucial for making NLP applications more natural, helpful, and safe, opening new possibilities and representing a significant challenge for the field's future.",
        "url": "https://www.semanticscholar.org/paper/61882c31b2e590a8df11ec91513848f1ef162399",
        "isOpenAccess": false
    },
    "2401.07836": {
        "title": "Two types of AI existential risk: decisive and accumulative",
        "authors": [
            "Atoosa Kasirzadeh"
        ],
        "arxiv_id": "2401.07836",
        "venue": "Philosophical Studies",
        "year": 2024,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 34,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This decisive view, however, often neglects the serious possibility of AI x-risk manifesting gradually through an incremental series of smaller yet interconnected disruptions, crossing critical thresholds over time. This paper contrasts the conventional decisive AI x-risk hypothesis with what I call an accumulative AI x-risk hypothesis. While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different pathway to existential catastrophes. This involves a gradual accumulation of AI-induced threats such as severe vulnerabilities and systemic erosion of critical economic and political structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly undermine systemic and societal resilience until a triggering event results in irreversible collapse. Through complex systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view can reconcile seemingly incompatible perspectives on AI risks. The implications of differentiating between the two types of pathway—the decisive and the accumulative—for the governance of AI as well as long-term AI safety are discussed.",
        "abstract_summary_gcp": "This paper introduces and contrasts two hypotheses regarding AI existential risks (x-risks).\n\nThe **conventional \"decisive AI x-risk hypothesis\"** posits that such risks arise from abrupt, dire events, often involving advanced AI surpassing human intelligence and leading to human extinction or irreversible civilizational collapse (e.g., an overt AI takeover by superintelligence).\n\nIn contrast, the author proposes an **\"accumulative AI x-risk hypothesis,\"** which posits a gradual pathway to catastrophe. This involves a \"boiling frog\" scenario where incremental AI-induced threats – such as severe vulnerabilities and the systemic erosion of critical economic and political structures – gradually undermine systemic and societal resilience until a triggering event causes irreversible collapse.\n\nUsing complex systems analysis, the paper differentiates these two hypotheses, arguing that the accumulative view can reconcile conflicting perspectives on AI risks and offers crucial implications for AI governance and long-term safety.",
        "url": "https://www.semanticscholar.org/paper/690382e5a348cbd13237f164835fb0672c6e517e",
        "isOpenAccess": false
    },
    "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery": {
        "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Zhichao Yang",
            "Lingxi Li",
            "Junhui Qian",
            "Hong Yu"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/62cfafb4ccf847cb2f06fd0d5493203d5080480f",
        "isOpenAccess": false
    },
    "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations": {
        "title": "The Empty Chair: Using LLMs to Raise Missing Perspectives in Policy Deliberations",
        "authors": [
            "S. Fulay",
            "Deb Roy"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/4f33aa48364c565a2a8290f03db1a584b45b9ab0",
        "isOpenAccess": false
    },
    "Evaluation of Crowdsourced Peer Review using Synthetic Data and Simulations": {
        "title": "Evaluation of Crowdsourced Peer Review using Synthetic Data and Simulations",
        "authors": [
            "Michael Soprano",
            "Eddy Maddalena",
            "Francesca Da Ros",
            "Maria Elena Zuliani",
            "Stefano Mizzaro"
        ],
        "arxiv_id": null,
        "venue": "Italian Research Conference on Digital Library Management Systems",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/8d0ea354ce744540a0be867929cdc97e359fe1ab",
        "isOpenAccess": false
    },
    "How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study": {
        "title": "How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study",
        "authors": [
            "Cathy Mengying Fang",
            "Auren R. Liu",
            "Valdemar Danry",
            "Eunhae Lee",
            "Samantha W. T. Chan",
            "Pat Pataranutaporn",
            "Pattie Maes",
            "Jason Phang",
            "Michael Lampe",
            "Lama Ahmad",
            "Sandhini Agarwal"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 24,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/4c5cb3db922952163f1508cbf3297eac48fca8dc",
        "isOpenAccess": false
    },
    "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs": {
        "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs",
        "authors": [
            "Yuxuan Lu",
            "Jing Huang",
            "Yan Han",
            "Bennet Bei",
            "Yaochen Xie",
            "Dakuo Wang",
            "Zheshen Jessie Wang",
            "Qi He"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/a990263bc92799f2af442e4bb32ac4d571605f65",
        "isOpenAccess": false
    },
    "Improving LLM Personas via Rationalization with Psychological Scaffolds": {
        "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds",
        "authors": [
            "Brihi Joshi",
            "Xiang Ren",
            "Swabha Swayamdipta",
            "Rik Koncel-Kedziorski",
            "Tim Paek"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/141c5b7133dc07337a9b6a8b28fa19e56d0cfe6f",
        "isOpenAccess": false
    },
    "Leveraging AI to advance psychological research for climate policy": {
        "title": "Leveraging AI to advance psychological research for climate policy",
        "authors": [
            "Dhara Yu",
            "Bill D Thompson",
            "Rachit Dubey"
        ],
        "arxiv_id": null,
        "venue": "Current Opinion in Behavioral Sciences",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/756fdda3f54084d8d8a3df731db1f3b1046b28f5",
        "isOpenAccess": false
    },
    "Research on Artificial Intelligence-Assisted Game Design and Development": {
        "title": "Research on Artificial Intelligence-Assisted Game Design and Development",
        "authors": [
            "Xian Chen"
        ],
        "arxiv_id": null,
        "venue": "Proceedings of the 2nd International Conference on Data Science and Engineering",
        "year": 2025,
        "publicationTypes": [
            "Conference"
        ],
        "citationCount": 0,
        "fieldsOfStudy": null,
        "abstract": ": With the rapid development of Artificial Intelligence technology, Its application in game design and development become more and more extensive.AI technology not only improve the level of the intelligence in game, but also has a profound influence on content creation. AI enables game developer and designer to generate high-quality visual content and text with lower cost, improve the efficacy and creativity. This research explored the impact of AI development of game design and development procession, especially how creative AI can reduce the cost of graphic in game development and a new way of create text. In this paper, four key technologies will be introduced: cross-model generation, Generation based on Generative Adversarial Network (GAN), Diffusion model-based generation, and Autoregressive model-based generation. Then the paper will discuss game-assisted design methods and concludes with a future outlook of the field. This research not only showcases the innovative application of AI in game development but also provides new ideas and directions for the future development of the gaming industry.",
        "abstract_summary_gcp": "This paper explores the rapidly expanding application of Artificial Intelligence (AI) in game design and development. It highlights how AI not only enhances game intelligence but also significantly influences content creation, allowing developers to generate high-quality visual and textual content more cost-effectively and creatively.\n\nThe research specifically investigates how creative AI can reduce graphic development costs and introduce novel methods for text creation. It will detail four key generative AI technologies: cross-modal generation, Generative Adversarial Networks (GANs), Diffusion models, and Autoregressive models. The paper will also discuss AI-assisted design methods and conclude with a future outlook for the field, showcasing AI's innovative role and offering new directions for the gaming industry.",
        "url": "https://www.semanticscholar.org/paper/a4f224d52ea27de0a4b170e774eabc33e8bff2bc",
        "isOpenAccess": false
    },
    "Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions": {
        "title": "Selective agreement, not sycophancy: investigating opinion dynamics in LLM interactions",
        "authors": [
            "Erica Cau",
            "Valentina Pansanella",
            "Dino Pedreschi",
            "Giulio Rossetti"
        ],
        "arxiv_id": null,
        "venue": "EPJ Data Science",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 5,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/5f6275ed082f8fb223d304f5e6f3d1311749c1d0",
        "isOpenAccess": false
    },
    "Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions": {
        "title": "Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions",
        "authors": [
            "Minwoo Kang",
            "Suhong Moon",
            "Seungyong Lee",
            "Ayush Raj",
            "Joseph Suh",
            "David M. Chan"
        ],
        "arxiv_id": null,
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle",
            "Review"
        ],
        "citationCount": 1,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": null,
        "abstract_summary_gcp": "[요약 없음]",
        "url": "https://www.semanticscholar.org/paper/2976de866327756d79514863505ba3258d46c038",
        "isOpenAccess": false
    },
    "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Games": {
        "title": "Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game",
        "authors": [
            "Byungjun Kim",
            "Dayeon Seo",
            "Bugeun Kim"
        ],
        "arxiv_id": null,
        "venue": "IEEE Access",
        "year": 2024,
        "publicationTypes": null,
        "citationCount": null,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Recent studies have investigated whether large language models (LLMs) can support obscured communication, which is characterized by core aspects such as inferring subtext and evading suspicions. To conduct the investigation, researchers have used social deduction games (SDGs) as their experimental environment, in which players conceal and infer specific information. However, prior work has often overlooked how LLMs should be evaluated in such settings. Specifically, we point out two limitations with the evaluation methods they employed. First, metrics used in prior studies are coarse-grained as they are based on overall game outcomes that often fail to capture event-level behaviors; Second, error analyses have lacked structured methodologies capable of producing insights that meaningfully support evaluation outcomes. To address these limitations, we propose a microscopic and systematic approach to the investigation. Specifically, we introduce six fine-grained metrics that resolve the first issue. To tackle the second issue, we conducted a thematic analysis and identified four major reasoning failures that undermine LLMs’ performance in obscured communication.",
        "abstract_summary_gcp": "Recent studies examine large language models' (LLMs) ability to support obscured communication, which involves inferring subtext and evading suspicion, often using social deduction games (SDGs) as an experimental setting.\n\nHowever, the authors identify two key limitations in prior evaluation methods:\n1.  **Coarse-grained metrics:** These metrics rely on overall game outcomes, failing to capture detailed, event-level behaviors.\n2.  **Unstructured error analyses:** Previous error analyses lacked structured methodologies to produce meaningful insights.\n\nTo address these issues, the authors propose a microscopic and systematic evaluation approach. They introduce six fine-grained metrics to better resolve event-level analysis and conducted a thematic analysis, identifying four major reasoning failures that hinder LLMs' performance in obscured communication.",
        "url": "https://www.semanticscholar.org/paper/f10ac22f42fffa58248c1f4b38c646639994fb94",
        "isOpenAccess": false
    },
    "2511.06175": {
        "title": "CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference",
        "authors": [
            "Kaijie Xu",
            "Fandi Meng",
            "Clark Verbrugge",
            "Simon Lucas"
        ],
        "arxiv_id": "2511.06175",
        "venue": "",
        "year": 2025,
        "publicationTypes": null,
        "citationCount": 0,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary\"reasoning tool.\"Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.",
        "abstract_summary_gcp": "The paper introduces CSP4SDG, a probabilistic, constraint-satisfaction framework designed to objectively analyze gameplay in Social Deduction Games (SDGs) like Avalon, Mafia, and Werewolf. Addressing the central challenge of hidden-role inference, CSP4SDG maps game events and dialogue into four linguistically-agnostic constraint classes: evidence, phenomena, assertions, and hypotheses.\n\nHard constraints are used to prune impossible role assignments, while weighted soft constraints score the remaining possibilities. These soft constraints employ information-gain weighting, linking hypotheses to their expected value under entropy reduction, and a specialized scoring rule ensures truthful assertions converge to classical hard logic with minimum error. This process generates an interpretable posterior over roles that updates in real time.\n\nExperiments on three public datasets demonstrate that CSP4SDG significantly outperforms LLM-based baselines in every inference scenario and boosts LLM performance when supplied as an auxiliary reasoning tool. The study concludes that principled probabilistic reasoning with information theory offers a scalable and effective alternative or complement to heavy-weight neural models for SDGs.",
        "url": "https://www.semanticscholar.org/paper/64e9b7375c713dfeeed8108b28725c1f1ee1b850",
        "isOpenAccess": false
    },
    "2509.25541": {
        "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play",
        "authors": [
            "Qinsi Wang",
            "Bo Liu",
            "Tianyi Zhou",
            "Jing Shi",
            "Yueqian Lin",
            "Yiran Chen",
            "Hai Li",
            "Kun Wan",
            "Wentian Zhao"
        ],
        "arxiv_id": "2509.25541",
        "venue": "arXiv.org",
        "year": 2025,
        "publicationTypes": [
            "JournalArticle"
        ],
        "citationCount": 4,
        "fieldsOfStudy": [
            "Computer Science"
        ],
        "abstract": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in\"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
        "abstract_summary_gcp": "Vision-Zero is a novel, domain-agnostic framework designed to enable Vision-Language Models (VLMs) to self-improve their reasoning capabilities, addressing the current limitation of heavy reliance on costly, human-annotated datasets for reinforcement learning.\n\nThe framework is characterized by three main attributes:\n\n1.  **Strategic Self-Play Framework:** Vision-Zero trains VLMs by having them play multi-role, \"Who Is the Spy\"-style competitive visual games. Through this interactive gameplay, the models autonomously generate their own training data, eliminating the need for human annotation.\n2.  **Gameplay from Arbitrary Images:** Unlike existing gamified approaches, Vision-Zero can create games from any image pair, supporting diverse domains like synthetic scenes (CLEVR), charts, and real-world images. This broad applicability enhances the VLM's reasoning ability and ensures strong generalization across various tasks.\n3.  **Sustainable Performance Gain:** It introduces **Iterative Self-Play Policy Optimization (Iterative-SPO)**, a new training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR). This method prevents performance plateaus common in self-play-only training, leading to sustained, long-term improvements.\n\nDespite using only label-free data, Vision-Zero achieves state-of-the-art performance on critical tasks such as reasoning, chart question answering, and vision-centric understanding, surpassing methods that depend on human annotations. Models and code are publicly available.",
        "url": "https://www.semanticscholar.org/paper/b5eeff55c73861fec39e6b02019dd76a32bb698c",
        "isOpenAccess": false
    }
}