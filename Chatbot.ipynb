{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9067c063",
   "metadata": {},
   "source": [
    "##### 1단계: LangChain 설치 및 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41827a5",
   "metadata": {},
   "source": [
    "gcloud cli 인증\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff545727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c931d",
   "metadata": {},
   "source": [
    "##### 2단계: Groq Qwen2.5 32B 설치 및 설정 지침  -> 대신 GCP gemini로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6afcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "\n",
    "# Vertex AI Gemini 기반 LLM\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.5-flash\",   # 또는 \"gemini-2.5-pro\"\n",
    "    temperature=0.0,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832462b4",
   "metadata": {},
   "source": [
    "##### 3단계: NVIDIA bge-m3 설치 및 설정 -> 대신 vertex 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda38ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -qU langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b304f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c157a31",
   "metadata": {},
   "source": [
    "##### 4단계: Milvus 설치 및 설정 -> 대신 faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22e832",
   "metadata": {},
   "source": [
    "##### 5단계: RAG 챗봇 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b54a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading FAISS index 'citing_papers.faiss'...\n",
      "[INFO] Loading paper data 'citing_papers.json' and map 'citing_papers_to_model.json'...\n",
      "[INFO] Rebuilding LangChain docstore from FAISS map...\n",
      "[INFO] Building LangChain FAISS VectorStore...\n",
      "[INFO] FAISS index loaded and reconstructed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "FAISS_INDEX_PATH = \"citing_papers.faiss\"\n",
    "DATA_JSON_PATH   = \"citing_papers.json\"\n",
    "MAP_JSON_PATH    = \"citing_papers_to_model.json\"\n",
    "\n",
    "\n",
    "def load_vector_store(\n",
    "    index_path: str = FAISS_INDEX_PATH,\n",
    "    data_path: str = DATA_JSON_PATH,\n",
    "    map_path: str = MAP_JSON_PATH,\n",
    "):\n",
    "    print(f\"[INFO] Loading FAISS index '{index_path}'...\")\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    print(f\"[INFO] Loading paper data '{data_path}' and map '{map_path}'...\")\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        paper_DATA = json.load(f)\n",
    "    with open(map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        index_to_model_map = json.load(f)\n",
    "\n",
    "    if not index_to_model_map:\n",
    "        raise ValueError(\"index_to_model_map is empty. Run build_database.py first.\")\n",
    "\n",
    "    print(\"[INFO] Rebuilding LangChain docstore from FAISS map...\")\n",
    "    docstore_dict = {}\n",
    "    index_to_docstore_id = {}\n",
    "\n",
    "    for str_idx, model_name in index_to_model_map.items():\n",
    "        int_idx = int(str_idx)\n",
    "        info = paper_DATA.get(model_name)\n",
    "        if info is None:\n",
    "            raise KeyError(f\"paper.json is missing entry for '{model_name}'.\")\n",
    "\n",
    "        doc_id = model_name\n",
    "        text = f\"model: {model_name}. \" + \" \".join(\n",
    "            f\"{key}: {value}\" for key, value in info.items() if value is not None\n",
    "        )\n",
    "\n",
    "        metadata = {\"model_name\": model_name, **info}\n",
    "        doc = Document(page_content=text, metadata=metadata)\n",
    "\n",
    "        docstore_dict[doc_id] = doc\n",
    "        index_to_docstore_id[int_idx] = doc_id\n",
    "\n",
    "    if index.ntotal != len(index_to_docstore_id):\n",
    "        print(f\"[WARN] Index size ({index.ntotal}) != mapping count ({len(index_to_docstore_id)}).\")\n",
    "\n",
    "    docstore = InMemoryDocstore(docstore_dict)\n",
    "\n",
    "    print(\"[INFO] Building LangChain FAISS VectorStore...\")\n",
    "    return FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "    )\n",
    "\n",
    "\n",
    "vector_store = None  \n",
    "\n",
    "try:\n",
    "    vector_store = load_vector_store()\n",
    "    print(\"[INFO] FAISS index loaded and reconstructed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[ERROR] Missing required file: {e}\")\n",
    "    print(\"Check paper.faiss, paper.json, paper_to_model.json paths.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"[ERROR] Exception while loading FAISS: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6005447",
   "metadata": {},
   "source": [
    "프롬프트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a83bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RAG 프롬프트 정의 중...\n",
      "[INFO] 프롬프트 설정 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"[INFO] RAG 프롬프트 정의 중...\")\n",
    "\n",
    "key_descriptions = key_descriptions = \"\"\"\n",
    "[데이터 필드 설명]\n",
    "\n",
    "- title: 논문의 정식 제목.\n",
    "\n",
    "- authors: 논문 저자 목록. 연구 기여자들의 전체 이름을 배열 형태로 제공한다.\n",
    "\n",
    "- arxiv_id: ArXiv에 게시된 논문의 식별자. 버전 정보와 독립적으로 논문을 특정하는 데 사용된다.\n",
    "\n",
    "- venue: 논문이 발표된 학회 또는 저널명. 비어 있을 경우 정식 출판 이전(arXiv 사전 공개 등) 상태를 의미한다.\n",
    "\n",
    "- year: 논문의 출판 연도 혹은 ArXiv 게시 연도 (최초 공개 시점)\n",
    "\n",
    "- publicationTypes: 출판 유형 정보(예: 저널, 학회, preprint 등). null인 경우 정식 분류가 제공되지 않은 상태를 의미한다.\n",
    "\n",
    "- citationCount: 논문이 다른 연구로부터 인용된 횟수. 연구 영향력 및 확산 정도를 나타내는 지표이다.\n",
    "\n",
    "- fieldsOfStudy: 논문이 속하는 학문 분야 목록. 연구의 범위 및 적용 영역을 설명한다.\n",
    "\n",
    "- abstract: 논문의 공식 초록. 연구의 동기, 목적, 방법론, 주요 결과, 결론을 요약한 핵심 텍스트이다.\n",
    "\n",
    "- abstract_summary_gcp: abstract를 기반으로 생성된 요약 정보(Google Cloud Pipeline 등 자동 요약).\n",
    "\n",
    "- keywords: 논문의 주요 개념, 연구 주제, 핵심 토픽을 요약한 키워드 목록. \n",
    "\n",
    "- url: Semantic Scholar 등에서 해당 논문을 참조할 수 있는 링크. 원문 조회를 위한 메타데이터이다.\n",
    "\n",
    "- isOpenAccess: 논문이 오픈 액세스 형태로 제공되는지 여부. True이면 누구나 접근 가능함을 의미한다.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 논문/모델 정보를 설명하는 어시스턴트입니다.\\n\"\n",
    "            f\"제공되는 정보는 다음과 같은 구조로 이루어져 있습니다:\\n{key_descriptions}\\n\"\n",
    "            \"주어진 context(논문 메타데이터와 설명)에 근거해서만 답변하세요. \"\n",
    "            \"모르면 모른다고 말하세요.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"질문: {question}\\n\\n\"\n",
    "            \"다음은 검색된 논문/모델 정보입니다:\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"위의 데이터 필드 설명과 정보를 바탕으로 한국어로 자세히 답변해 주세요.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"[INFO] 프롬프트 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f93397a25d14d49b785a73e6c3687a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TOP_TIER_VENUES = {\"NEURIPS\", \"ICML\", \"ICLR\", \"CVPR\", \"ACL\", \"NAACL\", \"EMNLP\", \"ECCV\"}\n",
    "DEFAULT_YEAR_WINDOW = 5\n",
    "RECENCY_WEIGHT = 0.1 #최신 논문 가중치 \n",
    "VENUE_WEIGHT = 0.3 #탑티어 학회 점수 가중치\n",
    "\n",
    "\n",
    "# 사용자의 선호 연도와 선호 학회를 입력 받음.\n",
    "def collect_user_preferences():\n",
    "    print(\"[SETUP] Enter your search preferences.\")\n",
    "    #최근 몇 년 논문에 집중할지? (숫자 입력, 기본값 5)\n",
    "    year_raw = input(\"최근 몇 년 논문을 우선할까요? (숫자 입력, 기본값 5):: \").strip()\n",
    "    try:\n",
    "        year_window = int(year_raw) if year_raw else DEFAULT_YEAR_WINDOW\n",
    "    except ValueError:\n",
    "        print(\"[경고] 숫자가 아닙니다. 기본값 5년을 사용합니다.\")\n",
    "        year_window = DEFAULT_YEAR_WINDOW\n",
    "\n",
    "    #학회 우선순위를 어떻게 둘지?\n",
    "    venue_raw = input(\"학회 우선순위를 선택하세요. (top/top-only/none or comma list, e.g., NeurIPS,ICML): \").strip()\n",
    "\n",
    "    venue_mode = \"all\"\n",
    "    preferred = []\n",
    "    if venue_raw:\n",
    "        lower = venue_raw.lower()\n",
    "        if lower in (\"top\", \"top-tier\", \"top priority\"): # 탑티어에 점수 부여 + 다른 학회도 봄 \n",
    "            venue_mode = \"top\"\n",
    "        elif lower in (\"top-only\", \"top tier only\", \"top-tier only\"): # 탑티어만 \n",
    "            venue_mode = \"top_only\"\n",
    "        else:\n",
    "            venue_mode = \"custom\" # 직접 지정\n",
    "            preferred = [v.strip().upper() for v in venue_raw.split(\",\") if v.strip()]\n",
    "\n",
    "    return {\n",
    "        \"year_window\": year_window, # 최근 몇 년 논문에 집중할지(default 5년)\n",
    "        \"venue_mode\": venue_mode, # top-tier 우선, top-tier only, custom venue, 전체 \n",
    "        \"preferred_venues\": preferred, # 사용자가 직접 지정한 venue 리스트\n",
    "    }\n",
    "\n",
    "\n",
    "#사용자의 선호 설정 문자열로 요약 (dict -> str)\n",
    "def describe_preferences(preferences: dict) -> str:\n",
    "    parts = [f\"recent {preferences.get('year_window', DEFAULT_YEAR_WINDOW)} years\"]\n",
    "    venue_mode = preferences.get(\"venue_mode\", \"all\")\n",
    "    if venue_mode == \"top\":\n",
    "        parts.append(\"top-tier prioritized\")\n",
    "    elif venue_mode == \"top_only\":\n",
    "        parts.append(\"top-tier only\")\n",
    "    elif venue_mode == \"custom\":\n",
    "        prefs = preferences.get(\"preferred_venues\", [])\n",
    "        if prefs:\n",
    "            parts.append(\"venues: \" + \", \".join(prefs))\n",
    "    else:\n",
    "        parts.append(\"all venues\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "def rerank_with_preferences(docs_with_scores, preferences: dict):\n",
    "    # 검색 결과(docs_with_scores)에 선호도 기반 가중치를 더해 재정렬.\n",
    "    preferences = preferences or {}\n",
    "    year_window = preferences.get(\"year_window\", DEFAULT_YEAR_WINDOW)\n",
    "    venue_mode = preferences.get(\"venue_mode\", \"all\")\n",
    "    preferred = {v.upper() for v in preferences.get(\"preferred_venues\", [])}\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    applied = {\"year_cutoff\": None, \"venue_mode\": venue_mode}\n",
    "    cutoff = current_year - year_window if year_window else None\n",
    "\n",
    "    filtered = docs_with_scores\n",
    "    \n",
    "    #연도 필터링 \n",
    "    if cutoff is not None:\n",
    "        #필터링 후 결과가 비면 필터링을 포기하고 원래 목록을 그대로 사용\n",
    "        filtered = [ \n",
    "            (doc, score)\n",
    "            for doc, score in filtered\n",
    "            if int(doc.metadata.get(\"year\", 0) or 0) >= cutoff\n",
    "        ]\n",
    "        applied[\"year_cutoff\"] = cutoff\n",
    "        if not filtered: \n",
    "            filtered = docs_with_scores\n",
    "            applied[\"year_cutoff\"] = None\n",
    "\n",
    "    reranked = []\n",
    "    for doc, score in filtered:\n",
    "        meta = doc.metadata or {}\n",
    "        #선호도 기반 점수 재계산\n",
    "        base_similarity = -float(score) \n",
    "        #최신 연도 가중치\n",
    "        year = meta.get(\"year\")\n",
    "        recency_bonus = 0.0\n",
    "        if isinstance(year, (int, float)):\n",
    "            years_old = max(current_year - int(year), 0)\n",
    "            if year_window:\n",
    "                recency_bonus = max(0, year_window - years_old) * RECENCY_WEIGHT\n",
    "        #학회 가중치\n",
    "        venue = str(meta.get(\"venue\", \"\")).upper()\n",
    "        venue_bonus = 0.0\n",
    "        if venue_mode == \"top\":\n",
    "            if venue in TOP_TIER_VENUES:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "        elif venue_mode == \"top_only\":\n",
    "            if venue in TOP_TIER_VENUES:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "            else:\n",
    "                continue # top-tier 아니면 아예 제외\n",
    "        elif venue_mode == \"custom\":\n",
    "            if venue in preferred:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "        #최종 점수\n",
    "        final_score = base_similarity + recency_bonus + venue_bonus\n",
    "        reranked.append((doc, final_score))\n",
    "    #점수가 높은 순으로 재정렬.\n",
    "    if not reranked:\n",
    "        reranked = [(doc, -float(score)) for doc, score in docs_with_scores]\n",
    "\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    return reranked, applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084a693",
   "metadata": {},
   "source": [
    "(2) RAG 한 번 호출하는 함수만 두기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b9d8121c594bb59fb0dae43ef1d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# RAG 정의 \n",
    "# ===============================\n",
    "\n",
    "# Similarity filtering thresholds (L2 distance: smaller -> closer)\n",
    "MAX_RESULTS = 4\n",
    "RAW_K_MULTIPLIER = 3  # fetch this many before filtering\n",
    "BEST_DISTANCE_MAX = 1.1  # above this, treat as unrelated\n",
    "RELATIVE_MARGIN = 0.12   # allow within 12% of best distance\n",
    "\n",
    "def filter_by_similarity(docs_with_scores, max_results: int = MAX_RESULTS):\n",
    "    \"\"\"Filter based on L2 distance relative to the best hit.\"\"\"\n",
    "    if not docs_with_scores:\n",
    "        return [], {\"best\": None, \"cutoff\": None, \"kept\": 0}\n",
    "\n",
    "    scores = [score for _, score in docs_with_scores]\n",
    "    best = min(scores)\n",
    "    cutoff = min(BEST_DISTANCE_MAX, best * (1 + RELATIVE_MARGIN))\n",
    "\n",
    "    filtered = [\n",
    "        (doc, score)\n",
    "        for doc, score in docs_with_scores\n",
    "        if score <= cutoff\n",
    "    ]\n",
    "    filtered.sort(key=lambda x: x[1])  # ascending distance (closer first)\n",
    "    filtered = filtered[:max_results]\n",
    "    return filtered, {\"best\": best, \"cutoff\": cutoff, \"kept\": len(filtered)}\n",
    "\n",
    "\n",
    "def rag_answer(question: str, k: int = MAX_RESULTS, preferences: dict | None = None):\n",
    "    \"\"\"FAISS search + LLM answer.\"\"\"\n",
    "    if vector_store is None:\n",
    "        raise RuntimeError(\"vector_store is not initialized. Run the FAISS load cell first.\")\n",
    "\n",
    "    max_results = min(k, MAX_RESULTS)\n",
    "    k_search = max_results * RAW_K_MULTIPLIER\n",
    "    print(f\"[RAG] Query: {question}\")\n",
    "    docs_with_scores = vector_store.similarity_search_with_score(question, k=k_search)\n",
    "    print(f\"[RAG] First-pass results: {len(docs_with_scores)}\")\n",
    "\n",
    "    # similarity gate\n",
    "    filtered_docs, sim_meta = filter_by_similarity(docs_with_scores, max_results=max_results)\n",
    "    if not filtered_docs:\n",
    "        print(\"[RAG] No documents passed the similarity gate.\")\n",
    "        return (\n",
    "            \"No papers sufficiently match the question. Please ask more specifically.\",\n",
    "            [],\n",
    "            {\"year_cutoff\": None, \"venue_mode\": None, \"no_results\": True, **sim_meta},\n",
    "        )\n",
    "    print(f\"[RAG] Best dist: {sim_meta['best']:.4f}, cutoff: {sim_meta['cutoff']:.4f}, kept: {sim_meta['kept']}\")\n",
    "\n",
    "    # rerank by preferences\n",
    "    reranked_docs, applied_filters = rerank_with_preferences(filtered_docs, preferences or {})\n",
    "    applied_filters.update(sim_meta)\n",
    "    docs = [doc for doc, _ in reranked_docs]\n",
    "    print(f\"[RAG] After filters: {len(docs)}\")\n",
    "\n",
    "    # concatenate context\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    messages = prompt.invoke({\"question\": question, \"context\": context_text})\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return response.content, docs, applied_filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebf30a4ed8546c99ce62806580a697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paper Chatbot ---\n",
      "[SETUP] Enter your search preferences.\n",
      "[INFO] Preferences: recent 3 years | top-tier prioritized\n",
      " [You]: 게임과 관련된 논문 찾아줘\n",
      "[RAG] Query: 게임과 관련된 논문 찾아줘\n",
      "[RAG] First-pass results: 12\n",
      "[RAG] Best dist: 1.0410, cutoff: 1.1000, kept: 4\n",
      "[RAG] After filters: 4\n",
      "[Bot]: 제공해주신 논문/모델 정보 중 '게임'과 직접적으로 관련된 내용은 찾을 수 없습니다.\n",
      "\n",
      "제공된 논문들은 주로 다음과 같은 주제를 다루고 있습니다:\n",
      "*   **Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance**: 멀티모달(비디오, 오디오, 텍스트) 데이터를 활용한 공감 수준 예측 방법에 대한 연구입니다.\n",
      "*   **Reasoning-Aware Multimodal Fusion for Hateful Video Detection**: 온라인 비디오에서 혐오 콘텐츠를 탐지하기 위한 멀티모달 융합 프레임워크에 대한 연구입니다.\n",
      "*   **DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models**: 대규모 언어 모델(LLM)의 계산 효율성, 추론 능력 및 에이전트 성능을 개선하는 새로운 모델에 대한 설명입니다.\n",
      "*   **A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models**: 대규모 AI 모델에서 희소 혼합 전문가(s-MoE)의 로드 밸런싱에 대한 이론적 프레임워크를 제시합니다.\n",
      "[Filters] filtered to >= 2022 year | top-tier prioritized\n",
      "--- Sources ---\n",
      "[1] 2512.02558 (2025, ): model: 2512.02558. title: Empathy Level Prediction in Multi-Modal Scenario with Supervisory Documentation Assistance aut...\n",
      "[2] 2512.02743 (2025, ): model: 2512.02743. title: Reasoning-Aware Multimodal Fusion for Hateful Video Detection authors: ['Shuonan Yang', 'Taili...\n",
      "[3] 2512.02556 (2025, ): model: 2512.02556. title: DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models authors: ['DeepSeek-AI', 'Ai...\n",
      "[4] 2512.03915 (2025, ): model: 2512.03915. title: A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Paper Chatbot ---\")\n",
    "\n",
    "if vector_store is None: #FAISS 인덱스가 로드되지 않았다면 실행할 수 없으므로 사용자에게 오류 전달\n",
    "    print(\"[ERROR] Vector store is not loaded. Run the FAISS load cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        preferences = collect_user_preferences() #사용자 선호도 입력받기\n",
    "        print(f\"[INFO] Preferences: {describe_preferences(preferences)}\")\n",
    "\n",
    "        user_question = input(\"Enter your question: \").strip()\n",
    "\n",
    "        print(f\" [You]: {user_question}\")\n",
    "        answer, used_docs, applied = rag_answer(user_question, k=4, preferences=preferences)\n",
    "\n",
    "        print(f\"[Bot]: {answer}\")\n",
    "        \n",
    "        # 실제로 RAG에서 어떤 필터가 적용되었는지 사용자에게 피드백\n",
    "        note = []\n",
    "        if applied.get(\"year_cutoff\"):\n",
    "            note.append(f\"filtered to >= {applied['year_cutoff']} year\")\n",
    "        vm = applied.get(\"venue_mode\")\n",
    "        if vm == \"top\":\n",
    "            note.append(\"top-tier prioritized\")\n",
    "        elif vm == \"top_only\":\n",
    "            note.append(\"top-tier only\")\n",
    "        elif vm == \"custom\":\n",
    "            note.append(\"custom venues prioritized\")\n",
    "        if note:\n",
    "            print(\"[Filters] \" + \" | \".join(note))\n",
    "        # 사용된 문서(Sources) 출력\n",
    "        print(\"--- Sources ---\")\n",
    "        for i, doc in enumerate(used_docs):\n",
    "            model_name = doc.metadata.get(\"model_name\", \"Unknown\")\n",
    "            year = doc.metadata.get(\"year\", \"?\")\n",
    "            venue = doc.metadata.get(\"venue\", \"?\")\n",
    "            preview = doc.page_content[:120].replace(\"\\n\", \" \")\n",
    "            print(f\"[{i+1}] {model_name} ({year}, {venue}): {preview}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"[ERROR] RAG run failed: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6f1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
