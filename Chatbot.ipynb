{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9067c063",
   "metadata": {},
   "source": [
    "##### 1단계: LangChain 설치 및 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41827a5",
   "metadata": {},
   "source": [
    "gcloud cli 인증\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff545727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c931d",
   "metadata": {},
   "source": [
    "##### 2단계: Groq Qwen2.5 32B 설치 및 설정 지침  -> 대신 GCP gemini로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6afcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "\n",
    "# Vertex AI Gemini 기반 LLM\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.5-flash\",   # 또는 \"gemini-2.5-pro\"\n",
    "    temperature=0.0,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832462b4",
   "metadata": {},
   "source": [
    "##### 3단계: NVIDIA bge-m3 설치 및 설정 -> 대신 vertex 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda38ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install -qU langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b304f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c157a31",
   "metadata": {},
   "source": [
    "##### 4단계: Milvus 설치 및 설정 -> 대신 faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22e832",
   "metadata": {},
   "source": [
    "##### 5단계: RAG 챗봇 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b54a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] FAISS 인덱스 'paper.faiss' 로드 중...\n",
      "[INFO] 원본 데이터 'paper.json' 및 맵 'paper_to_model.json' 로드 중...\n",
      "[INFO] LangChain Docstore 재구성 중...\n",
      "[INFO] LangChain FAISS VectorStore 생성 중...\n",
      "[INFO] FAISS 인덱스 수동 로드 및 재구성 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "FAISS_INDEX_PATH = \"paper.faiss\"\n",
    "DATA_JSON_PATH   = \"paper.json\"\n",
    "MAP_JSON_PATH    = \"paper_to_model.json\"\n",
    "\n",
    "\n",
    "def load_vector_store(\n",
    "    index_path: str = FAISS_INDEX_PATH,\n",
    "    data_path: str = DATA_JSON_PATH,\n",
    "    map_path: str = MAP_JSON_PATH,\n",
    "):\n",
    "    print(f\"[INFO] Loading FAISS index '{index_path}'...\")\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    print(f\"[INFO] Loading paper data '{data_path}' and map '{map_path}'...\")\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        paper_DATA = json.load(f)\n",
    "    with open(map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        index_to_model_map = json.load(f)\n",
    "\n",
    "    if not index_to_model_map:\n",
    "        raise ValueError(\"index_to_model_map is empty. Run build_database.py first.\")\n",
    "\n",
    "    print(\"[INFO] Rebuilding LangChain docstore from FAISS map...\")\n",
    "    docstore_dict = {}\n",
    "    index_to_docstore_id = {}\n",
    "\n",
    "    for str_idx, model_name in index_to_model_map.items():\n",
    "        int_idx = int(str_idx)\n",
    "        info = paper_DATA.get(model_name)\n",
    "        if info is None:\n",
    "            raise KeyError(f\"paper.json is missing entry for '{model_name}'.\")\n",
    "\n",
    "        doc_id = model_name\n",
    "        text = f\"model: {model_name}. \" + \" \".join(\n",
    "            f\"{key}: {value}\" for key, value in info.items() if value is not None\n",
    "        )\n",
    "\n",
    "        metadata = {\"model_name\": model_name, **info}\n",
    "        doc = Document(page_content=text, metadata=metadata)\n",
    "\n",
    "        docstore_dict[doc_id] = doc\n",
    "        index_to_docstore_id[int_idx] = doc_id\n",
    "\n",
    "    if index.ntotal != len(index_to_docstore_id):\n",
    "        print(f\"[WARN] Index size ({index.ntotal}) != mapping count ({len(index_to_docstore_id)}).\")\n",
    "\n",
    "    docstore = InMemoryDocstore(docstore_dict)\n",
    "\n",
    "    print(\"[INFO] Building LangChain FAISS VectorStore...\")\n",
    "    return FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "    )\n",
    "\n",
    "\n",
    "vector_store = None  \n",
    "\n",
    "try:\n",
    "    vector_store = load_vector_store()\n",
    "    print(\"[INFO] FAISS index loaded and reconstructed.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[ERROR] Missing required file: {e}\")\n",
    "    print(\"Check paper.faiss, paper.json, paper_to_model.json paths.\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"[ERROR] Exception while loading FAISS: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6005447",
   "metadata": {},
   "source": [
    "프롬프트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a83bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] RAG 프롬프트 정의 중...\n",
      "[INFO] 프롬프트 설정 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"[INFO] RAG 프롬프트 정의 중...\")\n",
    "\n",
    "key_descriptions = \"\"\"\n",
    "[데이터 필드 설명]\n",
    "- title: 논문의 제목\n",
    "- author: 저자 목록\n",
    "- year: 논문의 출판 연도 혹은 ArXiv 게시 연도 (최초 공개 시점)\n",
    "- keywords: 논문의 핵심 내용 요약\n",
    "- tasks: 논문에서 해결하고자 하는 주요 과업(Task) 정의\n",
    "- abstract: 논문의 초록 (연구의 목적, 방법, 결과 요약)\n",
    "- related work: 논문이 참고하거나 비교한 기존 연구들에 대한 설명 (기존 연구와의 차이점 등)\n",
    "- Experiments: 실험 내용 (데이터셋, 실험 환경, 성능 평가 결과 등)\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 논문/모델 정보를 설명하는 어시스턴트입니다.\\n\"\n",
    "            f\"제공되는 정보는 다음과 같은 구조로 이루어져 있습니다:\\n{key_descriptions}\\n\"\n",
    "            \"주어진 context(논문 메타데이터와 설명)에 근거해서만 답변하세요. \"\n",
    "            \"모르면 모른다고 말하세요.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"질문: {question}\\n\\n\"\n",
    "            \"다음은 검색된 논문/모델 정보입니다:\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"위의 데이터 필드 설명과 정보를 바탕으로 한국어로 자세히 답변해 주세요.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"[INFO] 프롬프트 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f93397a25d14d49b785a73e6c3687a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TOP_TIER_VENUES = {\"NEURIPS\", \"ICML\", \"ICLR\", \"CVPR\", \"ACL\", \"NAACL\", \"EMNLP\", \"ECCV\"}\n",
    "DEFAULT_YEAR_WINDOW = 5\n",
    "RECENCY_WEIGHT = 0.1 #최신 논문 가중치 \n",
    "VENUE_WEIGHT = 0.3 #탑티어 학회 점수 가중치\n",
    "\n",
    "\n",
    "# 사용자의 선호 연도와 선호 학회를 입력 받음.\n",
    "def collect_user_preferences():\n",
    "    print(\"[SETUP] Enter your search preferences.\")\n",
    "    #최근 몇 년 논문에 집중할지? (숫자 입력, 기본값 5)\n",
    "    year_raw = input(\"최근 몇 년 논문을 우선할까요? (숫자 입력, 기본값 5):: \").strip()\n",
    "    try:\n",
    "        year_window = int(year_raw) if year_raw else DEFAULT_YEAR_WINDOW\n",
    "    except ValueError:\n",
    "        print(\"[경고] 숫자가 아닙니다. 기본값 5년을 사용합니다.\")\n",
    "        year_window = DEFAULT_YEAR_WINDOW\n",
    "\n",
    "    #학회 우선순위를 어떻게 둘지?\n",
    "    venue_raw = input(\"학회 우선순위를 선택하세요. (top/top-only/none or comma list, e.g., NeurIPS,ICML): \").strip()\n",
    "\n",
    "    venue_mode = \"all\"\n",
    "    preferred = []\n",
    "    if venue_raw:\n",
    "        lower = venue_raw.lower()\n",
    "        if lower in (\"top\", \"top-tier\", \"top priority\"): # 탑티어에 점수 부여 + 다른 학회도 봄 \n",
    "            venue_mode = \"top\"\n",
    "        elif lower in (\"top-only\", \"top tier only\", \"top-tier only\"): # 탑티어만 \n",
    "            venue_mode = \"top_only\"\n",
    "        else:\n",
    "            venue_mode = \"custom\" # 직접 지정\n",
    "            preferred = [v.strip().upper() for v in venue_raw.split(\",\") if v.strip()]\n",
    "\n",
    "    return {\n",
    "        \"year_window\": year_window, # 최근 몇 년 논문에 집중할지(default 5년)\n",
    "        \"venue_mode\": venue_mode, # top-tier 우선, top-tier only, custom venue, 전체 \n",
    "        \"preferred_venues\": preferred, # 사용자가 직접 지정한 venue 리스트\n",
    "    }\n",
    "\n",
    "\n",
    "#사용자의 선호 설정 문자열로 요약 (dict -> str)\n",
    "def describe_preferences(preferences: dict) -> str:\n",
    "    parts = [f\"recent {preferences.get('year_window', DEFAULT_YEAR_WINDOW)} years\"]\n",
    "    venue_mode = preferences.get(\"venue_mode\", \"all\")\n",
    "    if venue_mode == \"top\":\n",
    "        parts.append(\"top-tier prioritized\")\n",
    "    elif venue_mode == \"top_only\":\n",
    "        parts.append(\"top-tier only\")\n",
    "    elif venue_mode == \"custom\":\n",
    "        prefs = preferences.get(\"preferred_venues\", [])\n",
    "        if prefs:\n",
    "            parts.append(\"venues: \" + \", \".join(prefs))\n",
    "    else:\n",
    "        parts.append(\"all venues\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "def rerank_with_preferences(docs_with_scores, preferences: dict):\n",
    "    # 검색 결과(docs_with_scores)에 선호도 기반 가중치를 더해 재정렬.\n",
    "    preferences = preferences or {}\n",
    "    year_window = preferences.get(\"year_window\", DEFAULT_YEAR_WINDOW)\n",
    "    venue_mode = preferences.get(\"venue_mode\", \"all\")\n",
    "    preferred = {v.upper() for v in preferences.get(\"preferred_venues\", [])}\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    applied = {\"year_cutoff\": None, \"venue_mode\": venue_mode}\n",
    "    cutoff = current_year - year_window if year_window else None\n",
    "\n",
    "    filtered = docs_with_scores\n",
    "    \n",
    "    #연도 필터링 \n",
    "    if cutoff is not None:\n",
    "        #필터링 후 결과가 비면 필터링을 포기하고 원래 목록을 그대로 사용\n",
    "        filtered = [ \n",
    "            (doc, score)\n",
    "            for doc, score in filtered\n",
    "            if int(doc.metadata.get(\"year\", 0) or 0) >= cutoff\n",
    "        ]\n",
    "        applied[\"year_cutoff\"] = cutoff\n",
    "        if not filtered: \n",
    "            filtered = docs_with_scores\n",
    "            applied[\"year_cutoff\"] = None\n",
    "\n",
    "    reranked = []\n",
    "    for doc, score in filtered:\n",
    "        meta = doc.metadata or {}\n",
    "        #선호도 기반 점수 재계산\n",
    "        base_similarity = -float(score) \n",
    "        #최신 연도 가중치\n",
    "        year = meta.get(\"year\")\n",
    "        recency_bonus = 0.0\n",
    "        if isinstance(year, (int, float)):\n",
    "            years_old = max(current_year - int(year), 0)\n",
    "            if year_window:\n",
    "                recency_bonus = max(0, year_window - years_old) * RECENCY_WEIGHT\n",
    "        #학회 가중치\n",
    "        venue = str(meta.get(\"venue\", \"\")).upper()\n",
    "        venue_bonus = 0.0\n",
    "        if venue_mode == \"top\":\n",
    "            if venue in TOP_TIER_VENUES:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "        elif venue_mode == \"top_only\":\n",
    "            if venue in TOP_TIER_VENUES:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "            else:\n",
    "                continue # top-tier 아니면 아예 제외\n",
    "        elif venue_mode == \"custom\":\n",
    "            if venue in preferred:\n",
    "                venue_bonus = VENUE_WEIGHT\n",
    "        #최종 점수\n",
    "        final_score = base_similarity + recency_bonus + venue_bonus\n",
    "        reranked.append((doc, final_score))\n",
    "    #점수가 높은 순으로 재정렬.\n",
    "    if not reranked:\n",
    "        reranked = [(doc, -float(score)) for doc, score in docs_with_scores]\n",
    "\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    return reranked, applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084a693",
   "metadata": {},
   "source": [
    "(2) RAG 한 번 호출하는 함수만 두기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b9d8121c594bb59fb0dae43ef1d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# RAG 함수 정의\n",
    "# ===============================\n",
    "\n",
    "def rag_answer(question: str, k: int = 4, preferences: dict | None = None):\n",
    "    \"\"\"FAISS에서 관련 문서를 검색하고, LLM으로 답변을 생성하는 단일 RAG 함수.\"\"\"\n",
    "    if vector_store is None:\n",
    "        raise RuntimeError(\"vector_store가 초기화되지 않았습니다. FAISS 로드 셀을 먼저 확인하세요.\")\n",
    "\n",
    "    print(f\"[RAG] 검색 질의: {question}\")\n",
    "    docs_with_scores = vector_store.similarity_search_with_score(question, k=k)\n",
    "    print(f\"[RAG] 검색된 문서 수: {len(docs_with_scores)}\")\n",
    "\n",
    "    #연도/학회 규칙 기반 재정렬\n",
    "    reranked_docs, applied_filters = rerank_with_preferences(docs_with_scores, preferences or {})\n",
    "    docs = [doc for doc, _ in reranked_docs]\n",
    "    print(f\"[RAG] After filters: {len(docs)}\")\n",
    "\n",
    "    # 검색된 문서 내용을 하나의 문자열로 합치기\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    # 프롬프트 + LLM 호출\n",
    "    messages = prompt.invoke({\"question\": question, \"context\": context_text})\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return response.content, docs, applied_filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebf30a4ed8546c99ce62806580a697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paper Chatbot ---\n",
      "[SETUP] Enter your search preferences.\n",
      "[경고] 숫자가 아닙니다. 기본값 5년을 사용합니다.\n",
      "[INFO] Preferences: recent 5 years | top-tier prioritized\n",
      " [You]: 고양이 발 개수는 몇개야?\n",
      "[RAG] 검색 질의: 고양이 발 개수는 몇개야?\n",
      "[RAG] 검색된 문서 수: 3\n",
      "[RAG] After filters: 1\n",
      "[Bot]: 제공된 논문/모델 정보에는 고양이 발 개수에 대한 정보가 포함되어 있지 않습니다.\n",
      "[Filters] filtered to >= 2020 year | top-tier prioritized\n",
      "--- Sources ---\n",
      "[1] GPT-3 (2020, NeurIPS): 모델명: GPT-3. title: Language Models are Few-Shot Learners author: Tom B. Brown et al. year: 2020 venue: NeurIPS task: Lan...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Paper Chatbot ---\")\n",
    "\n",
    "if vector_store is None: #FAISS 인덱스가 로드되지 않았다면 실행할 수 없으므로 사용자에게 오류 전달\n",
    "    print(\"[ERROR] Vector store is not loaded. Run the FAISS load cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        preferences = collect_user_preferences() #사용자 선호도 입력받기\n",
    "        print(f\"[INFO] Preferences: {describe_preferences(preferences)}\")\n",
    "\n",
    "        user_question = input(\"Enter your question: \").strip()\n",
    "\n",
    "        print(f\" [You]: {user_question}\")\n",
    "        answer, used_docs, applied = rag_answer(user_question, k=4, preferences=preferences)\n",
    "\n",
    "        print(f\"[Bot]: {answer}\")\n",
    "        \n",
    "        # 실제로 RAG에서 어떤 필터가 적용되었는지 사용자에게 피드백\n",
    "        note = []\n",
    "        if applied.get(\"year_cutoff\"):\n",
    "            note.append(f\"filtered to >= {applied['year_cutoff']} year\")\n",
    "        vm = applied.get(\"venue_mode\")\n",
    "        if vm == \"top\":\n",
    "            note.append(\"top-tier prioritized\")\n",
    "        elif vm == \"top_only\":\n",
    "            note.append(\"top-tier only\")\n",
    "        elif vm == \"custom\":\n",
    "            note.append(\"custom venues prioritized\")\n",
    "        if note:\n",
    "            print(\"[Filters] \" + \" | \".join(note))\n",
    "        # 사용된 문서(Sources) 출력\n",
    "        print(\"--- Sources ---\")\n",
    "        for i, doc in enumerate(used_docs):\n",
    "            model_name = doc.metadata.get(\"model_name\", \"Unknown\")\n",
    "            year = doc.metadata.get(\"year\", \"?\")\n",
    "            venue = doc.metadata.get(\"venue\", \"?\")\n",
    "            preview = doc.page_content[:120].replace(\"\\n\", \" \")\n",
    "            print(f\"[{i+1}] {model_name} ({year}, {venue}): {preview}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"[ERROR] RAG run failed: {e}\")\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16961170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}